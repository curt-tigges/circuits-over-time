{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import circuitsvis as cv\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformer_lens.cautils.notebook import *\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from utils.cspa_functions import (\n",
    "    get_cspa_results_batched,\n",
    "    get_result_mean\n",
    ")\n",
    "from utils.cspa_extra_utils import (\n",
    "    process_webtext,\n",
    ")\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "model.set_use_split_qkv_input(False)\n",
    "model.set_use_attn_result(True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500 # 80 for viz\n",
    "SEQ_LEN = 1000 # 61 for viz\n",
    "\n",
    "current_batch_size = 17 # These are smaller values we use for vizualization since only these appear on streamlit\n",
    "current_seq_len = 61\n",
    "\n",
    "NEGATIVE_HEADS = [(10, 7), (11, 10)]\n",
    "DATA_TOKS, DATA_STR_TOKS_PARSED, indices = process_webtext(seed=6, batch_size=BATCH_SIZE, seq_len=SEQ_LEN, model=model, verbose=True, return_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SEMANTICITY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SEMANTICITY:\n",
    "    cspa_semantic_dict = pickle.load(open(\"cspa/cspa_semantic_dict_full.pkl\", \"rb\"))\n",
    "\n",
    "else:\n",
    "    warnings.warn(\"Not using semanticity unlike old notebook versions!\")\n",
    "    cspa_semantic_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's save a mean for later use...\n",
    "\n",
    "result_mean = get_result_mean([(10, 7), (11, 10)], DATA_TOKS[:100, :], model, verbose=True)\n",
    "# t.save(result_mean, f\"/home/ubuntu/SERI-MATS-2023-Streamlit-pages/transformer_lens/rs/callum2/st_page/media/result_mean.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirically, as long as SEQ_LEN large, small BATCH_SIZE gives quite good estimates\n",
    "QK_OV_BATCH_SIZE = 20\n",
    "QK_OV_SEQ_LEN = 600\n",
    "\n",
    "cspa_results_qk_ov = get_cspa_results_batched(\n",
    "    model = model,\n",
    "    toks = DATA_TOKS[:QK_OV_BATCH_SIZE, :QK_OV_SEQ_LEN],\n",
    "    max_batch_size = 1, # 50,\n",
    "    negative_head = (10, 7),\n",
    "    interventions = [\"ov\", \"qk\"],\n",
    "    K_unembeddings = 0.05, # most interesting in range 3-8 (out of 80)\n",
    "    K_semantic = 1, # either 1 or up to 8 to capture all sem similar\n",
    "    semantic_dict = cspa_semantic_dict,\n",
    "    result_mean = result_mean,\n",
    "    use_cuda = True,\n",
    "    verbose = True,\n",
    "    compute_s_sstar_dict = False,\n",
    "    computation_device = \"cpu\", # device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
