{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "import einops\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from torch import Tensor\n",
    "from torchtyping import TensorType as TT\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from utils.data_utils import generate_data_and_caches\n",
    "from utils.data_processing import (\n",
    "    load_edge_scores_into_dictionary,\n",
    ")\n",
    "from utils.visualization import plot_attention_heads, imshow_p\n",
    "from utils.backup_analysis import (\n",
    "    load_model,\n",
    "    run_iteration,\n",
    "    process_backup_results,\n",
    "    get_past_nmhs_for_checkpoints,\n",
    "    plot_top_heads\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f7d29392a40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = 'ioi'\n",
    "PERFORMANCE_METRIC = 'logit_diff'\n",
    "BASE_MODEL = \"pythia-70m\"\n",
    "VARIANT = None #\"EleutherAI/pythia-70m-weight-seed3\"\n",
    "MODEL_SHORTNAME = BASE_MODEL if not VARIANT else VARIANT[11:]\n",
    "CACHE = \"model_cache\"\n",
    "IOI_DATASET_SIZE = 70\n",
    "COPY_SCORE_THRESHOLD = 75.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/143: results/graphs/pythia-70m/ioi/57000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000093       False       57000\n",
      "3576    a5.h5->logits  0.002090       False       57000\n",
      "3577    a5.h6->logits -0.000176       False       57000\n",
      "3578    a5.h7->logits  0.001198       False       57000\n",
      "3579       m5->logits  0.021729       False       57000\n",
      "\n",
      "[3580 rows x 4 columns]\n",
      "Processing file 2/143: results/graphs/pythia-70m/ioi/141000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000011       False      141000\n",
      "3576    a5.h5->logits  0.001083        True      141000\n",
      "3577    a5.h6->logits -0.001480        True      141000\n",
      "3578    a5.h7->logits -0.000893        True      141000\n",
      "3579       m5->logits  0.006775        True      141000\n",
      "\n",
      "[7160 rows x 4 columns]\n",
      "Processing file 3/143: results/graphs/pythia-70m/ioi/95000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000093        True       95000\n",
      "3576    a5.h5->logits  0.000858        True       95000\n",
      "3577    a5.h6->logits -0.000404        True       95000\n",
      "3578    a5.h7->logits  0.000093        True       95000\n",
      "3579       m5->logits  0.006622        True       95000\n",
      "\n",
      "[10740 rows x 4 columns]\n",
      "Processing file 4/143: results/graphs/pythia-70m/ioi/107000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001984        True      107000\n",
      "3576    a5.h5->logits  0.000546        True      107000\n",
      "3577    a5.h6->logits -0.001221        True      107000\n",
      "3578    a5.h7->logits -0.001228        True      107000\n",
      "3579       m5->logits  0.002975        True      107000\n",
      "\n",
      "[14320 rows x 4 columns]\n",
      "Processing file 5/143: results/graphs/pythia-70m/ioi/34000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000200       False       34000\n",
      "3576    a5.h5->logits  0.002457       False       34000\n",
      "3577    a5.h6->logits -0.000309       False       34000\n",
      "3578    a5.h7->logits  0.001953       False       34000\n",
      "3579       m5->logits  0.014221       False       34000\n",
      "\n",
      "[17900 rows x 4 columns]\n",
      "Processing file 6/143: results/graphs/pythia-70m/ioi/6000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000372       False        6000\n",
      "3576    a5.h5->logits  0.003403        True        6000\n",
      "3577    a5.h6->logits -0.000257       False        6000\n",
      "3578    a5.h7->logits -0.002350        True        6000\n",
      "3579       m5->logits -0.003067        True        6000\n",
      "\n",
      "[21480 rows x 4 columns]\n",
      "Processing file 7/143: results/graphs/pythia-70m/ioi/37000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000204       False       37000\n",
      "3576    a5.h5->logits  0.002686       False       37000\n",
      "3577    a5.h6->logits -0.000025       False       37000\n",
      "3578    a5.h7->logits  0.001984       False       37000\n",
      "3579       m5->logits  0.018799       False       37000\n",
      "\n",
      "[25060 rows x 4 columns]\n",
      "Processing file 8/143: results/graphs/pythia-70m/ioi/39000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000303       False       39000\n",
      "3576    a5.h5->logits  0.002045       False       39000\n",
      "3577    a5.h6->logits  0.000039       False       39000\n",
      "3578    a5.h7->logits  0.001350       False       39000\n",
      "3579       m5->logits  0.016113       False       39000\n",
      "\n",
      "[28640 rows x 4 columns]\n",
      "Processing file 9/143: results/graphs/pythia-70m/ioi/104000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.002167        True      104000\n",
      "3576    a5.h5->logits  0.001251        True      104000\n",
      "3577    a5.h6->logits -0.000469        True      104000\n",
      "3578    a5.h7->logits -0.000881        True      104000\n",
      "3579       m5->logits  0.003448        True      104000\n",
      "\n",
      "[32220 rows x 4 columns]\n",
      "Processing file 10/143: results/graphs/pythia-70m/ioi/59000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000854       False       59000\n",
      "3576    a5.h5->logits  0.001656       False       59000\n",
      "3577    a5.h6->logits -0.000401       False       59000\n",
      "3578    a5.h7->logits  0.001266       False       59000\n",
      "3579       m5->logits  0.026489       False       59000\n",
      "\n",
      "[35800 rows x 4 columns]\n",
      "Processing file 11/143: results/graphs/pythia-70m/ioi/67000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000450       False       67000\n",
      "3576    a5.h5->logits  0.002014       False       67000\n",
      "3577    a5.h6->logits -0.000614       False       67000\n",
      "3578    a5.h7->logits  0.000496       False       67000\n",
      "3579       m5->logits  0.019531       False       67000\n",
      "\n",
      "[39380 rows x 4 columns]\n",
      "Processing file 12/143: results/graphs/pythia-70m/ioi/111000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.002045       False      111000\n",
      "3576    a5.h5->logits  0.000282       False      111000\n",
      "3577    a5.h6->logits -0.001442       False      111000\n",
      "3578    a5.h7->logits -0.000992        True      111000\n",
      "3579       m5->logits -0.001724        True      111000\n",
      "\n",
      "[42960 rows x 4 columns]\n",
      "Processing file 13/143: results/graphs/pythia-70m/ioi/76000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000629        True       76000\n",
      "3576    a5.h5->logits  0.001022        True       76000\n",
      "3577    a5.h6->logits  0.000029        True       76000\n",
      "3578    a5.h7->logits -0.000387        True       76000\n",
      "3579       m5->logits  0.009705        True       76000\n",
      "\n",
      "[46540 rows x 4 columns]\n",
      "Processing file 14/143: results/graphs/pythia-70m/ioi/5000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000110       False        5000\n",
      "3576    a5.h5->logits  0.003998        True        5000\n",
      "3577    a5.h6->logits -0.000138       False        5000\n",
      "3578    a5.h7->logits -0.002167       False        5000\n",
      "3579       m5->logits -0.005890        True        5000\n",
      "\n",
      "[50120 rows x 4 columns]\n",
      "Processing file 15/143: results/graphs/pythia-70m/ioi/42000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000074       False       42000\n",
      "3576    a5.h5->logits  0.002197       False       42000\n",
      "3577    a5.h6->logits -0.000198       False       42000\n",
      "3578    a5.h7->logits  0.001450       False       42000\n",
      "3579       m5->logits  0.014526        True       42000\n",
      "\n",
      "[53700 rows x 4 columns]\n",
      "Processing file 16/143: results/graphs/pythia-70m/ioi/77000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000064       False       77000\n",
      "3576    a5.h5->logits  0.000383        True       77000\n",
      "3577    a5.h6->logits -0.000177        True       77000\n",
      "3578    a5.h7->logits -0.000599        True       77000\n",
      "3579       m5->logits  0.013977        True       77000\n",
      "\n",
      "[57280 rows x 4 columns]\n",
      "Processing file 17/143: results/graphs/pythia-70m/ioi/86000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001022        True       86000\n",
      "3576    a5.h5->logits  0.000465        True       86000\n",
      "3577    a5.h6->logits -0.001213        True       86000\n",
      "3578    a5.h7->logits -0.001175        True       86000\n",
      "3579       m5->logits  0.010437        True       86000\n",
      "\n",
      "[60860 rows x 4 columns]\n",
      "Processing file 18/143: results/graphs/pythia-70m/ioi/80000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001183       False       80000\n",
      "3576    a5.h5->logits  0.001144       False       80000\n",
      "3577    a5.h6->logits -0.000427       False       80000\n",
      "3578    a5.h7->logits -0.000591       False       80000\n",
      "3579       m5->logits  0.012512       False       80000\n",
      "\n",
      "[64440 rows x 4 columns]\n",
      "Processing file 19/143: results/graphs/pythia-70m/ioi/81000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.003189       False       81000\n",
      "3576    a5.h5->logits  0.000782       False       81000\n",
      "3577    a5.h6->logits -0.001373       False       81000\n",
      "3578    a5.h7->logits -0.000511       False       81000\n",
      "3579       m5->logits  0.013000       False       81000\n",
      "\n",
      "[68020 rows x 4 columns]\n",
      "Processing file 20/143: results/graphs/pythia-70m/ioi/63000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001007       False       63000\n",
      "3576    a5.h5->logits  0.001984        True       63000\n",
      "3577    a5.h6->logits -0.000031       False       63000\n",
      "3578    a5.h7->logits  0.000089       False       63000\n",
      "3579       m5->logits  0.020874        True       63000\n",
      "\n",
      "[71600 rows x 4 columns]\n",
      "Processing file 21/143: results/graphs/pythia-70m/ioi/142000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.003204       False      142000\n",
      "3576    a5.h5->logits  0.000219       False      142000\n",
      "3577    a5.h6->logits -0.000087       False      142000\n",
      "3578    a5.h7->logits -0.000847       False      142000\n",
      "3579       m5->logits -0.000309       False      142000\n",
      "\n",
      "[75180 rows x 4 columns]\n",
      "Processing file 22/143: results/graphs/pythia-70m/ioi/56000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000311       False       56000\n",
      "3576    a5.h5->logits  0.002502       False       56000\n",
      "3577    a5.h6->logits -0.000565       False       56000\n",
      "3578    a5.h7->logits  0.001106       False       56000\n",
      "3579       m5->logits  0.015259       False       56000\n",
      "\n",
      "[78760 rows x 4 columns]\n",
      "Processing file 23/143: results/graphs/pythia-70m/ioi/8000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000652       False        8000\n",
      "3576    a5.h5->logits  0.005432       False        8000\n",
      "3577    a5.h6->logits -0.000195       False        8000\n",
      "3578    a5.h7->logits -0.000020       False        8000\n",
      "3579       m5->logits  0.000717       False        8000\n",
      "\n",
      "[82340 rows x 4 columns]\n",
      "Processing file 24/143: results/graphs/pythia-70m/ioi/93000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000893       False       93000\n",
      "3576    a5.h5->logits  0.001518        True       93000\n",
      "3577    a5.h6->logits -0.002930       False       93000\n",
      "3578    a5.h7->logits -0.000782       False       93000\n",
      "3579       m5->logits  0.003479        True       93000\n",
      "\n",
      "[85920 rows x 4 columns]\n",
      "Processing file 25/143: results/graphs/pythia-70m/ioi/120000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.002213        True      120000\n",
      "3576    a5.h5->logits  0.000877        True      120000\n",
      "3577    a5.h6->logits -0.001923        True      120000\n",
      "3578    a5.h7->logits -0.001602        True      120000\n",
      "3579       m5->logits  0.009766        True      120000\n",
      "\n",
      "[89500 rows x 4 columns]\n",
      "Processing file 26/143: results/graphs/pythia-70m/ioi/62000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000504       False       62000\n",
      "3576    a5.h5->logits  0.001740       False       62000\n",
      "3577    a5.h6->logits  0.000049       False       62000\n",
      "3578    a5.h7->logits  0.001167       False       62000\n",
      "3579       m5->logits  0.020386        True       62000\n",
      "\n",
      "[93080 rows x 4 columns]\n",
      "Processing file 27/143: results/graphs/pythia-70m/ioi/70000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001518       False       70000\n",
      "3576    a5.h5->logits  0.001511       False       70000\n",
      "3577    a5.h6->logits -0.000847       False       70000\n",
      "3578    a5.h7->logits  0.001518       False       70000\n",
      "3579       m5->logits  0.026123       False       70000\n",
      "\n",
      "[96660 rows x 4 columns]\n",
      "Processing file 28/143: results/graphs/pythia-70m/ioi/19000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000004       False       19000\n",
      "3576    a5.h5->logits  0.003357       False       19000\n",
      "3577    a5.h6->logits -0.000607       False       19000\n",
      "3578    a5.h7->logits  0.000568       False       19000\n",
      "3579       m5->logits  0.007690        True       19000\n",
      "\n",
      "[100240 rows x 4 columns]\n",
      "Processing file 29/143: results/graphs/pythia-70m/ioi/121000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000961        True      121000\n",
      "3576    a5.h5->logits  0.000607        True      121000\n",
      "3577    a5.h6->logits -0.001434        True      121000\n",
      "3578    a5.h7->logits -0.000896        True      121000\n",
      "3579       m5->logits  0.005157        True      121000\n",
      "\n",
      "[103820 rows x 4 columns]\n",
      "Processing file 30/143: results/graphs/pythia-70m/ioi/105000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001274       False      105000\n",
      "3576    a5.h5->logits  0.000832       False      105000\n",
      "3577    a5.h6->logits -0.000607       False      105000\n",
      "3578    a5.h7->logits -0.001144       False      105000\n",
      "3579       m5->logits  0.002167       False      105000\n",
      "\n",
      "[107400 rows x 4 columns]\n",
      "Processing file 31/143: results/graphs/pythia-70m/ioi/129000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001022        True      129000\n",
      "3576    a5.h5->logits  0.001259        True      129000\n",
      "3577    a5.h6->logits -0.000345        True      129000\n",
      "3578    a5.h7->logits -0.001694        True      129000\n",
      "3579       m5->logits -0.002518        True      129000\n",
      "\n",
      "[110980 rows x 4 columns]\n",
      "Processing file 32/143: results/graphs/pythia-70m/ioi/2000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000759        True        2000\n",
      "3576    a5.h5->logits  0.000759        True        2000\n",
      "3577    a5.h6->logits -0.000079       False        2000\n",
      "3578    a5.h7->logits -0.003647        True        2000\n",
      "3579       m5->logits -0.013916        True        2000\n",
      "\n",
      "[114560 rows x 4 columns]\n",
      "Processing file 33/143: results/graphs/pythia-70m/ioi/96000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000238        True       96000\n",
      "3576    a5.h5->logits  0.000900        True       96000\n",
      "3577    a5.h6->logits  0.000058        True       96000\n",
      "3578    a5.h7->logits -0.001465        True       96000\n",
      "3579       m5->logits -0.000854        True       96000\n",
      "\n",
      "[118140 rows x 4 columns]\n",
      "Processing file 34/143: results/graphs/pythia-70m/ioi/124000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000999        True      124000\n",
      "3576    a5.h5->logits  0.000866        True      124000\n",
      "3577    a5.h6->logits -0.001808        True      124000\n",
      "3578    a5.h7->logits -0.001755        True      124000\n",
      "3579       m5->logits -0.001472        True      124000\n",
      "\n",
      "[121720 rows x 4 columns]\n",
      "Processing file 35/143: results/graphs/pythia-70m/ioi/143000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001305       False      143000\n",
      "3576    a5.h5->logits  0.000801       False      143000\n",
      "3577    a5.h6->logits -0.000454       False      143000\n",
      "3578    a5.h7->logits -0.001625       False      143000\n",
      "3579       m5->logits  0.006805        True      143000\n",
      "\n",
      "[125300 rows x 4 columns]\n",
      "Processing file 36/143: results/graphs/pythia-70m/ioi/79000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.003418        True       79000\n",
      "3576    a5.h5->logits  0.001663        True       79000\n",
      "3577    a5.h6->logits -0.001373        True       79000\n",
      "3578    a5.h7->logits -0.000664        True       79000\n",
      "3579       m5->logits  0.013489        True       79000\n",
      "\n",
      "[128880 rows x 4 columns]\n",
      "Processing file 37/143: results/graphs/pythia-70m/ioi/29000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000607       False       29000\n",
      "3576    a5.h5->logits  0.003174       False       29000\n",
      "3577    a5.h6->logits -0.000444       False       29000\n",
      "3578    a5.h7->logits  0.001305       False       29000\n",
      "3579       m5->logits  0.009399        True       29000\n",
      "\n",
      "[132460 rows x 4 columns]\n",
      "Processing file 38/143: results/graphs/pythia-70m/ioi/137000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.002548        True      137000\n",
      "3576    a5.h5->logits  0.000759        True      137000\n",
      "3577    a5.h6->logits -0.002930        True      137000\n",
      "3578    a5.h7->logits -0.000740        True      137000\n",
      "3579       m5->logits  0.003616        True      137000\n",
      "\n",
      "[136040 rows x 4 columns]\n",
      "Processing file 39/143: results/graphs/pythia-70m/ioi/10000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000629       False       10000\n",
      "3576    a5.h5->logits  0.005219       False       10000\n",
      "3577    a5.h6->logits -0.000228       False       10000\n",
      "3578    a5.h7->logits  0.000286       False       10000\n",
      "3579       m5->logits  0.004456       False       10000\n",
      "\n",
      "[139620 rows x 4 columns]\n",
      "Processing file 40/143: results/graphs/pythia-70m/ioi/135000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001587       False      135000\n",
      "3576    a5.h5->logits  0.001572       False      135000\n",
      "3577    a5.h6->logits -0.000751       False      135000\n",
      "3578    a5.h7->logits -0.001068       False      135000\n",
      "3579       m5->logits  0.001808       False      135000\n",
      "\n",
      "[143200 rows x 4 columns]\n",
      "Processing file 41/143: results/graphs/pythia-70m/ioi/65000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000740       False       65000\n",
      "3576    a5.h5->logits  0.001984       False       65000\n",
      "3577    a5.h6->logits  0.000309       False       65000\n",
      "3578    a5.h7->logits  0.001007       False       65000\n",
      "3579       m5->logits  0.025391        True       65000\n",
      "\n",
      "[146780 rows x 4 columns]\n",
      "Processing file 42/143: results/graphs/pythia-70m/ioi/60000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000942       False       60000\n",
      "3576    a5.h5->logits  0.001663       False       60000\n",
      "3577    a5.h6->logits  0.000047       False       60000\n",
      "3578    a5.h7->logits  0.001122       False       60000\n",
      "3579       m5->logits  0.019653       False       60000\n",
      "\n",
      "[150360 rows x 4 columns]\n",
      "Processing file 43/143: results/graphs/pythia-70m/ioi/90000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001328       False       90000\n",
      "3576    a5.h5->logits  0.001060        True       90000\n",
      "3577    a5.h6->logits -0.000179       False       90000\n",
      "3578    a5.h7->logits -0.001884        True       90000\n",
      "3579       m5->logits  0.004120        True       90000\n",
      "\n",
      "[153940 rows x 4 columns]\n",
      "Processing file 44/143: results/graphs/pythia-70m/ioi/106000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001175       False      106000\n",
      "3576    a5.h5->logits  0.000062       False      106000\n",
      "3577    a5.h6->logits -0.000652       False      106000\n",
      "3578    a5.h7->logits -0.001884       False      106000\n",
      "3579       m5->logits  0.000332       False      106000\n",
      "\n",
      "[157520 rows x 4 columns]\n",
      "Processing file 45/143: results/graphs/pythia-70m/ioi/1000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000889        True        1000\n",
      "3576    a5.h5->logits -0.000961        True        1000\n",
      "3577    a5.h6->logits -0.000147       False        1000\n",
      "3578    a5.h7->logits -0.001236        True        1000\n",
      "3579       m5->logits -0.012146        True        1000\n",
      "\n",
      "[161100 rows x 4 columns]\n",
      "Processing file 46/143: results/graphs/pythia-70m/ioi/33000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000389       False       33000\n",
      "3576    a5.h5->logits  0.003372       False       33000\n",
      "3577    a5.h6->logits -0.000179       False       33000\n",
      "3578    a5.h7->logits  0.001579       False       33000\n",
      "3579       m5->logits  0.012146        True       33000\n",
      "\n",
      "[164680 rows x 4 columns]\n",
      "Processing file 47/143: results/graphs/pythia-70m/ioi/103000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000319       False      103000\n",
      "3576    a5.h5->logits  0.000835       False      103000\n",
      "3577    a5.h6->logits -0.000214       False      103000\n",
      "3578    a5.h7->logits -0.000931        True      103000\n",
      "3579       m5->logits -0.000315       False      103000\n",
      "\n",
      "[168260 rows x 4 columns]\n",
      "Processing file 48/143: results/graphs/pythia-70m/ioi/113000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001106        True      113000\n",
      "3576    a5.h5->logits  0.001122        True      113000\n",
      "3577    a5.h6->logits -0.001953        True      113000\n",
      "3578    a5.h7->logits -0.000961        True      113000\n",
      "3579       m5->logits  0.000549        True      113000\n",
      "\n",
      "[171840 rows x 4 columns]\n",
      "Processing file 49/143: results/graphs/pythia-70m/ioi/35000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000299       False       35000\n",
      "3576    a5.h5->logits  0.002182       False       35000\n",
      "3577    a5.h6->logits -0.000271       False       35000\n",
      "3578    a5.h7->logits  0.001564       False       35000\n",
      "3579       m5->logits  0.011169       False       35000\n",
      "\n",
      "[175420 rows x 4 columns]\n",
      "Processing file 50/143: results/graphs/pythia-70m/ioi/133000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001083        True      133000\n",
      "3576    a5.h5->logits  0.001907        True      133000\n",
      "3577    a5.h6->logits -0.000189        True      133000\n",
      "3578    a5.h7->logits -0.000877        True      133000\n",
      "3579       m5->logits  0.003357        True      133000\n",
      "\n",
      "[179000 rows x 4 columns]\n",
      "Processing file 51/143: results/graphs/pythia-70m/ioi/18000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000096       False       18000\n",
      "3576    a5.h5->logits  0.003662        True       18000\n",
      "3577    a5.h6->logits -0.000523       False       18000\n",
      "3578    a5.h7->logits  0.000916       False       18000\n",
      "3579       m5->logits  0.007477        True       18000\n",
      "\n",
      "[182580 rows x 4 columns]\n",
      "Processing file 52/143: results/graphs/pythia-70m/ioi/55000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000603       False       55000\n",
      "3576    a5.h5->logits  0.002472       False       55000\n",
      "3577    a5.h6->logits -0.000309       False       55000\n",
      "3578    a5.h7->logits  0.001297       False       55000\n",
      "3579       m5->logits  0.021851       False       55000\n",
      "\n",
      "[186160 rows x 4 columns]\n",
      "Processing file 53/143: results/graphs/pythia-70m/ioi/102000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.002106       False      102000\n",
      "3576    a5.h5->logits  0.000954       False      102000\n",
      "3577    a5.h6->logits -0.001923       False      102000\n",
      "3578    a5.h7->logits -0.001442       False      102000\n",
      "3579       m5->logits  0.005371       False      102000\n",
      "\n",
      "[189740 rows x 4 columns]\n",
      "Processing file 54/143: results/graphs/pythia-70m/ioi/108000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.002182       False      108000\n",
      "3576    a5.h5->logits  0.000664       False      108000\n",
      "3577    a5.h6->logits  0.000471       False      108000\n",
      "3578    a5.h7->logits -0.001091       False      108000\n",
      "3579       m5->logits -0.001022       False      108000\n",
      "\n",
      "[193320 rows x 4 columns]\n",
      "Processing file 55/143: results/graphs/pythia-70m/ioi/49000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000889       False       49000\n",
      "3576    a5.h5->logits  0.003082       False       49000\n",
      "3577    a5.h6->logits -0.000238       False       49000\n",
      "3578    a5.h7->logits  0.002197       False       49000\n",
      "3579       m5->logits  0.018921        True       49000\n",
      "\n",
      "[196900 rows x 4 columns]\n",
      "Processing file 56/143: results/graphs/pythia-70m/ioi/130000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.001083       False      130000\n",
      "3576    a5.h5->logits  0.000183       False      130000\n",
      "3577    a5.h6->logits -0.003098       False      130000\n",
      "3578    a5.h7->logits -0.001503       False      130000\n",
      "3579       m5->logits  0.005707       False      130000\n",
      "\n",
      "[200480 rows x 4 columns]\n",
      "Processing file 57/143: results/graphs/pythia-70m/ioi/83000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000961       False       83000\n",
      "3576    a5.h5->logits  0.001083       False       83000\n",
      "3577    a5.h6->logits -0.000530       False       83000\n",
      "3578    a5.h7->logits -0.000448       False       83000\n",
      "3579       m5->logits  0.015259       False       83000\n",
      "\n",
      "[204060 rows x 4 columns]\n",
      "Processing file 58/143: results/graphs/pythia-70m/ioi/31000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000263       False       31000\n",
      "3576    a5.h5->logits  0.003036       False       31000\n",
      "3577    a5.h6->logits -0.000456       False       31000\n",
      "3578    a5.h7->logits  0.001328       False       31000\n",
      "3579       m5->logits  0.011047        True       31000\n",
      "\n",
      "[207640 rows x 4 columns]\n",
      "Processing file 59/143: results/graphs/pythia-70m/ioi/46000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000275       False       46000\n",
      "3576    a5.h5->logits  0.002625       False       46000\n",
      "3577    a5.h6->logits -0.000159       False       46000\n",
      "3578    a5.h7->logits  0.001678       False       46000\n",
      "3579       m5->logits  0.014099       False       46000\n",
      "\n",
      "[211220 rows x 4 columns]\n",
      "Processing file 60/143: results/graphs/pythia-70m/ioi/112000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000984        True      112000\n",
      "3576    a5.h5->logits  0.000725       False      112000\n",
      "3577    a5.h6->logits -0.003815        True      112000\n",
      "3578    a5.h7->logits -0.000984        True      112000\n",
      "3579       m5->logits  0.000380       False      112000\n",
      "\n",
      "[214800 rows x 4 columns]\n",
      "Processing file 61/143: results/graphs/pythia-70m/ioi/26000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000538       False       26000\n",
      "3576    a5.h5->logits  0.003204       False       26000\n",
      "3577    a5.h6->logits -0.000446       False       26000\n",
      "3578    a5.h7->logits  0.000572       False       26000\n",
      "3579       m5->logits  0.012268       False       26000\n",
      "\n",
      "[218380 rows x 4 columns]\n",
      "Processing file 62/143: results/graphs/pythia-70m/ioi/78000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001221       False       78000\n",
      "3576    a5.h5->logits  0.001366       False       78000\n",
      "3577    a5.h6->logits -0.000488       False       78000\n",
      "3578    a5.h7->logits -0.000546       False       78000\n",
      "3579       m5->logits  0.010559       False       78000\n",
      "\n",
      "[221960 rows x 4 columns]\n",
      "Processing file 63/143: results/graphs/pythia-70m/ioi/13000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000226       False       13000\n",
      "3576    a5.h5->logits  0.004333       False       13000\n",
      "3577    a5.h6->logits -0.000188       False       13000\n",
      "3578    a5.h7->logits  0.000265       False       13000\n",
      "3579       m5->logits  0.005402       False       13000\n",
      "\n",
      "[225540 rows x 4 columns]\n",
      "Processing file 64/143: results/graphs/pythia-70m/ioi/47000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000603       False       47000\n",
      "3576    a5.h5->logits  0.002350       False       47000\n",
      "3577    a5.h6->logits -0.000075       False       47000\n",
      "3578    a5.h7->logits  0.002121       False       47000\n",
      "3579       m5->logits  0.017822       False       47000\n",
      "\n",
      "[229120 rows x 4 columns]\n",
      "Processing file 65/143: results/graphs/pythia-70m/ioi/58000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000192       False       58000\n",
      "3576    a5.h5->logits  0.001938       False       58000\n",
      "3577    a5.h6->logits -0.000202       False       58000\n",
      "3578    a5.h7->logits  0.001343       False       58000\n",
      "3579       m5->logits  0.020142       False       58000\n",
      "\n",
      "[232700 rows x 4 columns]\n",
      "Processing file 66/143: results/graphs/pythia-70m/ioi/134000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001137        True      134000\n",
      "3576    a5.h5->logits  0.001259        True      134000\n",
      "3577    a5.h6->logits  0.000263        True      134000\n",
      "3578    a5.h7->logits  0.000288        True      134000\n",
      "3579       m5->logits  0.001213        True      134000\n",
      "\n",
      "[236280 rows x 4 columns]\n",
      "Processing file 67/143: results/graphs/pythia-70m/ioi/100000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000154       False      100000\n",
      "3576    a5.h5->logits  0.001175        True      100000\n",
      "3577    a5.h6->logits  0.000182        True      100000\n",
      "3578    a5.h7->logits -0.001785        True      100000\n",
      "3579       m5->logits -0.003036        True      100000\n",
      "\n",
      "[239860 rows x 4 columns]\n",
      "Processing file 68/143: results/graphs/pythia-70m/ioi/138000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.002899        True      138000\n",
      "3576    a5.h5->logits  0.000565        True      138000\n",
      "3577    a5.h6->logits -0.006256        True      138000\n",
      "3578    a5.h7->logits -0.001099        True      138000\n",
      "3579       m5->logits -0.000162       False      138000\n",
      "\n",
      "[243440 rows x 4 columns]\n",
      "Processing file 69/143: results/graphs/pythia-70m/ioi/27000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000458       False       27000\n",
      "3576    a5.h5->logits  0.002899        True       27000\n",
      "3577    a5.h6->logits -0.000538       False       27000\n",
      "3578    a5.h7->logits  0.001427       False       27000\n",
      "3579       m5->logits  0.008606        True       27000\n",
      "\n",
      "[247020 rows x 4 columns]\n",
      "Processing file 70/143: results/graphs/pythia-70m/ioi/48000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000854       False       48000\n",
      "3576    a5.h5->logits  0.002686       False       48000\n",
      "3577    a5.h6->logits -0.000169       False       48000\n",
      "3578    a5.h7->logits  0.001015       False       48000\n",
      "3579       m5->logits  0.013611       False       48000\n",
      "\n",
      "[250600 rows x 4 columns]\n",
      "Processing file 71/143: results/graphs/pythia-70m/ioi/91000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001587       False       91000\n",
      "3576    a5.h5->logits  0.001854        True       91000\n",
      "3577    a5.h6->logits -0.000159       False       91000\n",
      "3578    a5.h7->logits -0.000992        True       91000\n",
      "3579       m5->logits  0.001411        True       91000\n",
      "\n",
      "[254180 rows x 4 columns]\n",
      "Processing file 72/143: results/graphs/pythia-70m/ioi/122000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001175       False      122000\n",
      "3576    a5.h5->logits  0.001083        True      122000\n",
      "3577    a5.h6->logits -0.000629       False      122000\n",
      "3578    a5.h7->logits -0.002136       False      122000\n",
      "3579       m5->logits  0.000652        True      122000\n",
      "\n",
      "[257760 rows x 4 columns]\n",
      "Processing file 73/143: results/graphs/pythia-70m/ioi/99000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001656        True       99000\n",
      "3576    a5.h5->logits  0.001442        True       99000\n",
      "3577    a5.h6->logits -0.001053        True       99000\n",
      "3578    a5.h7->logits -0.000679        True       99000\n",
      "3579       m5->logits  0.009094        True       99000\n",
      "\n",
      "[261340 rows x 4 columns]\n",
      "Processing file 74/143: results/graphs/pythia-70m/ioi/32000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000153       False       32000\n",
      "3576    a5.h5->logits  0.002533       False       32000\n",
      "3577    a5.h6->logits -0.000296       False       32000\n",
      "3578    a5.h7->logits  0.001556       False       32000\n",
      "3579       m5->logits  0.012512       False       32000\n",
      "\n",
      "[264920 rows x 4 columns]\n",
      "Processing file 75/143: results/graphs/pythia-70m/ioi/30000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000115       False       30000\n",
      "3576    a5.h5->logits  0.003723       False       30000\n",
      "3577    a5.h6->logits -0.000376       False       30000\n",
      "3578    a5.h7->logits  0.001633       False       30000\n",
      "3579       m5->logits  0.013550        True       30000\n",
      "\n",
      "[268500 rows x 4 columns]\n",
      "Processing file 76/143: results/graphs/pythia-70m/ioi/44000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000250       False       44000\n",
      "3576    a5.h5->logits  0.002640       False       44000\n",
      "3577    a5.h6->logits -0.000002       False       44000\n",
      "3578    a5.h7->logits  0.001007       False       44000\n",
      "3579       m5->logits  0.014648       False       44000\n",
      "\n",
      "[272080 rows x 4 columns]\n",
      "Processing file 77/143: results/graphs/pythia-70m/ioi/136000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001106       False      136000\n",
      "3576    a5.h5->logits  0.001495       False      136000\n",
      "3577    a5.h6->logits -0.001862       False      136000\n",
      "3578    a5.h7->logits  0.000003       False      136000\n",
      "3579       m5->logits  0.003021       False      136000\n",
      "\n",
      "[275660 rows x 4 columns]\n",
      "Processing file 78/143: results/graphs/pythia-70m/ioi/116000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000603        True      116000\n",
      "3576    a5.h5->logits  0.001045        True      116000\n",
      "3577    a5.h6->logits -0.002274        True      116000\n",
      "3578    a5.h7->logits -0.000431        True      116000\n",
      "3579       m5->logits  0.001808        True      116000\n",
      "\n",
      "[279240 rows x 4 columns]\n",
      "Processing file 79/143: results/graphs/pythia-70m/ioi/74000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000336       False       74000\n",
      "3576    a5.h5->logits  0.001511       False       74000\n",
      "3577    a5.h6->logits -0.000003       False       74000\n",
      "3578    a5.h7->logits  0.000009       False       74000\n",
      "3579       m5->logits  0.024658       False       74000\n",
      "\n",
      "[282820 rows x 4 columns]\n",
      "Processing file 80/143: results/graphs/pythia-70m/ioi/118000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001442       False      118000\n",
      "3576    a5.h5->logits  0.000916        True      118000\n",
      "3577    a5.h6->logits -0.000397       False      118000\n",
      "3578    a5.h7->logits -0.000847       False      118000\n",
      "3579       m5->logits -0.000069       False      118000\n",
      "\n",
      "[286400 rows x 4 columns]\n",
      "Processing file 81/143: results/graphs/pythia-70m/ioi/94000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001091        True       94000\n",
      "3576    a5.h5->logits  0.001541        True       94000\n",
      "3577    a5.h6->logits  0.000183        True       94000\n",
      "3578    a5.h7->logits -0.000366        True       94000\n",
      "3579       m5->logits  0.004425        True       94000\n",
      "\n",
      "[289980 rows x 4 columns]\n",
      "Processing file 82/143: results/graphs/pythia-70m/ioi/119000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001328        True      119000\n",
      "3576    a5.h5->logits  0.001251        True      119000\n",
      "3577    a5.h6->logits -0.001724        True      119000\n",
      "3578    a5.h7->logits -0.001801        True      119000\n",
      "3579       m5->logits  0.002090        True      119000\n",
      "\n",
      "[293560 rows x 4 columns]\n",
      "Processing file 83/143: results/graphs/pythia-70m/ioi/64000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000284       False       64000\n",
      "3576    a5.h5->logits  0.001884       False       64000\n",
      "3577    a5.h6->logits -0.000193       False       64000\n",
      "3578    a5.h7->logits  0.000790       False       64000\n",
      "3579       m5->logits  0.022949       False       64000\n",
      "\n",
      "[297140 rows x 4 columns]\n",
      "Processing file 84/143: results/graphs/pythia-70m/ioi/69000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000900       False       69000\n",
      "3576    a5.h5->logits  0.001656       False       69000\n",
      "3577    a5.h6->logits -0.000237       False       69000\n",
      "3578    a5.h7->logits  0.001259       False       69000\n",
      "3579       m5->logits  0.026001       False       69000\n",
      "\n",
      "[300720 rows x 4 columns]\n",
      "Processing file 85/143: results/graphs/pythia-70m/ioi/140000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001701        True      140000\n",
      "3576    a5.h5->logits  0.001503        True      140000\n",
      "3577    a5.h6->logits  0.000112       False      140000\n",
      "3578    a5.h7->logits -0.001503        True      140000\n",
      "3579       m5->logits -0.000751        True      140000\n",
      "\n",
      "[304300 rows x 4 columns]\n",
      "Processing file 86/143: results/graphs/pythia-70m/ioi/72000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001091       False       72000\n",
      "3576    a5.h5->logits  0.000977       False       72000\n",
      "3577    a5.h6->logits -0.000324       False       72000\n",
      "3578    a5.h7->logits  0.000862       False       72000\n",
      "3579       m5->logits  0.025269       False       72000\n",
      "\n",
      "[307880 rows x 4 columns]\n",
      "Processing file 87/143: results/graphs/pythia-70m/ioi/117000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.001244       False      117000\n",
      "3576    a5.h5->logits  0.000908       False      117000\n",
      "3577    a5.h6->logits -0.000729       False      117000\n",
      "3578    a5.h7->logits -0.000565       False      117000\n",
      "3579       m5->logits  0.006683        True      117000\n",
      "\n",
      "[311460 rows x 4 columns]\n",
      "Processing file 88/143: results/graphs/pythia-70m/ioi/9000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000546       False        9000\n",
      "3576    a5.h5->logits  0.005981       False        9000\n",
      "3577    a5.h6->logits -0.000257       False        9000\n",
      "3578    a5.h7->logits  0.000368       False        9000\n",
      "3579       m5->logits  0.003769       False        9000\n",
      "\n",
      "[315040 rows x 4 columns]\n",
      "Processing file 89/143: results/graphs/pythia-70m/ioi/98000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001862        True       98000\n",
      "3576    a5.h5->logits  0.001663        True       98000\n",
      "3577    a5.h6->logits -0.000931        True       98000\n",
      "3578    a5.h7->logits -0.000671        True       98000\n",
      "3579       m5->logits  0.004242        True       98000\n",
      "\n",
      "[318620 rows x 4 columns]\n",
      "Processing file 90/143: results/graphs/pythia-70m/ioi/110000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000790        True      110000\n",
      "3576    a5.h5->logits  0.000500        True      110000\n",
      "3577    a5.h6->logits -0.000698       False      110000\n",
      "3578    a5.h7->logits -0.001045        True      110000\n",
      "3579       m5->logits  0.003006        True      110000\n",
      "\n",
      "[322200 rows x 4 columns]\n",
      "Processing file 91/143: results/graphs/pythia-70m/ioi/88000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000984        True       88000\n",
      "3576    a5.h5->logits  0.000637        True       88000\n",
      "3577    a5.h6->logits -0.000288       False       88000\n",
      "3578    a5.h7->logits -0.001068        True       88000\n",
      "3579       m5->logits  0.006805        True       88000\n",
      "\n",
      "[325780 rows x 4 columns]\n",
      "Processing file 92/143: results/graphs/pythia-70m/ioi/16000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000153       False       16000\n",
      "3576    a5.h5->logits  0.003815        True       16000\n",
      "3577    a5.h6->logits -0.000393       False       16000\n",
      "3578    a5.h7->logits  0.000618       False       16000\n",
      "3579       m5->logits  0.006317       False       16000\n",
      "\n",
      "[329360 rows x 4 columns]\n",
      "Processing file 93/143: results/graphs/pythia-70m/ioi/23000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000064       False       23000\n",
      "3576    a5.h5->logits  0.003815       False       23000\n",
      "3577    a5.h6->logits -0.000431       False       23000\n",
      "3578    a5.h7->logits  0.000328       False       23000\n",
      "3579       m5->logits  0.008911        True       23000\n",
      "\n",
      "[332940 rows x 4 columns]\n",
      "Processing file 94/143: results/graphs/pythia-70m/ioi/109000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001938       False      109000\n",
      "3576    a5.h5->logits  0.000782        True      109000\n",
      "3577    a5.h6->logits -0.000477       False      109000\n",
      "3578    a5.h7->logits -0.001877        True      109000\n",
      "3579       m5->logits -0.003418        True      109000\n",
      "\n",
      "[336520 rows x 4 columns]\n",
      "Processing file 95/143: results/graphs/pythia-70m/ioi/22000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000184       False       22000\n",
      "3576    a5.h5->logits  0.003387       False       22000\n",
      "3577    a5.h6->logits -0.000360       False       22000\n",
      "3578    a5.h7->logits  0.000751       False       22000\n",
      "3579       m5->logits  0.009399       False       22000\n",
      "\n",
      "[340100 rows x 4 columns]\n",
      "Processing file 96/143: results/graphs/pythia-70m/ioi/17000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000116       False       17000\n",
      "3576    a5.h5->logits  0.003067       False       17000\n",
      "3577    a5.h6->logits -0.000534       False       17000\n",
      "3578    a5.h7->logits  0.000984       False       17000\n",
      "3579       m5->logits  0.005707       False       17000\n",
      "\n",
      "[343680 rows x 4 columns]\n",
      "Processing file 97/143: results/graphs/pythia-70m/ioi/73000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000332       False       73000\n",
      "3576    a5.h5->logits  0.001198       False       73000\n",
      "3577    a5.h6->logits -0.000128       False       73000\n",
      "3578    a5.h7->logits -0.000694       False       73000\n",
      "3579       m5->logits  0.013916       False       73000\n",
      "\n",
      "[347260 rows x 4 columns]\n",
      "Processing file 98/143: results/graphs/pythia-70m/ioi/3000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001320       False        3000\n",
      "3576    a5.h5->logits  0.002014        True        3000\n",
      "3577    a5.h6->logits -0.000097       False        3000\n",
      "3578    a5.h7->logits -0.002945        True        3000\n",
      "3579       m5->logits -0.007202        True        3000\n",
      "\n",
      "[350840 rows x 4 columns]\n",
      "Processing file 99/143: results/graphs/pythia-70m/ioi/71000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000418       False       71000\n",
      "3576    a5.h5->logits  0.000927       False       71000\n",
      "3577    a5.h6->logits -0.000374       False       71000\n",
      "3578    a5.h7->logits  0.000380       False       71000\n",
      "3579       m5->logits  0.024048       False       71000\n",
      "\n",
      "[354420 rows x 4 columns]\n",
      "Processing file 100/143: results/graphs/pythia-70m/ioi/125000.json\n",
      "                 edge         score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  1.869202e-03       False       57000\n",
      "1     input->a0.h0<k>  4.444122e-04       False       57000\n",
      "2     input->a0.h0<v>  2.117157e-04       False       57000\n",
      "3     input->a0.h1<q>  3.585815e-04       False       57000\n",
      "4     input->a0.h1<k> -3.623962e-04       False       57000\n",
      "...               ...           ...         ...         ...\n",
      "3575    a5.h4->logits -1.846313e-03       False      125000\n",
      "3576    a5.h5->logits  1.121521e-03        True      125000\n",
      "3577    a5.h6->logits -4.079193e-07       False      125000\n",
      "3578    a5.h7->logits -6.942749e-04        True      125000\n",
      "3579       m5->logits  6.370544e-04        True      125000\n",
      "\n",
      "[358000 rows x 4 columns]\n",
      "Processing file 101/143: results/graphs/pythia-70m/ioi/84000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.002884        True       84000\n",
      "3576    a5.h5->logits  0.001328        True       84000\n",
      "3577    a5.h6->logits -0.000242       False       84000\n",
      "3578    a5.h7->logits -0.000366       False       84000\n",
      "3579       m5->logits  0.008667        True       84000\n",
      "\n",
      "[361580 rows x 4 columns]\n",
      "Processing file 102/143: results/graphs/pythia-70m/ioi/87000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000467       False       87000\n",
      "3576    a5.h5->logits  0.001167       False       87000\n",
      "3577    a5.h6->logits  0.000095       False       87000\n",
      "3578    a5.h7->logits -0.000622       False       87000\n",
      "3579       m5->logits  0.011658       False       87000\n",
      "\n",
      "[365160 rows x 4 columns]\n",
      "Processing file 103/143: results/graphs/pythia-70m/ioi/131000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001228       False      131000\n",
      "3576    a5.h5->logits  0.001114       False      131000\n",
      "3577    a5.h6->logits -0.000439       False      131000\n",
      "3578    a5.h7->logits -0.001244       False      131000\n",
      "3579       m5->logits  0.002670       False      131000\n",
      "\n",
      "[368740 rows x 4 columns]\n",
      "Processing file 104/143: results/graphs/pythia-70m/ioi/24000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000210       False       24000\n",
      "3576    a5.h5->logits  0.003372       False       24000\n",
      "3577    a5.h6->logits -0.000530       False       24000\n",
      "3578    a5.h7->logits  0.000450       False       24000\n",
      "3579       m5->logits  0.009583        True       24000\n",
      "\n",
      "[372320 rows x 4 columns]\n",
      "Processing file 105/143: results/graphs/pythia-70m/ioi/128000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000656       False      128000\n",
      "3576    a5.h5->logits  0.001106       False      128000\n",
      "3577    a5.h6->logits -0.003418       False      128000\n",
      "3578    a5.h7->logits -0.000456       False      128000\n",
      "3579       m5->logits  0.000003       False      128000\n",
      "\n",
      "[375900 rows x 4 columns]\n",
      "Processing file 106/143: results/graphs/pythia-70m/ioi/51000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000240       False       51000\n",
      "3576    a5.h5->logits  0.002167       False       51000\n",
      "3577    a5.h6->logits -0.000199       False       51000\n",
      "3578    a5.h7->logits  0.000435       False       51000\n",
      "3579       m5->logits  0.014099       False       51000\n",
      "\n",
      "[379480 rows x 4 columns]\n",
      "Processing file 107/143: results/graphs/pythia-70m/ioi/52000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000282       False       52000\n",
      "3576    a5.h5->logits  0.002350       False       52000\n",
      "3577    a5.h6->logits -0.000031       False       52000\n",
      "3578    a5.h7->logits  0.001137       False       52000\n",
      "3579       m5->logits  0.013672       False       52000\n",
      "\n",
      "[383060 rows x 4 columns]\n",
      "Processing file 108/143: results/graphs/pythia-70m/ioi/132000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001488       False      132000\n",
      "3576    a5.h5->logits  0.000782       False      132000\n",
      "3577    a5.h6->logits -0.001572       False      132000\n",
      "3578    a5.h7->logits -0.001259       False      132000\n",
      "3579       m5->logits  0.001518       False      132000\n",
      "\n",
      "[386640 rows x 4 columns]\n",
      "Processing file 109/143: results/graphs/pythia-70m/ioi/101000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000904       False      101000\n",
      "3576    a5.h5->logits  0.000999       False      101000\n",
      "3577    a5.h6->logits -0.000935       False      101000\n",
      "3578    a5.h7->logits -0.001022       False      101000\n",
      "3579       m5->logits  0.001686        True      101000\n",
      "\n",
      "[390220 rows x 4 columns]\n",
      "Processing file 110/143: results/graphs/pythia-70m/ioi/89000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.002533        True       89000\n",
      "3576    a5.h5->logits -0.000090       False       89000\n",
      "3577    a5.h6->logits -0.002518        True       89000\n",
      "3578    a5.h7->logits -0.000950        True       89000\n",
      "3579       m5->logits  0.003082        True       89000\n",
      "\n",
      "[393800 rows x 4 columns]\n",
      "Processing file 111/143: results/graphs/pythia-70m/ioi/40000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000060       False       40000\n",
      "3576    a5.h5->logits  0.002121       False       40000\n",
      "3577    a5.h6->logits -0.000162       False       40000\n",
      "3578    a5.h7->logits  0.001205       False       40000\n",
      "3579       m5->logits  0.012756        True       40000\n",
      "\n",
      "[397380 rows x 4 columns]\n",
      "Processing file 112/143: results/graphs/pythia-70m/ioi/43000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000018       False       43000\n",
      "3576    a5.h5->logits  0.002167       False       43000\n",
      "3577    a5.h6->logits -0.000254       False       43000\n",
      "3578    a5.h7->logits  0.001511       False       43000\n",
      "3579       m5->logits  0.013245       False       43000\n",
      "\n",
      "[400960 rows x 4 columns]\n",
      "Processing file 113/143: results/graphs/pythia-70m/ioi/54000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000056       False       54000\n",
      "3576    a5.h5->logits  0.002060       False       54000\n",
      "3577    a5.h6->logits -0.000057       False       54000\n",
      "3578    a5.h7->logits  0.001022       False       54000\n",
      "3579       m5->logits  0.018066       False       54000\n",
      "\n",
      "[404540 rows x 4 columns]\n",
      "Processing file 114/143: results/graphs/pythia-70m/ioi/11000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000372       False       11000\n",
      "3576    a5.h5->logits  0.004364       False       11000\n",
      "3577    a5.h6->logits -0.000233       False       11000\n",
      "3578    a5.h7->logits  0.000229       False       11000\n",
      "3579       m5->logits  0.004425       False       11000\n",
      "\n",
      "[408120 rows x 4 columns]\n",
      "Processing file 115/143: results/graphs/pythia-70m/ioi/36000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000423       False       36000\n",
      "3576    a5.h5->logits  0.002380       False       36000\n",
      "3577    a5.h6->logits -0.000252       False       36000\n",
      "3578    a5.h7->logits  0.001823       False       36000\n",
      "3579       m5->logits  0.010559       False       36000\n",
      "\n",
      "[411700 rows x 4 columns]\n",
      "Processing file 116/143: results/graphs/pythia-70m/ioi/45000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000889       False       45000\n",
      "3576    a5.h5->logits  0.002472       False       45000\n",
      "3577    a5.h6->logits -0.000256       False       45000\n",
      "3578    a5.h7->logits  0.001396       False       45000\n",
      "3579       m5->logits  0.014038       False       45000\n",
      "\n",
      "[415280 rows x 4 columns]\n",
      "Processing file 117/143: results/graphs/pythia-70m/ioi/50000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000889       False       50000\n",
      "3576    a5.h5->logits  0.002487       False       50000\n",
      "3577    a5.h6->logits -0.000182       False       50000\n",
      "3578    a5.h7->logits  0.002106       False       50000\n",
      "3579       m5->logits  0.018799        True       50000\n",
      "\n",
      "[418860 rows x 4 columns]\n",
      "Processing file 118/143: results/graphs/pythia-70m/ioi/114000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001602       False      114000\n",
      "3576    a5.h5->logits  0.000360       False      114000\n",
      "3577    a5.h6->logits -0.002899       False      114000\n",
      "3578    a5.h7->logits -0.001015       False      114000\n",
      "3579       m5->logits  0.000401       False      114000\n",
      "\n",
      "[422440 rows x 4 columns]\n",
      "Processing file 119/143: results/graphs/pythia-70m/ioi/66000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000942       False       66000\n",
      "3576    a5.h5->logits  0.001785       False       66000\n",
      "3577    a5.h6->logits -0.000013       False       66000\n",
      "3578    a5.h7->logits  0.000584       False       66000\n",
      "3579       m5->logits  0.017944       False       66000\n",
      "\n",
      "[426020 rows x 4 columns]\n",
      "Processing file 120/143: results/graphs/pythia-70m/ioi/28000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000565       False       28000\n",
      "3576    a5.h5->logits  0.003204       False       28000\n",
      "3577    a5.h6->logits -0.000309       False       28000\n",
      "3578    a5.h7->logits  0.000294       False       28000\n",
      "3579       m5->logits  0.009827       False       28000\n",
      "\n",
      "[429600 rows x 4 columns]\n",
      "Processing file 121/143: results/graphs/pythia-70m/ioi/82000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000332       False       82000\n",
      "3576    a5.h5->logits  0.001450       False       82000\n",
      "3577    a5.h6->logits -0.000935       False       82000\n",
      "3578    a5.h7->logits -0.000881       False       82000\n",
      "3579       m5->logits  0.012512        True       82000\n",
      "\n",
      "[433180 rows x 4 columns]\n",
      "Processing file 122/143: results/graphs/pythia-70m/ioi/127000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000759       False      127000\n",
      "3576    a5.h5->logits  0.000633       False      127000\n",
      "3577    a5.h6->logits -0.001480       False      127000\n",
      "3578    a5.h7->logits -0.000954       False      127000\n",
      "3579       m5->logits -0.000668        True      127000\n",
      "\n",
      "[436760 rows x 4 columns]\n",
      "Processing file 123/143: results/graphs/pythia-70m/ioi/14000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000402       False       14000\n",
      "3576    a5.h5->logits  0.003738       False       14000\n",
      "3577    a5.h6->logits -0.000223       False       14000\n",
      "3578    a5.h7->logits  0.000923       False       14000\n",
      "3579       m5->logits  0.005676        True       14000\n",
      "\n",
      "[440340 rows x 4 columns]\n",
      "Processing file 124/143: results/graphs/pythia-70m/ioi/75000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001175        True       75000\n",
      "3576    a5.h5->logits  0.001907        True       75000\n",
      "3577    a5.h6->logits -0.000324        True       75000\n",
      "3578    a5.h7->logits -0.000387        True       75000\n",
      "3579       m5->logits  0.014587        True       75000\n",
      "\n",
      "[443920 rows x 4 columns]\n",
      "Processing file 125/143: results/graphs/pythia-70m/ioi/115000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001808        True      115000\n",
      "3576    a5.h5->logits  0.001488        True      115000\n",
      "3577    a5.h6->logits -0.001556        True      115000\n",
      "3578    a5.h7->logits -0.003021        True      115000\n",
      "3579       m5->logits -0.001862        True      115000\n",
      "\n",
      "[447500 rows x 4 columns]\n",
      "Processing file 126/143: results/graphs/pythia-70m/ioi/12000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000334       False       12000\n",
      "3576    a5.h5->logits  0.005371       False       12000\n",
      "3577    a5.h6->logits -0.000240       False       12000\n",
      "3578    a5.h7->logits  0.000345       False       12000\n",
      "3579       m5->logits  0.004578       False       12000\n",
      "\n",
      "[451080 rows x 4 columns]\n",
      "Processing file 127/143: results/graphs/pythia-70m/ioi/21000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000004       False       21000\n",
      "3576    a5.h5->logits  0.003311       False       21000\n",
      "3577    a5.h6->logits -0.000538       False       21000\n",
      "3578    a5.h7->logits  0.000370       False       21000\n",
      "3579       m5->logits  0.007629       False       21000\n",
      "\n",
      "[454660 rows x 4 columns]\n",
      "Processing file 128/143: results/graphs/pythia-70m/ioi/38000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000404       False       38000\n",
      "3576    a5.h5->logits  0.002914       False       38000\n",
      "3577    a5.h6->logits -0.000355       False       38000\n",
      "3578    a5.h7->logits  0.001846       False       38000\n",
      "3579       m5->logits  0.016113        True       38000\n",
      "\n",
      "[458240 rows x 4 columns]\n",
      "Processing file 129/143: results/graphs/pythia-70m/ioi/15000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000277       False       15000\n",
      "3576    a5.h5->logits  0.002670       False       15000\n",
      "3577    a5.h6->logits -0.000395       False       15000\n",
      "3578    a5.h7->logits  0.000414       False       15000\n",
      "3579       m5->logits  0.005981        True       15000\n",
      "\n",
      "[461820 rows x 4 columns]\n",
      "Processing file 130/143: results/graphs/pythia-70m/ioi/85000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.002731       False       85000\n",
      "3576    a5.h5->logits  0.001640       False       85000\n",
      "3577    a5.h6->logits -0.000393       False       85000\n",
      "3578    a5.h7->logits -0.000031       False       85000\n",
      "3579       m5->logits  0.010071       False       85000\n",
      "\n",
      "[465400 rows x 4 columns]\n",
      "Processing file 131/143: results/graphs/pythia-70m/ioi/139000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001129        True      139000\n",
      "3576    a5.h5->logits  0.000553        True      139000\n",
      "3577    a5.h6->logits -0.004028        True      139000\n",
      "3578    a5.h7->logits -0.002777        True      139000\n",
      "3579       m5->logits -0.003342        True      139000\n",
      "\n",
      "[468980 rows x 4 columns]\n",
      "Processing file 132/143: results/graphs/pythia-70m/ioi/61000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001450       False       61000\n",
      "3576    a5.h5->logits  0.001366       False       61000\n",
      "3577    a5.h6->logits -0.000086       False       61000\n",
      "3578    a5.h7->logits  0.002045       False       61000\n",
      "3579       m5->logits  0.018799       False       61000\n",
      "\n",
      "[472560 rows x 4 columns]\n",
      "Processing file 133/143: results/graphs/pythia-70m/ioi/7000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.001129       False        7000\n",
      "3576    a5.h5->logits  0.005859        True        7000\n",
      "3577    a5.h6->logits -0.000214       False        7000\n",
      "3578    a5.h7->logits -0.001091       False        7000\n",
      "3579       m5->logits  0.000546       False        7000\n",
      "\n",
      "[476140 rows x 4 columns]\n",
      "Processing file 134/143: results/graphs/pythia-70m/ioi/68000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000223       False       68000\n",
      "3576    a5.h5->logits  0.001778       False       68000\n",
      "3577    a5.h6->logits -0.000177       False       68000\n",
      "3578    a5.h7->logits  0.001259       False       68000\n",
      "3579       m5->logits  0.028687       False       68000\n",
      "\n",
      "[479720 rows x 4 columns]\n",
      "Processing file 135/143: results/graphs/pythia-70m/ioi/53000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000089       False       53000\n",
      "3576    a5.h5->logits  0.002106       False       53000\n",
      "3577    a5.h6->logits -0.000181       False       53000\n",
      "3578    a5.h7->logits  0.000954       False       53000\n",
      "3579       m5->logits  0.012573       False       53000\n",
      "\n",
      "[483300 rows x 4 columns]\n",
      "Processing file 136/143: results/graphs/pythia-70m/ioi/25000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000256       False       25000\n",
      "3576    a5.h5->logits  0.004822       False       25000\n",
      "3577    a5.h6->logits -0.000584       False       25000\n",
      "3578    a5.h7->logits  0.000227       False       25000\n",
      "3579       m5->logits  0.011780       False       25000\n",
      "\n",
      "[486880 rows x 4 columns]\n",
      "Processing file 137/143: results/graphs/pythia-70m/ioi/97000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.001740        True       97000\n",
      "3576    a5.h5->logits  0.000261       False       97000\n",
      "3577    a5.h6->logits  0.000240       False       97000\n",
      "3578    a5.h7->logits -0.001457        True       97000\n",
      "3579       m5->logits  0.000710        True       97000\n",
      "\n",
      "[490460 rows x 4 columns]\n",
      "Processing file 138/143: results/graphs/pythia-70m/ioi/4000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000618       False        4000\n",
      "3576    a5.h5->logits  0.003815        True        4000\n",
      "3577    a5.h6->logits -0.000200       False        4000\n",
      "3578    a5.h7->logits -0.002777        True        4000\n",
      "3579       m5->logits -0.006744        True        4000\n",
      "\n",
      "[494040 rows x 4 columns]\n",
      "Processing file 139/143: results/graphs/pythia-70m/ioi/123000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000637        True      123000\n",
      "3576    a5.h5->logits  0.000969        True      123000\n",
      "3577    a5.h6->logits -0.000298       False      123000\n",
      "3578    a5.h7->logits -0.001305        True      123000\n",
      "3579       m5->logits -0.001938        True      123000\n",
      "\n",
      "[497620 rows x 4 columns]\n",
      "Processing file 140/143: results/graphs/pythia-70m/ioi/20000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits  0.000050       False       20000\n",
      "3576    a5.h5->logits  0.003815       False       20000\n",
      "3577    a5.h6->logits -0.000427       False       20000\n",
      "3578    a5.h7->logits  0.000193       False       20000\n",
      "3579       m5->logits  0.008789       False       20000\n",
      "\n",
      "[501200 rows x 4 columns]\n",
      "Processing file 141/143: results/graphs/pythia-70m/ioi/41000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.000404       False       41000\n",
      "3576    a5.h5->logits  0.002197       False       41000\n",
      "3577    a5.h6->logits -0.000096       False       41000\n",
      "3578    a5.h7->logits  0.001732       False       41000\n",
      "3579       m5->logits  0.017090       False       41000\n",
      "\n",
      "[504780 rows x 4 columns]\n",
      "Processing file 142/143: results/graphs/pythia-70m/ioi/126000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.002167        True      126000\n",
      "3576    a5.h5->logits  0.000168       False      126000\n",
      "3577    a5.h6->logits -0.003113        True      126000\n",
      "3578    a5.h7->logits -0.000481       False      126000\n",
      "3579       m5->logits -0.000175       False      126000\n",
      "\n",
      "[508360 rows x 4 columns]\n",
      "Processing file 143/143: results/graphs/pythia-70m/ioi/92000.json\n",
      "                 edge     score  in_circuit  checkpoint\n",
      "0     input->a0.h0<q>  0.001869       False       57000\n",
      "1     input->a0.h0<k>  0.000444       False       57000\n",
      "2     input->a0.h0<v>  0.000212       False       57000\n",
      "3     input->a0.h1<q>  0.000359       False       57000\n",
      "4     input->a0.h1<k> -0.000362       False       57000\n",
      "...               ...       ...         ...         ...\n",
      "3575    a5.h4->logits -0.002106        True       92000\n",
      "3576    a5.h5->logits  0.001236        True       92000\n",
      "3577    a5.h6->logits  0.000877        True       92000\n",
      "3578    a5.h7->logits -0.001343        True       92000\n",
      "3579       m5->logits  0.002945        True       92000\n",
      "\n",
      "[511940 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = f'results/graphs/{MODEL_SHORTNAME}/{TASK}'\n",
    "df = load_edge_scores_into_dictionary(folder_path)\n",
    "\n",
    "# filter everything before 1000 steps\n",
    "df = df[df['checkpoint'] >= 1000]\n",
    "\n",
    "df[['source', 'target']] = df['edge'].str.split('->', expand=True)\n",
    "len(df['target'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eeffeddbcc84e30aadc7a06a15339c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4f1a792ccb49579456d4f25d647e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c6406687824b41826761465d2e55ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "initial_model = load_model(BASE_MODEL, VARIANT, 143000, CACHE, device)\n",
    "size=70\n",
    "ioi_dataset, abc_dataset = generate_data_and_caches(initial_model, size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imshow_p(\n",
    "#     per_head_ablated_logit_diffs,\n",
    "#     title=\"Headwise logit diff contribution, post NMH KO\",\n",
    "#     labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Logit diff attribution\"},\n",
    "#     #coloraxis=dict(colorbar_ticksuffix = \"%\"),\n",
    "#     border=True,\n",
    "#     width=600,\n",
    "#     margin={\"r\": 100, \"l\": 100}\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 3.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 18.571428571428573%\n",
      "Checkpoint 4000:\n",
      "Heads ablated:            [(4, 6), (3, 6)]\n",
      "Original logit diff:      -0.8775387406\n",
      "Post ablation logit diff: -0.5249566436\n",
      "Logit diff % change:      -40.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 7.142857142857142%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 30.476190476190478%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 38.095238095238095%\n",
      "Checkpoint 5000:\n",
      "Heads ablated:            [(4, 6)]\n",
      "Original logit diff:      -0.9491387606\n",
      "Post ablation logit diff: -0.2954781055\n",
      "Logit diff % change:      -68.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 57.61904761904761%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 50.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 16.19047619047619%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 89.04761904761904%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 6000:\n",
      "Heads ablated:            [(4, 6), (3, 1)]\n",
      "Original logit diff:      -0.8585914969\n",
      "Post ablation logit diff: -0.2592725456\n",
      "Logit diff % change:      -69.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 54.285714285714285%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 27.61904761904762%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 59.523809523809526%\n",
      "Checkpoint 7000:\n",
      "Heads ablated:            [(4, 6)]\n",
      "Original logit diff:      -0.2771649659\n",
      "Post ablation logit diff: 0.0070780516\n",
      "Logit diff % change:      -102.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 8000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.0115882810\n",
      "Post ablation logit diff: 0.0115882810\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 9000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.2201550752\n",
      "Post ablation logit diff: 0.2201550752\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 10000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.0404976159\n",
      "Post ablation logit diff: 0.0404976159\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 11000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.3416843414\n",
      "Post ablation logit diff: 0.3416843414\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 12000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.4083966017\n",
      "Post ablation logit diff: 0.4083966017\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 13000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.3803787529\n",
      "Post ablation logit diff: 0.3803787529\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 52.85714285714286%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 14000:\n",
      "Heads ablated:            [(3, 5), (4, 6)]\n",
      "Original logit diff:      0.6185692549\n",
      "Post ablation logit diff: 1.0815589428\n",
      "Logit diff % change:      74.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 57.14285714285714%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 15000:\n",
      "Heads ablated:            [(3, 5), (4, 6)]\n",
      "Original logit diff:      0.6759502888\n",
      "Post ablation logit diff: 1.0873620510\n",
      "Logit diff % change:      60.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 58.0952380952381%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 90.95238095238095%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 60.0%\n",
      "Checkpoint 16000:\n",
      "Heads ablated:            [(4, 6), (3, 5)]\n",
      "Original logit diff:      0.7325052619\n",
      "Post ablation logit diff: 1.1954573393\n",
      "Logit diff % change:      63.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 17000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.7017971873\n",
      "Post ablation logit diff: 0.7017971873\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 57.61904761904761%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 61.904761904761905%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 18000:\n",
      "Heads ablated:            [(3, 5), (4, 6)]\n",
      "Original logit diff:      0.7491797209\n",
      "Post ablation logit diff: 1.3264752626\n",
      "Logit diff % change:      77.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 65.71428571428571%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 19000:\n",
      "Heads ablated:            [(3, 5), (4, 6)]\n",
      "Original logit diff:      0.7100559473\n",
      "Post ablation logit diff: 1.2574195862\n",
      "Logit diff % change:      77.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 20000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.7370791435\n",
      "Post ablation logit diff: 0.7370791435\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 21000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.8281600475\n",
      "Post ablation logit diff: 0.8281600475\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 22000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.8246081471\n",
      "Post ablation logit diff: 0.8246081471\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 96.66666666666667%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 73.80952380952381%\n",
      "Checkpoint 23000:\n",
      "Heads ablated:            [(4, 6), (3, 5)]\n",
      "Original logit diff:      1.0021547079\n",
      "Post ablation logit diff: 1.4895738363\n",
      "Logit diff % change:      48.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 68.0952380952381%\n",
      "Checkpoint 24000:\n",
      "Heads ablated:            [(3, 5), (4, 6)]\n",
      "Original logit diff:      0.5890020132\n",
      "Post ablation logit diff: 1.0051034689\n",
      "Logit diff % change:      70.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 25000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.7022525072\n",
      "Post ablation logit diff: 0.7022525072\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 26000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.4500888288\n",
      "Post ablation logit diff: 0.4500888288\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 32.857142857142854%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 27000:\n",
      "Heads ablated:            [(3, 5), (4, 4), (4, 6)]\n",
      "Original logit diff:      0.7189214826\n",
      "Post ablation logit diff: 1.5496541262\n",
      "Logit diff % change:      115.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 28000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.2834420502\n",
      "Post ablation logit diff: 0.2834420502\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 77.14285714285715%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 99.52380952380952%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 29000:\n",
      "Heads ablated:            [(4, 4), (3, 5), (4, 6)]\n",
      "Original logit diff:      0.6070242524\n",
      "Post ablation logit diff: 1.6267482042\n",
      "Logit diff % change:      167.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 81.42857142857143%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 30000:\n",
      "Heads ablated:            [(4, 4), (4, 6)]\n",
      "Original logit diff:      1.2188258171\n",
      "Post ablation logit diff: 2.4047615528\n",
      "Logit diff % change:      97.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 76.66666666666667%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 31000:\n",
      "Heads ablated:            [(4, 4), (4, 6)]\n",
      "Original logit diff:      0.9041007161\n",
      "Post ablation logit diff: 1.9113290310\n",
      "Logit diff % change:      111.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 32000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.7107272148\n",
      "Post ablation logit diff: 0.7107272148\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Checkpoint 33000:\n",
      "Heads ablated:            [(4, 6), (4, 4)]\n",
      "Original logit diff:      0.7348417044\n",
      "Post ablation logit diff: 1.8796205521\n",
      "Logit diff % change:      155.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 34000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.7538008690\n",
      "Post ablation logit diff: 0.7538008690\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 35000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.6723266840\n",
      "Post ablation logit diff: 0.6723266840\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 36000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.8214648962\n",
      "Post ablation logit diff: 0.8214648962\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 37000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.9073672891\n",
      "Post ablation logit diff: 0.9073672891\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 38000:\n",
      "Heads ablated:            [(4, 6), (4, 4), (3, 5)]\n",
      "Original logit diff:      1.0821753740\n",
      "Post ablation logit diff: 2.2819392681\n",
      "Logit diff % change:      110.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 39000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.5315364599\n",
      "Post ablation logit diff: 0.5315364599\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 81.42857142857143%\n",
      "Checkpoint 40000:\n",
      "Heads ablated:            [(4, 6), (4, 4)]\n",
      "Original logit diff:      0.7468996644\n",
      "Post ablation logit diff: 2.1283068657\n",
      "Logit diff % change:      184.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 41000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.6730849743\n",
      "Post ablation logit diff: 0.6730849743\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Checkpoint 42000:\n",
      "Heads ablated:            [(4, 6), (4, 4)]\n",
      "Original logit diff:      0.7323057652\n",
      "Post ablation logit diff: 2.0341403484\n",
      "Logit diff % change:      177.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 43000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.4700644314\n",
      "Post ablation logit diff: 0.4700644314\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 44000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.4418857098\n",
      "Post ablation logit diff: 0.4418857098\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 45000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.4903239608\n",
      "Post ablation logit diff: 0.4903239608\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 46000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.6233853102\n",
      "Post ablation logit diff: 0.6233853102\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 47000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.7634024620\n",
      "Post ablation logit diff: 0.7634024620\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 48000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.1296335012\n",
      "Post ablation logit diff: 0.1296335012\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 49000:\n",
      "Heads ablated:            [(4, 4), (4, 6)]\n",
      "Original logit diff:      0.9950267076\n",
      "Post ablation logit diff: 2.9128465652\n",
      "Logit diff % change:      192.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Checkpoint 50000:\n",
      "Heads ablated:            [(4, 6), (4, 4)]\n",
      "Original logit diff:      1.1135458946\n",
      "Post ablation logit diff: 2.7943894863\n",
      "Logit diff % change:      150.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 51000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.3575604558\n",
      "Post ablation logit diff: 0.3575604558\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 52000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.2349558175\n",
      "Post ablation logit diff: 0.2349558175\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 53000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.6085900664\n",
      "Post ablation logit diff: 0.6085900664\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 54000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.4084658921\n",
      "Post ablation logit diff: 0.4084658921\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 55000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.4217540324\n",
      "Post ablation logit diff: 0.4217540324\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 56000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.4323191643\n",
      "Post ablation logit diff: 0.4323191643\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 57000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.6403176188\n",
      "Post ablation logit diff: 0.6403176188\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 58000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.5754644871\n",
      "Post ablation logit diff: 0.5754644871\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 59000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      1.0096291304\n",
      "Post ablation logit diff: 1.0096291304\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 60000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.6222140789\n",
      "Post ablation logit diff: 0.6222140789\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 61000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.7324095368\n",
      "Post ablation logit diff: 0.7324095368\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Checkpoint 62000:\n",
      "Heads ablated:            [(3, 5), (4, 4), (4, 6)]\n",
      "Original logit diff:      0.6263018250\n",
      "Post ablation logit diff: 2.2275896072\n",
      "Logit diff % change:      255.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 4.0 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 36.19047619047619%\n",
      "Checkpoint 63000:\n",
      "Heads ablated:            [(4, 6), (4, 4)]\n",
      "Original logit diff:      0.3326402009\n",
      "Post ablation logit diff: 2.4476330280\n",
      "Logit diff % change:      635.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 64000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.7449644208\n",
      "Post ablation logit diff: 0.7449644208\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 99.52380952380952%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 39.04761904761905%\n",
      "Checkpoint 65000:\n",
      "Heads ablated:            [(4, 6), (4, 4)]\n",
      "Original logit diff:      0.5483941436\n",
      "Post ablation logit diff: 2.4920144081\n",
      "Logit diff % change:      354.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 66000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.2861988246\n",
      "Post ablation logit diff: 0.2861988246\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 67000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.5615631938\n",
      "Post ablation logit diff: 0.5615631938\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 68000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      1.0039848089\n",
      "Post ablation logit diff: 1.0039848089\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 69000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.7860768437\n",
      "Post ablation logit diff: 0.7860768437\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 70000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.6147008538\n",
      "Post ablation logit diff: 0.6147008538\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 71000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.6746646762\n",
      "Post ablation logit diff: 0.6746646762\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 72000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.9760091305\n",
      "Post ablation logit diff: 0.9760091305\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 73000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.3267331719\n",
      "Post ablation logit diff: 0.3267331719\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 74000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.7971808314\n",
      "Post ablation logit diff: 0.7971808314\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.0 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 64.28571428571429%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 4.0 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Checkpoint 75000:\n",
      "Heads ablated:            [(4, 6), (4, 4), (3, 5), (3, 3)]\n",
      "Original logit diff:      0.1481638998\n",
      "Post ablation logit diff: 0.8562752008\n",
      "Logit diff % change:      477.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 15.238095238095239%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 4.0 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 3.6 (sign=1) : Top 5 accuracy: 7.142857142857142%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 59.523809523809526%\n",
      "Copy circuit for head 3.0 (sign=1) : Top 5 accuracy: 9.047619047619047%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 76000:\n",
      "Heads ablated:            [(4, 6), (4, 4), (3, 5), (3, 3)]\n",
      "Original logit diff:      0.2394576818\n",
      "Post ablation logit diff: 0.9086235166\n",
      "Logit diff % change:      279.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 64.76190476190476%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 3.6 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 4.0 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 4.2 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 20.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Checkpoint 77000:\n",
      "Heads ablated:            [(3, 3), (3, 5), (4, 6), (4, 4)]\n",
      "Original logit diff:      0.4678331614\n",
      "Post ablation logit diff: 0.9344913960\n",
      "Logit diff % change:      99.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 78000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.5213122368\n",
      "Post ablation logit diff: 0.5213122368\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 26.666666666666668%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 4.2 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 4.0 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 3.6 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 71.9047619047619%\n",
      "Copy circuit for head 3.0 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 98.57142857142858%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 79000:\n",
      "Heads ablated:            [(4, 6), (4, 4), (3, 3), (3, 5)]\n",
      "Original logit diff:      0.2219315469\n",
      "Post ablation logit diff: 0.9247933626\n",
      "Logit diff % change:      316.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 80000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.6110703945\n",
      "Post ablation logit diff: 0.6110703945\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 81000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.3440084159\n",
      "Post ablation logit diff: 0.3440084159\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 98.57142857142858%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Checkpoint 82000:\n",
      "Heads ablated:            [(3, 3), (3, 5), (4, 6), (4, 4)]\n",
      "Original logit diff:      -0.2331327200\n",
      "Post ablation logit diff: 0.3311641216\n",
      "Logit diff % change:      -242.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 83000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.7873736620\n",
      "Post ablation logit diff: 0.7873736620\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.2 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Checkpoint 84000:\n",
      "Heads ablated:            [(4, 6), (4, 4), (3, 5), (3, 3)]\n",
      "Original logit diff:      0.8117069602\n",
      "Post ablation logit diff: 0.8107366562\n",
      "Logit diff % change:      -0.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 85000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.7867392302\n",
      "Post ablation logit diff: 0.7867392302\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 15.714285714285714%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 4.2 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 3.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 56.666666666666664%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 86000:\n",
      "Heads ablated:            [(4, 6), (4, 4), (3, 5), (3, 3)]\n",
      "Original logit diff:      0.4473558962\n",
      "Post ablation logit diff: 0.8085982203\n",
      "Logit diff % change:      80.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 87000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.5273746848\n",
      "Post ablation logit diff: 0.5273746848\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 3.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 61.904761904761905%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 88000:\n",
      "Heads ablated:            [(4, 6), (4, 4), (3, 5), (3, 3)]\n",
      "Original logit diff:      0.4967110157\n",
      "Post ablation logit diff: 0.6719725728\n",
      "Logit diff % change:      35.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 4.2 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 20.952380952380953%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 57.61904761904761%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Checkpoint 89000:\n",
      "Heads ablated:            [(3, 3), (3, 5), (4, 4), (4, 6)]\n",
      "Original logit diff:      0.2750924528\n",
      "Post ablation logit diff: 0.3040034473\n",
      "Logit diff % change:      10.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 7.142857142857142%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 90000:\n",
      "Heads ablated:            [(4, 6), (4, 4), (3, 5), (3, 3)]\n",
      "Original logit diff:      0.0590756014\n",
      "Post ablation logit diff: 0.2464233190\n",
      "Logit diff % change:      317.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 63.8095238095238%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 6.666666666666667%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Checkpoint 91000:\n",
      "Heads ablated:            [(3, 3), (3, 5), (4, 4), (4, 6)]\n",
      "Original logit diff:      0.2936813235\n",
      "Post ablation logit diff: 0.3727281392\n",
      "Logit diff % change:      26.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 47.61904761904761%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 3.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 3.0 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 92.38095238095238%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Checkpoint 92000:\n",
      "Heads ablated:            [(3, 3), (3, 5), (4, 6), (4, 4)]\n",
      "Original logit diff:      0.2990749776\n",
      "Post ablation logit diff: 0.4748756289\n",
      "Logit diff % change:      58.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 93000:\n",
      "Heads ablated:            [(4, 6), (4, 4), (3, 5), (3, 3)]\n",
      "Original logit diff:      0.2925741076\n",
      "Post ablation logit diff: 0.5181595683\n",
      "Logit diff % change:      77.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 2.7 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 3.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 58.0952380952381%\n",
      "Copy circuit for head 3.0 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Checkpoint 94000:\n",
      "Heads ablated:            [(4, 6), (4, 4), (3, 5), (3, 3)]\n",
      "Original logit diff:      0.6073801517\n",
      "Post ablation logit diff: 0.9710022211\n",
      "Logit diff % change:      59.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 3.0 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 56.19047619047619%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 99.52380952380952%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 95000:\n",
      "Heads ablated:            [(3, 3), (4, 4), (3, 5), (4, 6)]\n",
      "Original logit diff:      0.6306185126\n",
      "Post ablation logit diff: 1.0065962076\n",
      "Logit diff % change:      59.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 52.38095238095239%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 3.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Checkpoint 96000:\n",
      "Heads ablated:            [(4, 6), (3, 5), (3, 3), (4, 4)]\n",
      "Original logit diff:      0.7083563805\n",
      "Post ablation logit diff: 0.7913498878\n",
      "Logit diff % change:      11.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 50.95238095238095%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 79.52380952380952%\n",
      "Copy circuit for head 3.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 3.0 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 79.04761904761905%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Checkpoint 97000:\n",
      "Heads ablated:            [(3, 3), (3, 5), (4, 6), (4, 4)]\n",
      "Original logit diff:      0.3010700345\n",
      "Post ablation logit diff: 0.6712829471\n",
      "Logit diff % change:      122.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 74.28571428571429%\n",
      "Copy circuit for head 3.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 98000:\n",
      "Heads ablated:            [(4, 6), (3, 5), (3, 3)]\n",
      "Original logit diff:      0.2624198496\n",
      "Post ablation logit diff: 0.2571922839\n",
      "Logit diff % change:      -1.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 2.7 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 50.95238095238095%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 3.0 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 3.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 72.38095238095238%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 59.523809523809526%\n",
      "Checkpoint 99000:\n",
      "Heads ablated:            [(4, 6), (3, 3)]\n",
      "Original logit diff:      0.2385216355\n",
      "Post ablation logit diff: 0.9398254156\n",
      "Logit diff % change:      294.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 75.23809523809524%\n",
      "Copy circuit for head 3.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 57.14285714285714%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 3.0 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 47.61904761904761%\n",
      "Copy circuit for head 2.5 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Checkpoint 100000:\n",
      "Heads ablated:            [(3, 5), (3, 3), (4, 6)]\n",
      "Original logit diff:      0.5716239810\n",
      "Post ablation logit diff: 0.5129665732\n",
      "Logit diff % change:      -10.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 61.904761904761905%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 50.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 4.7 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Checkpoint 101000:\n",
      "Heads ablated:            [(3, 3), (4, 6)]\n",
      "Original logit diff:      0.5246245265\n",
      "Post ablation logit diff: 0.9023212790\n",
      "Logit diff % change:      71.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 102000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.3756940365\n",
      "Post ablation logit diff: 0.3756940365\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 45.23809523809524%\n",
      "Copy circuit for head 3.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 25.238095238095237%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 89.52380952380953%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 103000:\n",
      "Heads ablated:            [(3, 3), (4, 6)]\n",
      "Original logit diff:      0.3557770848\n",
      "Post ablation logit diff: 0.7098906636\n",
      "Logit diff % change:      99.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 46.19047619047619%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 3.0 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 104000:\n",
      "Heads ablated:            [(3, 3), (4, 6)]\n",
      "Original logit diff:      0.5528450012\n",
      "Post ablation logit diff: 0.7790927887\n",
      "Logit diff % change:      40.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 105000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.6195412874\n",
      "Post ablation logit diff: 0.6195412874\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 15.714285714285714%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 76.19047619047619%\n",
      "Copy circuit for head 3.0 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Checkpoint 106000:\n",
      "Heads ablated:            [(3, 3), (4, 6)]\n",
      "Original logit diff:      0.5664122701\n",
      "Post ablation logit diff: 0.7221776843\n",
      "Logit diff % change:      27.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 70.47619047619048%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 3.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 32.38095238095238%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 27.61904761904762%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 107000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.6464560628\n",
      "Post ablation logit diff: 0.4299367070\n",
      "Logit diff % change:      -33.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 108000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.2972801626\n",
      "Post ablation logit diff: 0.2972801626\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 51.90476190476191%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 18.571428571428573%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 109000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.4022381604\n",
      "Post ablation logit diff: 0.7478917241\n",
      "Logit diff % change:      85.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 21.904761904761905%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 40.0%\n",
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 110000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.7161034942\n",
      "Post ablation logit diff: 1.0572936535\n",
      "Logit diff % change:      47.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 36.19047619047619%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Checkpoint 111000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.3562529385\n",
      "Post ablation logit diff: 0.5876967311\n",
      "Logit diff % change:      64.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 25.71428571428571%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 4.4 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Checkpoint 112000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.7781585455\n",
      "Post ablation logit diff: 1.0054256916\n",
      "Logit diff % change:      29.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 20.0%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Checkpoint 113000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.2637522519\n",
      "Post ablation logit diff: 0.2178120762\n",
      "Logit diff % change:      -17.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 8.571428571428571%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 114000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.2372829169\n",
      "Post ablation logit diff: 0.2857869864\n",
      "Logit diff % change:      20.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 115000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.7258030772\n",
      "Post ablation logit diff: 1.1025993824\n",
      "Logit diff % change:      51.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 116000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.4999492764\n",
      "Post ablation logit diff: 0.5994606018\n",
      "Logit diff % change:      19.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Checkpoint 117000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.5295794010\n",
      "Post ablation logit diff: 0.5739812255\n",
      "Logit diff % change:      8.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 3.5 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 118000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.6762881875\n",
      "Post ablation logit diff: 0.6997689605\n",
      "Logit diff % change:      3.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 8.095238095238095%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 119000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.4347186089\n",
      "Post ablation logit diff: 0.5855047107\n",
      "Logit diff % change:      34.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 120000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.6068837047\n",
      "Post ablation logit diff: 0.9287611842\n",
      "Logit diff % change:      53.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 8.571428571428571%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 121000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.8142279387\n",
      "Post ablation logit diff: 1.2300114632\n",
      "Logit diff % change:      51.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Checkpoint 122000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.1607776731\n",
      "Post ablation logit diff: 0.4940848351\n",
      "Logit diff % change:      207.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 123000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.3641097546\n",
      "Post ablation logit diff: 0.6507028937\n",
      "Logit diff % change:      78.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 4.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 124000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.5563088655\n",
      "Post ablation logit diff: 0.6837146282\n",
      "Logit diff % change:      22.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 125000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.8047014475\n",
      "Post ablation logit diff: 1.0347937346\n",
      "Logit diff % change:      28.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 126000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.4695343077\n",
      "Post ablation logit diff: 0.5832934976\n",
      "Logit diff % change:      24.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 127000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.7830362916\n",
      "Post ablation logit diff: 1.1844577789\n",
      "Logit diff % change:      51.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 128000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.5455057025\n",
      "Post ablation logit diff: 0.5455057025\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 129000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.4639167190\n",
      "Post ablation logit diff: 0.6947611570\n",
      "Logit diff % change:      49.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 130000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.6102454662\n",
      "Post ablation logit diff: 0.6102454662\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 131000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.3642901182\n",
      "Post ablation logit diff: 0.3642901182\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 132000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.5015211701\n",
      "Post ablation logit diff: 0.8407520652\n",
      "Logit diff % change:      67.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 133000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.4646611810\n",
      "Post ablation logit diff: 0.8741306663\n",
      "Logit diff % change:      88.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 134000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.6061727405\n",
      "Post ablation logit diff: 0.7949532866\n",
      "Logit diff % change:      31.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 135000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.1763992012\n",
      "Post ablation logit diff: 0.4546210766\n",
      "Logit diff % change:      157.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Checkpoint 136000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.6439035535\n",
      "Post ablation logit diff: 0.6439035535\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 137000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.3428845406\n",
      "Post ablation logit diff: 0.4921698570\n",
      "Logit diff % change:      43.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 138000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.4177483618\n",
      "Post ablation logit diff: 0.7855994701\n",
      "Logit diff % change:      88.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 139000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.3257012963\n",
      "Post ablation logit diff: 0.7338541746\n",
      "Logit diff % change:      125.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 4.761904761904762%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 140000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.4503014088\n",
      "Post ablation logit diff: 0.7627047300\n",
      "Logit diff % change:      69.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.1 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 141000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.4758135080\n",
      "Post ablation logit diff: 0.7533391714\n",
      "Logit diff % change:      58.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 142000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.3878436387\n",
      "Post ablation logit diff: 0.5403104424\n",
      "Logit diff % change:      39.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Copy circuit for head 3.3 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 143000:\n",
      "Heads ablated:            [(3, 3)]\n",
      "Original logit diff:      0.3769233525\n",
      "Post ablation logit diff: 0.7746383548\n",
      "Logit diff % change:      105.52%\n"
     ]
    }
   ],
   "source": [
    "experiment_metrics = dict()\n",
    "# create folder\n",
    "os.makedirs(f'results/backup/{MODEL_SHORTNAME}', exist_ok=True)\n",
    "\n",
    "for checkpoint in range(4000, 144000, 1000):\n",
    "\n",
    "    experiment_metrics = run_iteration(\n",
    "        BASE_MODEL, VARIANT, df, checkpoint=checkpoint, dataset=ioi_dataset, experiment_metrics=experiment_metrics, \n",
    "        threshold=COPY_SCORE_THRESHOLD\n",
    "    )\n",
    "    experiment_metrics = process_backup_results(df, checkpoint, experiment_metrics)\n",
    "\n",
    "    # save to file, using pytorch format\n",
    "    torch.save(experiment_metrics, f'results/backup/{MODEL_SHORTNAME}/nmh_backup_metrics.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([4000, 5000, 6000, 7000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_metrics.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pythia 160m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TO_VIEW = \"pythia-160m-alldropout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_metrics = torch.load(f'results/backup/{MODEL_TO_VIEW}/nmh_backup_metrics.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['logit_diff', 'per_head_logit_diffs', 'ablation_targets', 'ablated_logit_diff', 'per_head_ablated_logit_diffs', 'per_head_logit_diff_delta', 'in_circuit_head_delta', 'outside_circuit_head_delta', 'summed_in_circuit_head_delta', 'summed_outside_circuit_head_delta', 'summed_total_head_delta'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_metrics[4000].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_in_circuit_head_deltas = {checkpoint: experiment_metrics[checkpoint][\"summed_in_circuit_head_delta\"] for checkpoint in experiment_metrics.keys()}\n",
    "summed_outside_circuit_head_deltas = {checkpoint: experiment_metrics[checkpoint][\"summed_outside_circuit_head_delta\"] for checkpoint in experiment_metrics.keys()}\n",
    "summed_total_head_deltas = {checkpoint: experiment_metrics[checkpoint][\"summed_total_head_delta\"] for checkpoint in experiment_metrics.keys()}\n",
    "per_head_logit_diff_deltas = {checkpoint: experiment_metrics[checkpoint][\"per_head_logit_diff_delta\"] for checkpoint in experiment_metrics.keys()}\n",
    "total_logit_diff_deltas = {checkpoint: experiment_metrics[checkpoint]['ablated_logit_diff'] - experiment_metrics[checkpoint]['logit_diff'] for checkpoint in experiment_metrics.keys()}\n",
    "\n",
    "for checkpoint in experiment_metrics.keys():\n",
    "    # divide by total original logit diff\n",
    "    summed_in_circuit_head_deltas[checkpoint] = summed_in_circuit_head_deltas[checkpoint] / experiment_metrics[checkpoint][\"logit_diff\"]\n",
    "    summed_outside_circuit_head_deltas[checkpoint] = summed_outside_circuit_head_deltas[checkpoint] / experiment_metrics[checkpoint][\"logit_diff\"]\n",
    "    summed_total_head_deltas[checkpoint] = summed_total_head_deltas[checkpoint] / experiment_metrics[checkpoint][\"logit_diff\"]\n",
    "    per_head_logit_diff_deltas[checkpoint] = per_head_logit_diff_deltas[checkpoint] / experiment_metrics[checkpoint][\"logit_diff\"]\n",
    "    total_logit_diff_deltas[checkpoint] = total_logit_diff_deltas[checkpoint] / experiment_metrics[checkpoint][\"logit_diff\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Checkpoint=%{x}<br>Change as % of original logit diff=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          4000,
          5000,
          6000,
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          126000,
          127000,
          128000,
          129000,
          130000,
          131000,
          132000,
          133000,
          134000,
          135000,
          136000,
          137000,
          138000,
          139000,
          140000,
          141000,
          142000,
          143000
         ],
         "xaxis": "x",
         "y": [
          -0.04874759206355586,
          0,
          -0.004927556561437061,
          0.0006743107202406805,
          0.04036578656102951,
          0.09597346521368576,
          0.10268239756901225,
          0.05690063353421415,
          0.09948137809491371,
          0.07299122642324421,
          0.10252651704533937,
          0.11269554053433817,
          0.10051376825947178,
          0.1088063776437573,
          0.12957015031302271,
          0.10681447081913822,
          0.14895983960725495,
          0.12758877854470932,
          0.13645988726566838,
          0.14865951587102463,
          0.14164626970710809,
          0.12957601809369412,
          0.12878618492862373,
          0.08272382214374036,
          0.10825265798896781,
          0.11976527111009047,
          0.11758727988664926,
          0.12307609275652977,
          0.10645015916966778,
          0.09973174072990366,
          0.1004438753795971,
          0.11348029558509394,
          0.11181337549663709,
          0.08856981395822675,
          0.07580316626216874,
          0.10308891037353882,
          0.08925231558624137,
          0.06620143341399017,
          0.06140121536160252,
          0.09611860008277916,
          0.07933803025040254,
          0.10934576306318014,
          0.07065813294263679,
          0.07085727218614304,
          0.05536279021059895,
          0.09221853705243352,
          0.09588969922890397,
          0.08822287125266501,
          0.06862912712177309,
          0.0877592843441588,
          0.07751692498788725,
          0.0737459113971808,
          0.08568443159713797,
          0.0813632727738016,
          0.08691193583856249,
          0.056643390227164594,
          0.06657101217973874,
          0.10561306791571012,
          0.07475683633136156,
          0.0750266473402659,
          0.07487797617407495,
          0.08496792846714966,
          0.062175367993380956,
          0.06392574909504538,
          0.09451198044168062,
          0.05890794534716129,
          0.09054137132725114,
          0.07413826899615883,
          0.0844143143946076,
          0.06570059019004362,
          0.07920544498006071,
          0.11261990881072628,
          0.09277152714132121,
          0.07376765518034124,
          0.04737410089469147,
          0.07426975146102822,
          0.03485145286770831,
          0.0651146774228661,
          0.07628403816979303,
          0.05445336968129576,
          0.029765411406656533,
          0.05696537247676474,
          0.044782428656720163,
          0.045010959778588974,
          0.05228933268163094,
          0.07097674095813095,
          0.04246955357100003,
          0.06467394143593337,
          0.04563181243663678,
          0.04340673084905829,
          0.04306732583159576,
          0.05211456211993966,
          0.05871719707301907,
          0.05810109109598053,
          0.06079037078225046,
          0.03944882268243631,
          0.04728356991911338,
          0.05733666305026681,
          0.0388132774520031,
          0.04730385263207168,
          0.04114124955393634,
          0.03949183592119944,
          0.03457835370841648,
          0.03459737426985162,
          0.0319678010528512,
          0.03308853582748141,
          0.027661118782368894,
          0.03456853108911376,
          0.02840640385111496,
          0.01930603554786443,
          0.03601782777642817,
          0.023082051785884863,
          0.03125438246715211,
          0.01682396002549297,
          0.020912885085288317,
          0.015418706460666776,
          0.017511601479999958,
          0.009844104245258343,
          0.017460074066661557,
          0.026944187919356082,
          0.014296447739947957,
          0.012739445342321756,
          0.006476104462168121,
          0.009815081547151416,
          0.009769135654954898,
          0.010748837822350003,
          0.010674380710562491,
          0.020791681357431868,
          0.015429756254052416,
          0.007981631458420691,
          0.01217451486085811,
          0.018553015444649157,
          0.015657068481213406,
          0.020563142097295095,
          0.011444063409415926,
          0.011145184411754565,
          0.019839719319202333,
          0.015308745634116293,
          0.009208870476862177,
          0.011535058932260723
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Summed Post-NMH-Ablation In-Circuit Head Logit Diff Change Over Time (pythia-160m-alldropout)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Checkpoint"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Change as % of original logit diff"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"0515f89e-4987-4eec-9bed-743ec134d2f6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0515f89e-4987-4eec-9bed-743ec134d2f6\")) {                    Plotly.newPlot(                        \"0515f89e-4987-4eec-9bed-743ec134d2f6\",                        [{\"hovertemplate\":\"Checkpoint=%{x}\\u003cbr\\u003eChange as % of original logit diff=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4000,5000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000],\"xaxis\":\"x\",\"y\":[-0.04874759206355586,0.0,-0.004927556561437061,0.0006743107202406805,0.04036578656102951,0.09597346521368576,0.10268239756901225,0.05690063353421415,0.09948137809491371,0.07299122642324421,0.10252651704533937,0.11269554053433817,0.10051376825947178,0.1088063776437573,0.12957015031302271,0.10681447081913822,0.14895983960725495,0.12758877854470932,0.13645988726566838,0.14865951587102463,0.14164626970710809,0.12957601809369412,0.12878618492862373,0.08272382214374036,0.10825265798896781,0.11976527111009047,0.11758727988664926,0.12307609275652977,0.10645015916966778,0.09973174072990366,0.1004438753795971,0.11348029558509394,0.11181337549663709,0.08856981395822675,0.07580316626216874,0.10308891037353882,0.08925231558624137,0.06620143341399017,0.06140121536160252,0.09611860008277916,0.07933803025040254,0.10934576306318014,0.07065813294263679,0.07085727218614304,0.05536279021059895,0.09221853705243352,0.09588969922890397,0.08822287125266501,0.06862912712177309,0.0877592843441588,0.07751692498788725,0.0737459113971808,0.08568443159713797,0.0813632727738016,0.08691193583856249,0.056643390227164594,0.06657101217973874,0.10561306791571012,0.07475683633136156,0.0750266473402659,0.07487797617407495,0.08496792846714966,0.062175367993380956,0.06392574909504538,0.09451198044168062,0.05890794534716129,0.09054137132725114,0.07413826899615883,0.0844143143946076,0.06570059019004362,0.07920544498006071,0.11261990881072628,0.09277152714132121,0.07376765518034124,0.04737410089469147,0.07426975146102822,0.03485145286770831,0.0651146774228661,0.07628403816979303,0.05445336968129576,0.029765411406656533,0.05696537247676474,0.044782428656720163,0.045010959778588974,0.05228933268163094,0.07097674095813095,0.04246955357100003,0.06467394143593337,0.04563181243663678,0.04340673084905829,0.04306732583159576,0.05211456211993966,0.05871719707301907,0.05810109109598053,0.06079037078225046,0.03944882268243631,0.04728356991911338,0.05733666305026681,0.0388132774520031,0.04730385263207168,0.04114124955393634,0.03949183592119944,0.03457835370841648,0.03459737426985162,0.0319678010528512,0.03308853582748141,0.027661118782368894,0.03456853108911376,0.02840640385111496,0.01930603554786443,0.03601782777642817,0.023082051785884863,0.03125438246715211,0.01682396002549297,0.020912885085288317,0.015418706460666776,0.017511601479999958,0.009844104245258343,0.017460074066661557,0.026944187919356082,0.014296447739947957,0.012739445342321756,0.006476104462168121,0.009815081547151416,0.009769135654954898,0.010748837822350003,0.010674380710562491,0.020791681357431868,0.015429756254052416,0.007981631458420691,0.01217451486085811,0.018553015444649157,0.015657068481213406,0.020563142097295095,0.011444063409415926,0.011145184411754565,0.019839719319202333,0.015308745634116293,0.009208870476862177,0.011535058932260723],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Checkpoint\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Change as % of original logit diff\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Summed Post-NMH-Ablation In-Circuit Head Logit Diff Change Over Time (pythia-160m-alldropout)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('0515f89e-4987-4eec-9bed-743ec134d2f6');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot summed_in_circuit_head_deltas with plotly express\n",
    "fig = px.line(\n",
    "    x=list(summed_in_circuit_head_deltas.keys()), \n",
    "    y=list(summed_in_circuit_head_deltas.values()), \n",
    "    title=f\"Summed Post-NMH-Ablation In-Circuit Head Logit Diff Change Over Time ({MODEL_TO_VIEW})\",\n",
    "    labels={'x': 'Checkpoint', 'y': 'Change as % of original logit diff'} \n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Checkpoint=%{x}<br>Change as % of original logit diff=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          4000,
          5000,
          6000,
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          126000,
          127000,
          128000,
          129000,
          130000,
          131000,
          132000,
          133000,
          134000,
          135000,
          136000,
          137000,
          138000,
          139000,
          140000,
          141000,
          142000,
          143000
         ],
         "xaxis": "x",
         "y": [
          -0.043071704995338224,
          0,
          -0.034838356390819954,
          0.010491199797567822,
          0.010968819453935222,
          0.0015313982563985365,
          0.00532543547393766,
          0.010752839912506215,
          0.02531287764098698,
          0.018958980378701782,
          0.012663146585631049,
          0.015745443908849608,
          0.001253890462658407,
          0.014050342054257883,
          0.01441419142351048,
          0.013618688241534208,
          0.029345154340804314,
          0.00206194990681872,
          0.02779381463404436,
          0.02809484386991504,
          0.030302537238172285,
          0.002692509833182894,
          0.02245335973625196,
          0.0010800536696668995,
          -0.00033378300289789686,
          0.02278989011528307,
          -0.0007894061245561732,
          0.0005445490384878283,
          -0.0007571442634131655,
          -0.000715703684103466,
          -0.000020625481121832807,
          0.0004697040728810489,
          0.0007784223780814578,
          0.012825845535615372,
          0.03468700412608689,
          0.015627993597741183,
          0.0419716025512508,
          0.02877968397416872,
          0.019712446398668718,
          0.026902490602005048,
          0.016781069312192788,
          0.002071077983715289,
          0.018275545416346094,
          0.015047084829562117,
          0.030879653712632793,
          0.016442919292460903,
          0.0044167596571767906,
          0.028159189672801635,
          0.07909375240706666,
          0.05178431847726203,
          0.0018901401182523845,
          0.0034961710715242637,
          0.00292895533018044,
          0.002126415556090234,
          0.0030492390829039333,
          0.0030223033136976794,
          0.02008166412505668,
          0.0035456041883057607,
          0.020610581350076086,
          0.024104055071373132,
          0.0029305579335846266,
          0.004411356993980214,
          0.0036760890413248598,
          0.0552390828331396,
          0.010253089622964395,
          0.003070315727987247,
          0.004598480793756628,
          0.025428259363388748,
          0.004157358861025079,
          0.0033033339611961395,
          0.02958754344612417,
          0.019078342996456648,
          0.005432832619587728,
          0.025313198970504536,
          0.01829353996991709,
          0.0031303388257330022,
          0.04855533403575165,
          0.00331778741172562,
          0.0019858912256388987,
          0.00414795719644388,
          0.04796310680849898,
          0.006522627675811524,
          0.0433516493848596,
          0.026138310138358635,
          0.009362040020852181,
          0.006914942879938271,
          0.042187168378208253,
          0.0037349999310240796,
          0.049817972231316204,
          0.002789436935838183,
          0.002797758411895943,
          0.05204835040152067,
          0.059878940640000565,
          0.0163152878400242,
          0.0022534859389283317,
          0.0319401522048137,
          0.007048238089235162,
          0.0121535512832368,
          0.0038414043481558528,
          0.0037456667069525173,
          0.008391687502348944,
          0.0027589807898952116,
          0.005605328460150576,
          0.0013763728010548737,
          0.0011645933953647026,
          0.008593100898644101,
          0.004654583232310729,
          0.00584879137740316,
          0.0100021931536962,
          0.03065953835606348,
          0.008855759003169365,
          0.019745341236709697,
          0.002796119614619538,
          0.011352454245092403,
          0.00666919775743624,
          0.004675234223743095,
          0.0018308533567863894,
          0.009164939117404373,
          0.004221618018106341,
          0.004134954167825886,
          0.01302100501016565,
          0.012646882327714179,
          0.0011572067096873562,
          0.0007284961634218276,
          0.0018684002774627673,
          0.012999804107363734,
          0.004371371921108856,
          0.01478777802326914,
          0.004974097550796613,
          0.0007935620404013462,
          0.011419992415595876,
          0.00855909908955385,
          0.0055227099616256955,
          0.005477746329584981,
          0.004117184404643398,
          0.005805052026921262,
          0.0030363544844265026,
          0.0010752626343117785,
          0.00492113538277586,
          0.0021855264517527504
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Summed Post-NMH-Ablation Outside-Circuit Head Attribution Change (pythia-160m-alldropout)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Checkpoint"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Change as % of original logit diff"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"bd1257e2-2024-4d66-b393-aa731fdc075b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bd1257e2-2024-4d66-b393-aa731fdc075b\")) {                    Plotly.newPlot(                        \"bd1257e2-2024-4d66-b393-aa731fdc075b\",                        [{\"hovertemplate\":\"Checkpoint=%{x}\\u003cbr\\u003eChange as % of original logit diff=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4000,5000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000],\"xaxis\":\"x\",\"y\":[-0.043071704995338224,0.0,-0.034838356390819954,0.010491199797567822,0.010968819453935222,0.0015313982563985365,0.00532543547393766,0.010752839912506215,0.02531287764098698,0.018958980378701782,0.012663146585631049,0.015745443908849608,0.001253890462658407,0.014050342054257883,0.01441419142351048,0.013618688241534208,0.029345154340804314,0.00206194990681872,0.02779381463404436,0.02809484386991504,0.030302537238172285,0.002692509833182894,0.02245335973625196,0.0010800536696668995,-0.00033378300289789686,0.02278989011528307,-0.0007894061245561732,0.0005445490384878283,-0.0007571442634131655,-0.000715703684103466,-2.0625481121832807e-05,0.0004697040728810489,0.0007784223780814578,0.012825845535615372,0.03468700412608689,0.015627993597741183,0.0419716025512508,0.02877968397416872,0.019712446398668718,0.026902490602005048,0.016781069312192788,0.002071077983715289,0.018275545416346094,0.015047084829562117,0.030879653712632793,0.016442919292460903,0.0044167596571767906,0.028159189672801635,0.07909375240706666,0.05178431847726203,0.0018901401182523845,0.0034961710715242637,0.00292895533018044,0.002126415556090234,0.0030492390829039333,0.0030223033136976794,0.02008166412505668,0.0035456041883057607,0.020610581350076086,0.024104055071373132,0.0029305579335846266,0.004411356993980214,0.0036760890413248598,0.0552390828331396,0.010253089622964395,0.003070315727987247,0.004598480793756628,0.025428259363388748,0.004157358861025079,0.0033033339611961395,0.02958754344612417,0.019078342996456648,0.005432832619587728,0.025313198970504536,0.01829353996991709,0.0031303388257330022,0.04855533403575165,0.00331778741172562,0.0019858912256388987,0.00414795719644388,0.04796310680849898,0.006522627675811524,0.0433516493848596,0.026138310138358635,0.009362040020852181,0.006914942879938271,0.042187168378208253,0.0037349999310240796,0.049817972231316204,0.002789436935838183,0.002797758411895943,0.05204835040152067,0.059878940640000565,0.0163152878400242,0.0022534859389283317,0.0319401522048137,0.007048238089235162,0.0121535512832368,0.0038414043481558528,0.0037456667069525173,0.008391687502348944,0.0027589807898952116,0.005605328460150576,0.0013763728010548737,0.0011645933953647026,0.008593100898644101,0.004654583232310729,0.00584879137740316,0.0100021931536962,0.03065953835606348,0.008855759003169365,0.019745341236709697,0.002796119614619538,0.011352454245092403,0.00666919775743624,0.004675234223743095,0.0018308533567863894,0.009164939117404373,0.004221618018106341,0.004134954167825886,0.01302100501016565,0.012646882327714179,0.0011572067096873562,0.0007284961634218276,0.0018684002774627673,0.012999804107363734,0.004371371921108856,0.01478777802326914,0.004974097550796613,0.0007935620404013462,0.011419992415595876,0.00855909908955385,0.0055227099616256955,0.005477746329584981,0.004117184404643398,0.005805052026921262,0.0030363544844265026,0.0010752626343117785,0.00492113538277586,0.0021855264517527504],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Checkpoint\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Change as % of original logit diff\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Summed Post-NMH-Ablation Outside-Circuit Head Attribution Change (pythia-160m-alldropout)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('bd1257e2-2024-4d66-b393-aa731fdc075b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot summed_outside_circuit_head_deltas\n",
    "fig = px.line(\n",
    "    x=list(summed_outside_circuit_head_deltas.keys()), \n",
    "    y=list(summed_outside_circuit_head_deltas.values()), \n",
    "    title=f\"Summed Post-NMH-Ablation Outside-Circuit Head Attribution Change ({MODEL_TO_VIEW})\",\n",
    "    labels={'x': 'Checkpoint', 'y': 'Change as % of original logit diff'} \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Checkpoint=%{x}<br>Change as % of original logit diff=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          4000,
          5000,
          6000,
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          126000,
          127000,
          128000,
          129000,
          130000,
          131000,
          132000,
          133000,
          134000,
          135000,
          136000,
          137000,
          138000,
          139000,
          140000,
          141000,
          142000,
          143000
         ],
         "xaxis": "x",
         "y": [
          -0.09181928713007813,
          0,
          -0.039765914020408424,
          0.01116551022715545,
          0.051334606014964736,
          0.09750486636720443,
          0.10800784066806234,
          0.06765347279882251,
          0.12479425826108283,
          0.09195020963770843,
          0.11518965522190165,
          0.128440988434745,
          0.10176766107991159,
          0.1228567347769904,
          0.1439843356084074,
          0.12043316510958856,
          0.17830498310010995,
          0.12965072871278943,
          0.1642537085664069,
          0.17675435150077187,
          0.17194881988970384,
          0.13226852553203503,
          0.15123954058477676,
          0.08380387909712117,
          0.10791887682335967,
          0.1425551631691294,
          0.11679787032397541,
          0.12362063481083123,
          0.10569302566158754,
          0.09901603454923519,
          0.10042324979623075,
          0.11394998603594424,
          0.11259181107306139,
          0.10139566819219548,
          0.11049018338626121,
          0.11871690580446906,
          0.13122389728812567,
          0.09498112064017077,
          0.0811136667586547,
          0.1230210873512162,
          0.09611909437568336,
          0.11141684850653204,
          0.08893368347231226,
          0.08590435365016641,
          0.08624244750859282,
          0.10866145001564591,
          0.10030645493135638,
          0.11638205412696341,
          0.14772288633808067,
          0.13954360282142084,
          0.0794070589818,
          0.07724207831375601,
          0.08861338397714501,
          0.08348970034680132,
          0.08996116278146113,
          0.059665694624924985,
          0.08665267466094624,
          0.10915866688377719,
          0.09536740728816968,
          0.09913069721492958,
          0.07780853291173524,
          0.08937928730952241,
          0.06585145890019257,
          0.11916483192818499,
          0.10476508250137885,
          0.06197825677345977,
          0.095139840021588,
          0.099566513295163,
          0.08857167325563267,
          0.06900393207472022,
          0.10879298468315261,
          0.1316982499112236,
          0.09820434704559171,
          0.0990808577939736,
          0.06566764086460855,
          0.0774000920619909,
          0.08340680601639823,
          0.06843246187673153,
          0.0782699267371303,
          0.05860133021861987,
          0.07772852410521922,
          0.06348800371155022,
          0.0881340741239871,
          0.07114926564199614,
          0.061651373663250186,
          0.07789168640282297,
          0.08465671829094253,
          0.06840894264239977,
          0.09544978466795298,
          0.0461961634318856,
          0.04586508070564317,
          0.10416291642660444,
          0.11859615118839517,
          0.07441637472111778,
          0.06304386017745693,
          0.07138897488725,
          0.05433181309565516,
          0.06949022205997323,
          0.042654679505241315,
          0.05104951438495987,
          0.04953293803828524,
          0.042250814394357246,
          0.040183682168567054,
          0.03597374950986658,
          0.033132390085256556,
          0.041681634966804854,
          0.03231570583529021,
          0.04041732603867094,
          0.03840859613269863,
          0.0499655723974868,
          0.04487359440843663,
          0.04282739825967072,
          0.03405050291304387,
          0.028176418197317206,
          0.02758208242884376,
          0.02009394152225901,
          0.019342454420329797,
          0.019009043362662716,
          0.0216816916780099,
          0.03107914129881196,
          0.02731745354737444,
          0.025386325255579545,
          0.0076333108536929305,
          0.010543577524289915,
          0.011637534614462893,
          0.023748641929713737,
          0.015045752631671348,
          0.035579460993888985,
          0.020403854963299786,
          0.008775193305801598,
          0.023594506477032005,
          0.027112112985555506,
          0.02117977670743777,
          0.026040886831036357,
          0.015561247034568867,
          0.016950236041370415,
          0.02287607401389129,
          0.016384007975106016,
          0.014130005106486683,
          0.013720585958523226
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Summed Total Post-NMH-Ablation Head Attribution Change (pythia-160m-alldropout)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Checkpoint"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Change as % of original logit diff"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"37dee492-fe36-4bfd-8803-c69cb5a2b6ff\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"37dee492-fe36-4bfd-8803-c69cb5a2b6ff\")) {                    Plotly.newPlot(                        \"37dee492-fe36-4bfd-8803-c69cb5a2b6ff\",                        [{\"hovertemplate\":\"Checkpoint=%{x}\\u003cbr\\u003eChange as % of original logit diff=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4000,5000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000],\"xaxis\":\"x\",\"y\":[-0.09181928713007813,0.0,-0.039765914020408424,0.01116551022715545,0.051334606014964736,0.09750486636720443,0.10800784066806234,0.06765347279882251,0.12479425826108283,0.09195020963770843,0.11518965522190165,0.128440988434745,0.10176766107991159,0.1228567347769904,0.1439843356084074,0.12043316510958856,0.17830498310010995,0.12965072871278943,0.1642537085664069,0.17675435150077187,0.17194881988970384,0.13226852553203503,0.15123954058477676,0.08380387909712117,0.10791887682335967,0.1425551631691294,0.11679787032397541,0.12362063481083123,0.10569302566158754,0.09901603454923519,0.10042324979623075,0.11394998603594424,0.11259181107306139,0.10139566819219548,0.11049018338626121,0.11871690580446906,0.13122389728812567,0.09498112064017077,0.0811136667586547,0.1230210873512162,0.09611909437568336,0.11141684850653204,0.08893368347231226,0.08590435365016641,0.08624244750859282,0.10866145001564591,0.10030645493135638,0.11638205412696341,0.14772288633808067,0.13954360282142084,0.0794070589818,0.07724207831375601,0.08861338397714501,0.08348970034680132,0.08996116278146113,0.059665694624924985,0.08665267466094624,0.10915866688377719,0.09536740728816968,0.09913069721492958,0.07780853291173524,0.08937928730952241,0.06585145890019257,0.11916483192818499,0.10476508250137885,0.06197825677345977,0.095139840021588,0.099566513295163,0.08857167325563267,0.06900393207472022,0.10879298468315261,0.1316982499112236,0.09820434704559171,0.0990808577939736,0.06566764086460855,0.0774000920619909,0.08340680601639823,0.06843246187673153,0.0782699267371303,0.05860133021861987,0.07772852410521922,0.06348800371155022,0.0881340741239871,0.07114926564199614,0.061651373663250186,0.07789168640282297,0.08465671829094253,0.06840894264239977,0.09544978466795298,0.0461961634318856,0.04586508070564317,0.10416291642660444,0.11859615118839517,0.07441637472111778,0.06304386017745693,0.07138897488725,0.05433181309565516,0.06949022205997323,0.042654679505241315,0.05104951438495987,0.04953293803828524,0.042250814394357246,0.040183682168567054,0.03597374950986658,0.033132390085256556,0.041681634966804854,0.03231570583529021,0.04041732603867094,0.03840859613269863,0.0499655723974868,0.04487359440843663,0.04282739825967072,0.03405050291304387,0.028176418197317206,0.02758208242884376,0.02009394152225901,0.019342454420329797,0.019009043362662716,0.0216816916780099,0.03107914129881196,0.02731745354737444,0.025386325255579545,0.0076333108536929305,0.010543577524289915,0.011637534614462893,0.023748641929713737,0.015045752631671348,0.035579460993888985,0.020403854963299786,0.008775193305801598,0.023594506477032005,0.027112112985555506,0.02117977670743777,0.026040886831036357,0.015561247034568867,0.016950236041370415,0.02287607401389129,0.016384007975106016,0.014130005106486683,0.013720585958523226],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Checkpoint\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Change as % of original logit diff\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Summed Total Post-NMH-Ablation Head Attribution Change (pythia-160m-alldropout)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('37dee492-fe36-4bfd-8803-c69cb5a2b6ff');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot total_head_deltas\n",
    "fig = px.line(\n",
    "    x=list(summed_total_head_deltas.keys()), \n",
    "    y=list(summed_total_head_deltas.values()), \n",
    "    title=f\"Summed Total Post-NMH-Ablation Head Attribution Change ({MODEL_TO_VIEW})\",\n",
    "    labels={'x': 'Checkpoint', 'y': 'Change as % of original logit diff'}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_nmhs, checkpoint_nmhs = get_past_nmhs_for_checkpoints(experiment_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Layer-Head=Layer 10-Head 7<br>Checkpoint=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "Layer 10-Head 7",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Layer 10-Head 7",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          4000,
          6000,
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          20000,
          21000,
          24000,
          25000,
          26000,
          28000,
          29000,
          30000,
          31000,
          33000,
          35000,
          55000,
          66000,
          72000,
          78000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          129000,
          130000,
          132000
         ],
         "xaxis": "x",
         "y": [
          0.0026725521311163902,
          0.000629258924163878,
          0.0007657704991288483,
          0.0003119302273262292,
          0.0005408104043453932,
          0.0005257618031464517,
          0.00027662841603159904,
          0.00040443631587550044,
          0.00045996386324986815,
          0.0005266859079711139,
          0.0006991972331888974,
          0.0004529976285994053,
          0.0007804500637575984,
          0.0006502866744995117,
          0.0009810348274186254,
          0.0008349480340257287,
          0.0009809155017137527,
          0.000916038581635803,
          0.0005797024932689965,
          0.0003255648189224303,
          0.0006155213923193514,
          0.00020697912259493023,
          0.00013133237371221185,
          0.00025235459906980395,
          0.0004624385910574347,
          0.0008549588965252042,
          0.0006586548988707364,
          0.0004686087486334145,
          0.0015414286172017455,
          0.003221277380362153,
          0.00985526293516159,
          0.0173241775482893,
          0.029232485219836235,
          0.022043347358703613,
          0.028252970427274704,
          0.022193506360054016,
          0.022391492500901222,
          0.03811975196003914,
          0.03271893411874771,
          0.022330261766910553,
          0.033639296889305115,
          0.022757917642593384,
          0.02427608333528042,
          0.029711777344346046,
          0.025612127035856247,
          0.016769522801041603,
          0.022821063175797462,
          0.03760848566889763,
          0.02798982709646225,
          0.027296870946884155,
          0.037040431052446365,
          0.024523871019482613,
          0.020268121734261513,
          0.02189512364566326,
          0.021337129175662994,
          0.022509777918457985,
          0.014212765730917454,
          0.01839056983590126,
          0.01428866945207119,
          0.013621959835290909,
          0.013122176751494408,
          0.01007688045501709,
          0.01160617545247078,
          0.01563279703259468,
          0.0035480703227221966,
          0.011431626044213772,
          0.007735598832368851,
          0.012061845511198044,
          0.005630972795188427,
          0.004875671584159136,
          0.004165762569755316,
          0.002620133338496089,
          0.005015193950384855,
          0.00404421566054225,
          0.0008534201770089567,
          0.0004718709387816489,
          0.0004573932965286076,
          0.00028085170197300613
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Layer-Head=Layer 9-Head 9<br>Checkpoint=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "Layer 9-Head 9",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Layer 9-Head 9",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          7000,
          8000,
          11000,
          13000,
          14000,
          15000,
          17000,
          18000,
          19000,
          20000,
          22000,
          23000,
          24000,
          26000,
          29000,
          30000,
          31000,
          35000,
          36000,
          39000,
          40000,
          41000,
          43000,
          45000,
          49000,
          51000,
          52000,
          53000,
          55000,
          58000,
          61000,
          62000,
          63000,
          65000,
          66000,
          67000,
          68000,
          70000,
          71000,
          74000,
          75000,
          76000,
          77000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          106000,
          109000,
          110000,
          112000,
          114000,
          117000,
          118000,
          119000,
          124000,
          125000,
          128000,
          129000,
          131000,
          132000,
          134000
         ],
         "xaxis": "x",
         "y": [
          0.0017917121294885874,
          0.005667125340551138,
          0.007299109827727079,
          0.008191687986254692,
          0.012010126374661922,
          0.012112344615161419,
          0.01314469613134861,
          0.012295766733586788,
          0.012480290606617928,
          0.02863914519548416,
          0.024729236960411072,
          0.021760735660791397,
          0.02855166792869568,
          0.02219049260020256,
          0.02279970981180668,
          0.019900595769286156,
          0.02176225557923317,
          0.020162083208560944,
          0.01848527416586876,
          0.019513001665472984,
          0.018014416098594666,
          0.012220167554914951,
          0.017404286190867424,
          0.017807412892580032,
          0.016632266342639923,
          0.02064628154039383,
          0.01906583644449711,
          0.019717315211892128,
          0.011843998916447163,
          0.015505749732255936,
          0.020441871136426926,
          0.017366889864206314,
          0.01892448030412197,
          0.01852373033761978,
          0.013165752403438091,
          0.019930146634578705,
          0.021192502230405807,
          0.022864865139126778,
          0.019892964512109756,
          0.023508219048380852,
          0.025774521753191948,
          0.022104782983660698,
          0.021632587537169456,
          0.02096971869468689,
          0.020188646391034126,
          0.018835877999663353,
          0.016392558813095093,
          0.011681637726724148,
          0.008836541324853897,
          0.004337572958320379,
          0.003690236946567893,
          0.003030212363228202,
          0.0037146529648452997,
          0.0023884151596575975,
          0.004061004612594843,
          0.0027726173866540194,
          0.0021455190144479275,
          0.00363927218131721,
          0.0034062149934470654,
          0.004100973252207041,
          0.004484336357563734,
          0.0040551722049713135,
          0.0038121941033750772,
          0.0022667511366307735,
          0.0052386196330189705,
          0.0036718465853482485,
          0.0033050274942070246,
          0.0027817883528769016,
          0.0023229457437992096,
          0.0021952930837869644,
          0.00239189644344151,
          0.002751966007053852,
          0.0016180173261091113,
          0.0024881938006728888,
          0.0016974697355180979,
          0.00177038146648556,
          0.001340522663667798,
          0.0009367555612698197,
          0.0009449610952287912,
          0.0009817652171477675,
          0.001115450169891119,
          0.0014319070614874363
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Layer-Head=Layer 9-Head 6<br>Checkpoint=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "Layer 9-Head 6",
         "line": {
          "color": "#00cc96",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Layer 9-Head 6",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          84000,
          86000,
          90000,
          92000,
          95000,
          96000,
          113000,
          115000,
          121000,
          124000,
          125000,
          129000,
          131000,
          134000
         ],
         "xaxis": "x",
         "y": [
          0.002411766443401575,
          0.0033179090823978186,
          0.0038747189100831747,
          0.005100138485431671,
          0.0032757434528321028,
          0.005085034761577845,
          0.00363577320240438,
          0.004538726061582565,
          0.004653990268707275,
          0.0029530839528888464,
          0.005190160591155291,
          0.004965000320225954,
          0.005145061761140823,
          0.010231755673885345,
          0.0066286372020840645,
          0.008754000999033451,
          0.009256397373974323,
          0.011626716703176498,
          0.009798008017241955,
          0.008918024599552155,
          0.006602451205253601,
          0.009992163628339767,
          0.012730184011161327,
          0.009922172874212265,
          0.01246168464422226,
          0.012267830781638622,
          0.012358670122921467,
          0.011819890700280666,
          0.009157104417681694,
          0.011974005028605461,
          0.012825957499444485,
          0.010397862643003464,
          0.014318552799522877,
          0.012905200012028217,
          0.009982398711144924,
          0.01014366652816534,
          0.011478029191493988,
          0.014981011860072613,
          0.012985504232347012,
          0.015007461421191692,
          0.012812497094273567,
          0.010753141716122627,
          0.014166788198053837,
          0.016267696395516396,
          0.015970410779118538,
          0.015791580080986023,
          0.016894401982426643,
          0.013483737595379353,
          0.012541297823190689,
          0.015036255121231079,
          0.01540802139788866,
          0.013801447115838528,
          0.010804715566337109,
          0.016004567965865135,
          0.01627776399254799,
          0.015161830931901932,
          0.017550276592373848,
          0.015830403193831444,
          0.015380950644612312,
          0.012211491353809834,
          0.01687275804579258,
          0.017069285735487938,
          0.013757806271314621,
          0.018924430012702942,
          0.019756918773055077,
          0.02118491567671299,
          0.01618352346122265,
          0.020782627165317535,
          0.02340947650372982,
          0.019920840859413147,
          0.02092892676591873,
          0.016276195645332336,
          0.019673574715852737,
          0.01910588890314102,
          0.024919889867305756,
          0.025067415088415146,
          0.03254231810569763,
          0.03709037974476814,
          0.0400158166885376,
          0.039756856858730316,
          0.046799853444099426,
          0.05384105071425438,
          0.01724526286125183,
          0.01298323180526495,
          0.005388436373323202,
          0.003529398702085018,
          0.0063859689980745316,
          0.005692761391401291,
          0.009092289954423904,
          0.005608438979834318
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Layer-Head=Layer 9-Head 10<br>Checkpoint=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "Layer 9-Head 10",
         "line": {
          "color": "#ab63fa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Layer 9-Head 10",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          126000,
          127000,
          128000,
          129000,
          130000,
          131000,
          132000,
          133000,
          134000,
          135000,
          136000,
          137000,
          138000,
          139000,
          140000,
          141000,
          142000,
          143000
         ],
         "xaxis": "x",
         "y": [
          0.03222522884607315,
          0.07681969553232193,
          0.09267992526292801,
          0.050537109375,
          0.09160967916250229,
          0.06424630433320999,
          0.08639313280582428,
          0.09022432565689087,
          0.08270614594221115,
          0.09181993454694748,
          0.09710001945495605,
          0.08648547530174255,
          0.10375629365444183,
          0.09595594555139542,
          0.1080184206366539,
          0.10138272494077682,
          0.10140974074602127,
          0.09014643728733063,
          0.09361708164215088,
          0.062364812940359116,
          0.0844980850815773,
          0.09353552013635635,
          0.07908909767866135,
          0.07943552732467651,
          0.08190986514091492,
          0.07658962160348892,
          0.07329165190458298,
          0.07407847046852112,
          0.07207773625850677,
          0.07527463883161545,
          0.06588879227638245,
          0.07392334192991257,
          0.06360535323619843,
          0.04968547821044922,
          0.04612740874290466,
          0.06416758894920349,
          0.06921551376581192,
          0.07046577334403992,
          0.060766879469156265,
          0.06194257363677025,
          0.04834931716322899,
          0.06553802639245987,
          0.07097754627466202,
          0.06695930659770966,
          0.06924576312303543,
          0.067400261759758,
          0.05826321989297867,
          0.04571755602955818,
          0.06163875758647919,
          0.058743007481098175,
          0.05438004061579704,
          0.04628763347864151,
          0.05936237797141075,
          0.06313302367925644,
          0.054775454103946686,
          0.05461150407791138,
          0.0506063774228096,
          0.050201185047626495,
          0.03528834134340286,
          0.054817210882902145,
          0.05598009377717972,
          0.042144741863012314,
          0.052121851593256,
          0.053592946380376816,
          0.05531100556254387,
          0.04852185770869255,
          0.06011071056127548,
          0.05953570082783699,
          0.05484941974282265,
          0.052112944424152374,
          0.04271329194307327,
          0.05540832132101059,
          0.010691503994166851,
          0.01124979741871357,
          0.013822130858898163,
          0.007997299544513226,
          0.008184021338820457,
          0.017130574211478233,
          0.017278531566262245,
          0.02039288729429245,
          0.013443195261061192,
          0.02441134676337242,
          0.009756485000252724,
          0.024407168850302696,
          0.017793167382478714,
          0.013443860225379467,
          0.006988817825913429,
          0.01977366767823696,
          0.02635638415813446,
          0.0285998173058033,
          0.017801448702812195,
          0.02694586105644703,
          0.012630818411707878,
          0.012424803338944912,
          0.011190235614776611,
          0.023921163752675056,
          0.019014524295926094,
          0.016663946211338043,
          0.011803888715803623,
          0.01717207580804825,
          0.01147424802184105,
          0.01782493107020855,
          0.013697193004190922,
          0.021145636215806007,
          0.016838466748595238,
          0.006195883732289076,
          0.02008146233856678,
          0.016879579052329063,
          0.01805323176085949,
          0.008822017349302769,
          0.008493449538946152,
          0.009293714538216591,
          0.012265020981431007,
          0.005372397601604462,
          0.014539504423737526,
          0.0185600183904171,
          0.010052982717752457,
          0.011502386070787907,
          0.008226056583225727,
          0.009178150445222855,
          0.009944180026650429,
          0.010204619728028774,
          0.009734218940138817,
          0.02032715082168579,
          0.014304188080132008,
          0.006560435984283686,
          0.011993381194770336,
          0.019200043752789497,
          0.01598062738776207,
          0.02094927988946438,
          0.011559292674064636,
          0.010404814966022968,
          0.019887054339051247,
          0.012579450383782387,
          0.008997111581265926,
          0.009807485155761242
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Layer-Head=Layer 9-Head 4<br>Checkpoint=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "Layer 9-Head 4",
         "line": {
          "color": "#FFA15A",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Layer 9-Head 4",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          12000,
          13000,
          18000,
          23000,
          67000,
          80000,
          87000,
          89000,
          90000,
          96000,
          99000,
          100000,
          101000,
          115000,
          117000,
          123000,
          124000,
          125000,
          129000,
          130000,
          136000,
          139000
         ],
         "xaxis": "x",
         "y": [
          0.020280687138438225,
          0.0105930520221591,
          0.013085578568279743,
          0.02583824098110199,
          0.03103538230061531,
          0.02496826834976673,
          0.01818022131919861,
          0.01043331902474165,
          0.006169225089251995,
          0.006062897853553295,
          0.005714803002774715,
          0.005004238337278366,
          0.003871041350066662,
          0.003994766157120466,
          0.002384752035140991,
          0.0021040483843535185,
          0.002621949417516589,
          0.002662095008417964,
          0.0017760018818080425,
          0.0018928518984466791,
          0.0027008475735783577,
          0.0018888781778514385
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "height": 500,
        "legend": {
         "title": {
          "text": "Layer-Head"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Top Backup Heads Attribution Across Checkpoints (pythia-160m-alldropout)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Checkpoint"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"ad7c08d3-c340-433a-b5fb-8c8e1a8ade02\" class=\"plotly-graph-div\" style=\"height:500px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ad7c08d3-c340-433a-b5fb-8c8e1a8ade02\")) {                    Plotly.newPlot(                        \"ad7c08d3-c340-433a-b5fb-8c8e1a8ade02\",                        [{\"hovertemplate\":\"Layer-Head=Layer 10-Head 7\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Layer 10-Head 7\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Layer 10-Head 7\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[4000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,20000,21000,24000,25000,26000,28000,29000,30000,31000,33000,35000,55000,66000,72000,78000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,129000,130000,132000],\"xaxis\":\"x\",\"y\":[0.0026725521311163902,0.000629258924163878,0.0007657704991288483,0.0003119302273262292,0.0005408104043453932,0.0005257618031464517,0.00027662841603159904,0.00040443631587550044,0.00045996386324986815,0.0005266859079711139,0.0006991972331888974,0.0004529976285994053,0.0007804500637575984,0.0006502866744995117,0.0009810348274186254,0.0008349480340257287,0.0009809155017137527,0.000916038581635803,0.0005797024932689965,0.0003255648189224303,0.0006155213923193514,0.00020697912259493023,0.00013133237371221185,0.00025235459906980395,0.0004624385910574347,0.0008549588965252042,0.0006586548988707364,0.0004686087486334145,0.0015414286172017455,0.003221277380362153,0.00985526293516159,0.0173241775482893,0.029232485219836235,0.022043347358703613,0.028252970427274704,0.022193506360054016,0.022391492500901222,0.03811975196003914,0.03271893411874771,0.022330261766910553,0.033639296889305115,0.022757917642593384,0.02427608333528042,0.029711777344346046,0.025612127035856247,0.016769522801041603,0.022821063175797462,0.03760848566889763,0.02798982709646225,0.027296870946884155,0.037040431052446365,0.024523871019482613,0.020268121734261513,0.02189512364566326,0.021337129175662994,0.022509777918457985,0.014212765730917454,0.01839056983590126,0.01428866945207119,0.013621959835290909,0.013122176751494408,0.01007688045501709,0.01160617545247078,0.01563279703259468,0.0035480703227221966,0.011431626044213772,0.007735598832368851,0.012061845511198044,0.005630972795188427,0.004875671584159136,0.004165762569755316,0.002620133338496089,0.005015193950384855,0.00404421566054225,0.0008534201770089567,0.0004718709387816489,0.0004573932965286076,0.00028085170197300613],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Layer-Head=Layer 9-Head 9\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Layer 9-Head 9\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Layer 9-Head 9\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[7000,8000,11000,13000,14000,15000,17000,18000,19000,20000,22000,23000,24000,26000,29000,30000,31000,35000,36000,39000,40000,41000,43000,45000,49000,51000,52000,53000,55000,58000,61000,62000,63000,65000,66000,67000,68000,70000,71000,74000,75000,76000,77000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,106000,109000,110000,112000,114000,117000,118000,119000,124000,125000,128000,129000,131000,132000,134000],\"xaxis\":\"x\",\"y\":[0.0017917121294885874,0.005667125340551138,0.007299109827727079,0.008191687986254692,0.012010126374661922,0.012112344615161419,0.01314469613134861,0.012295766733586788,0.012480290606617928,0.02863914519548416,0.024729236960411072,0.021760735660791397,0.02855166792869568,0.02219049260020256,0.02279970981180668,0.019900595769286156,0.02176225557923317,0.020162083208560944,0.01848527416586876,0.019513001665472984,0.018014416098594666,0.012220167554914951,0.017404286190867424,0.017807412892580032,0.016632266342639923,0.02064628154039383,0.01906583644449711,0.019717315211892128,0.011843998916447163,0.015505749732255936,0.020441871136426926,0.017366889864206314,0.01892448030412197,0.01852373033761978,0.013165752403438091,0.019930146634578705,0.021192502230405807,0.022864865139126778,0.019892964512109756,0.023508219048380852,0.025774521753191948,0.022104782983660698,0.021632587537169456,0.02096971869468689,0.020188646391034126,0.018835877999663353,0.016392558813095093,0.011681637726724148,0.008836541324853897,0.004337572958320379,0.003690236946567893,0.003030212363228202,0.0037146529648452997,0.0023884151596575975,0.004061004612594843,0.0027726173866540194,0.0021455190144479275,0.00363927218131721,0.0034062149934470654,0.004100973252207041,0.004484336357563734,0.0040551722049713135,0.0038121941033750772,0.0022667511366307735,0.0052386196330189705,0.0036718465853482485,0.0033050274942070246,0.0027817883528769016,0.0023229457437992096,0.0021952930837869644,0.00239189644344151,0.002751966007053852,0.0016180173261091113,0.0024881938006728888,0.0016974697355180979,0.00177038146648556,0.001340522663667798,0.0009367555612698197,0.0009449610952287912,0.0009817652171477675,0.001115450169891119,0.0014319070614874363],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Layer-Head=Layer 9-Head 6\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Layer 9-Head 6\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Layer 9-Head 6\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,84000,86000,90000,92000,95000,96000,113000,115000,121000,124000,125000,129000,131000,134000],\"xaxis\":\"x\",\"y\":[0.002411766443401575,0.0033179090823978186,0.0038747189100831747,0.005100138485431671,0.0032757434528321028,0.005085034761577845,0.00363577320240438,0.004538726061582565,0.004653990268707275,0.0029530839528888464,0.005190160591155291,0.004965000320225954,0.005145061761140823,0.010231755673885345,0.0066286372020840645,0.008754000999033451,0.009256397373974323,0.011626716703176498,0.009798008017241955,0.008918024599552155,0.006602451205253601,0.009992163628339767,0.012730184011161327,0.009922172874212265,0.01246168464422226,0.012267830781638622,0.012358670122921467,0.011819890700280666,0.009157104417681694,0.011974005028605461,0.012825957499444485,0.010397862643003464,0.014318552799522877,0.012905200012028217,0.009982398711144924,0.01014366652816534,0.011478029191493988,0.014981011860072613,0.012985504232347012,0.015007461421191692,0.012812497094273567,0.010753141716122627,0.014166788198053837,0.016267696395516396,0.015970410779118538,0.015791580080986023,0.016894401982426643,0.013483737595379353,0.012541297823190689,0.015036255121231079,0.01540802139788866,0.013801447115838528,0.010804715566337109,0.016004567965865135,0.01627776399254799,0.015161830931901932,0.017550276592373848,0.015830403193831444,0.015380950644612312,0.012211491353809834,0.01687275804579258,0.017069285735487938,0.013757806271314621,0.018924430012702942,0.019756918773055077,0.02118491567671299,0.01618352346122265,0.020782627165317535,0.02340947650372982,0.019920840859413147,0.02092892676591873,0.016276195645332336,0.019673574715852737,0.01910588890314102,0.024919889867305756,0.025067415088415146,0.03254231810569763,0.03709037974476814,0.0400158166885376,0.039756856858730316,0.046799853444099426,0.05384105071425438,0.01724526286125183,0.01298323180526495,0.005388436373323202,0.003529398702085018,0.0063859689980745316,0.005692761391401291,0.009092289954423904,0.005608438979834318],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Layer-Head=Layer 9-Head 10\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Layer 9-Head 10\",\"line\":{\"color\":\"#ab63fa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Layer 9-Head 10\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000],\"xaxis\":\"x\",\"y\":[0.03222522884607315,0.07681969553232193,0.09267992526292801,0.050537109375,0.09160967916250229,0.06424630433320999,0.08639313280582428,0.09022432565689087,0.08270614594221115,0.09181993454694748,0.09710001945495605,0.08648547530174255,0.10375629365444183,0.09595594555139542,0.1080184206366539,0.10138272494077682,0.10140974074602127,0.09014643728733063,0.09361708164215088,0.062364812940359116,0.0844980850815773,0.09353552013635635,0.07908909767866135,0.07943552732467651,0.08190986514091492,0.07658962160348892,0.07329165190458298,0.07407847046852112,0.07207773625850677,0.07527463883161545,0.06588879227638245,0.07392334192991257,0.06360535323619843,0.04968547821044922,0.04612740874290466,0.06416758894920349,0.06921551376581192,0.07046577334403992,0.060766879469156265,0.06194257363677025,0.04834931716322899,0.06553802639245987,0.07097754627466202,0.06695930659770966,0.06924576312303543,0.067400261759758,0.05826321989297867,0.04571755602955818,0.06163875758647919,0.058743007481098175,0.05438004061579704,0.04628763347864151,0.05936237797141075,0.06313302367925644,0.054775454103946686,0.05461150407791138,0.0506063774228096,0.050201185047626495,0.03528834134340286,0.054817210882902145,0.05598009377717972,0.042144741863012314,0.052121851593256,0.053592946380376816,0.05531100556254387,0.04852185770869255,0.06011071056127548,0.05953570082783699,0.05484941974282265,0.052112944424152374,0.04271329194307327,0.05540832132101059,0.010691503994166851,0.01124979741871357,0.013822130858898163,0.007997299544513226,0.008184021338820457,0.017130574211478233,0.017278531566262245,0.02039288729429245,0.013443195261061192,0.02441134676337242,0.009756485000252724,0.024407168850302696,0.017793167382478714,0.013443860225379467,0.006988817825913429,0.01977366767823696,0.02635638415813446,0.0285998173058033,0.017801448702812195,0.02694586105644703,0.012630818411707878,0.012424803338944912,0.011190235614776611,0.023921163752675056,0.019014524295926094,0.016663946211338043,0.011803888715803623,0.01717207580804825,0.01147424802184105,0.01782493107020855,0.013697193004190922,0.021145636215806007,0.016838466748595238,0.006195883732289076,0.02008146233856678,0.016879579052329063,0.01805323176085949,0.008822017349302769,0.008493449538946152,0.009293714538216591,0.012265020981431007,0.005372397601604462,0.014539504423737526,0.0185600183904171,0.010052982717752457,0.011502386070787907,0.008226056583225727,0.009178150445222855,0.009944180026650429,0.010204619728028774,0.009734218940138817,0.02032715082168579,0.014304188080132008,0.006560435984283686,0.011993381194770336,0.019200043752789497,0.01598062738776207,0.02094927988946438,0.011559292674064636,0.010404814966022968,0.019887054339051247,0.012579450383782387,0.008997111581265926,0.009807485155761242],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Layer-Head=Layer 9-Head 4\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Layer 9-Head 4\",\"line\":{\"color\":\"#FFA15A\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Layer 9-Head 4\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[12000,13000,18000,23000,67000,80000,87000,89000,90000,96000,99000,100000,101000,115000,117000,123000,124000,125000,129000,130000,136000,139000],\"xaxis\":\"x\",\"y\":[0.020280687138438225,0.0105930520221591,0.013085578568279743,0.02583824098110199,0.03103538230061531,0.02496826834976673,0.01818022131919861,0.01043331902474165,0.006169225089251995,0.006062897853553295,0.005714803002774715,0.005004238337278366,0.003871041350066662,0.003994766157120466,0.002384752035140991,0.0021040483843535185,0.002621949417516589,0.002662095008417964,0.0017760018818080425,0.0018928518984466791,0.0027008475735783577,0.0018888781778514385],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Checkpoint\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Value\"}},\"legend\":{\"title\":{\"text\":\"Layer-Head\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Top Backup Heads Attribution Across Checkpoints (pythia-160m-alldropout)\"},\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ad7c08d3-c340-433a-b5fb-8c8e1a8ade02');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_backup_heads = plot_top_heads(model_name=MODEL_TO_VIEW, checkpoint_dict=per_head_logit_diff_deltas, cumulative_nmhs=cumulative_nmhs, top_k_per_checkpoint=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Head: %{x}<br>Layer: %{y}<br>Logit diff attribution: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           -0.00031765131279826164,
           0.3156936466693878,
           0.0032243020832538605,
           0.0043573444709181786,
           0.02283836156129837,
           0.1909862607717514,
           0.015091687440872192,
           0.03277648985385895,
           0.0030522309243679047,
           0.0035090791061520576,
           0.000053514144383370876,
           -0.00011852476745843887
          ],
          [
           -0.00014347699470818043,
           0.027818620204925537,
           -0.0041803522035479546,
           0.0003439248539507389,
           0.000005809706635773182,
           -0.001769278198480606,
           -0.0033143162727355957,
           0.039005570113658905,
           0.003304571844637394,
           -0.0014683082699775696,
           0.007542278617620468,
           -0.002905469387769699
          ],
          [
           -0.0010985229164361954,
           0.0011500654509291053,
           -0.0004440473858267069,
           0.00000995676964521408,
           -0.003775139572098851,
           0.02032054215669632,
           -0.0019474942237138748,
           -0.0006243046373128891,
           0.0016780085861682892,
           -0.00171709805727005,
           0.01802472025156021,
           -0.0001716803526505828
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorbar": {
          "title": {
           "text": "Logit diff attribution"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "margin": {
         "l": 100,
         "r": 100
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Headwise logit diff contribution, post NMH KO"
        },
        "width": 600,
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "scaleanchor": "y",
         "showline": true,
         "title": {
          "text": "Head"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "showline": true,
         "title": {
          "text": "Layer"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"ff1a0e17-38be-4c43-99f6-37b628aadbba\" class=\"plotly-graph-div\" style=\"height:525px; width:600px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ff1a0e17-38be-4c43-99f6-37b628aadbba\")) {                    Plotly.newPlot(                        \"ff1a0e17-38be-4c43-99f6-37b628aadbba\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[-0.00031765131279826164,0.3156936466693878,0.0032243020832538605,0.0043573444709181786,0.02283836156129837,0.1909862607717514,0.015091687440872192,0.03277648985385895,0.0030522309243679047,0.0035090791061520576,5.3514144383370876e-05,-0.00011852476745843887],[-0.00014347699470818043,0.027818620204925537,-0.0041803522035479546,0.0003439248539507389,5.809706635773182e-06,-0.001769278198480606,-0.0033143162727355957,0.039005570113658905,0.003304571844637394,-0.0014683082699775696,0.007542278617620468,-0.002905469387769699],[-0.0010985229164361954,0.0011500654509291053,-0.0004440473858267069,9.95676964521408e-06,-0.003775139572098851,0.02032054215669632,-0.0019474942237138748,-0.0006243046373128891,0.0016780085861682892,-0.00171709805727005,0.01802472025156021,-0.0001716803526505828]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003eLogit diff attribution: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Logit diff attribution\"}},\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Headwise logit diff contribution, post NMH KO\"},\"width\":600,\"margin\":{\"r\":100,\"l\":100}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ff1a0e17-38be-4c43-99f6-37b628aadbba');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#per_head_logit_diff_deltas\n",
    "\n",
    "imshow_p(\n",
    "    experiment_metrics[143000]['per_head_logit_diff_delta'], #[143000],\n",
    "    title=\"Headwise logit diff contribution, post NMH KO\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Logit diff attribution\"},\n",
    "    #coloraxis=dict(colorbar_ticksuffix = \"%\"),\n",
    "    border=True,\n",
    "    width=600,\n",
    "    margin={\"r\": 100, \"l\": 100}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['logit_diff', 'per_head_logit_diffs', 'ablation_targets', 'ablated_logit_diff', 'per_head_ablated_logit_diffs', 'per_head_logit_diff_delta', 'in_circuit_head_delta', 'outside_circuit_head_delta', 'summed_in_circuit_head_delta', 'summed_outside_circuit_head_delta', 'summed_total_head_delta'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_metrics[143000].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Checkpoint</th>\n",
       "      <th>Layer-Head</th>\n",
       "      <th>Layer</th>\n",
       "      <th>Head</th>\n",
       "      <th>Value</th>\n",
       "      <th>Previous NMH</th>\n",
       "      <th>Checkpoint_sum</th>\n",
       "      <th>Value_sum</th>\n",
       "      <th>Previous NMH_sum</th>\n",
       "      <th>Top K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>20000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>25000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>30000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006404</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>35000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.014621</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>37000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013440</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>40000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010845</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>45000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035425</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>46000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033058</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>47000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.014968</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>48000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.008163</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>52000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.020150</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>61000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.015234</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>62000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>63000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007319</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>64000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005687</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>65000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>66000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004789</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>66000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>68000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003615</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>69000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005792</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>70000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>71000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005183</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>72000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003973</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>73000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>74000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003834</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>75000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005873</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>76000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>77000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>78000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>79000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>83000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>84000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>86000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>89000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>90000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>94000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>106000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>107000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.007569</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>109000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>110000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005541</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>111000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.008061</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>113000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>114000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>116000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>121000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.008774</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>122000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006190</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>123000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.008291</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>125000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>130000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004050</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>131000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.007888</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Checkpoint       Layer-Head  Layer  Head     Value  Previous NMH  \\\n",
       "72        20000   Layer 9-Head 4      9     4  0.002460          True   \n",
       "97        25000   Layer 9-Head 4      9     4  0.006947          True   \n",
       "121       30000   Layer 9-Head 4      9     4  0.006404          True   \n",
       "149       35000   Layer 9-Head 4      9     4  0.014621          True   \n",
       "156       37000   Layer 9-Head 4      9     4  0.013440          True   \n",
       "171       40000   Layer 9-Head 4      9     4  0.010845          True   \n",
       "197       45000   Layer 9-Head 4      9     4  0.035425          True   \n",
       "202       46000   Layer 9-Head 4      9     4  0.033058          True   \n",
       "207       47000   Layer 9-Head 4      9     4  0.014968          True   \n",
       "212       48000   Layer 9-Head 4      9     4  0.008163          True   \n",
       "234       52000   Layer 9-Head 4      9     4  0.020150          True   \n",
       "276       61000   Layer 9-Head 4      9     4  0.015234          True   \n",
       "283       62000   Layer 9-Head 4      9     4  0.007663          True   \n",
       "287       63000   Layer 9-Head 4      9     4  0.007319          True   \n",
       "291       64000   Layer 9-Head 4      9     4  0.005687          True   \n",
       "295       65000   Layer 9-Head 4      9     4  0.007628          True   \n",
       "300       66000   Layer 9-Head 4      9     4  0.004789          True   \n",
       "301       66000  Layer 10-Head 7     10     7  0.005050          True   \n",
       "310       68000  Layer 10-Head 7     10     7  0.003615          True   \n",
       "316       69000   Layer 9-Head 4      9     4  0.005792          True   \n",
       "320       70000   Layer 9-Head 4      9     4  0.002369          True   \n",
       "326       71000   Layer 9-Head 4      9     4  0.005183          True   \n",
       "330       72000  Layer 10-Head 7     10     7  0.003973          True   \n",
       "335       73000   Layer 9-Head 4      9     4  0.002946          True   \n",
       "340       74000   Layer 9-Head 4      9     4  0.003834          True   \n",
       "345       75000   Layer 9-Head 4      9     4  0.005873          True   \n",
       "351       76000  Layer 10-Head 7     10     7  0.003486          True   \n",
       "355       77000  Layer 10-Head 7     10     7  0.003142          True   \n",
       "360       78000  Layer 10-Head 7     10     7  0.003221          True   \n",
       "368       79000  Layer 10-Head 7     10     7  0.001927          True   \n",
       "385       83000  Layer 10-Head 7     10     7  0.002465          True   \n",
       "390       84000  Layer 10-Head 7     10     7  0.002987          True   \n",
       "400       86000  Layer 10-Head 7     10     7  0.003382          True   \n",
       "415       89000  Layer 10-Head 7     10     7  0.003476          True   \n",
       "420       90000  Layer 10-Head 7     10     7  0.003540          True   \n",
       "440       94000  Layer 10-Head 7     10     7  0.004098          True   \n",
       "500      106000   Layer 9-Head 4      9     4  0.003578          True   \n",
       "506      107000  Layer 10-Head 7     10     7  0.007569          True   \n",
       "515      109000   Layer 9-Head 4      9     4  0.005699          True   \n",
       "520      110000   Layer 9-Head 4      9     4  0.005541          True   \n",
       "526      111000  Layer 10-Head 7     10     7  0.008061          True   \n",
       "536      113000   Layer 9-Head 4      9     4  0.005804          True   \n",
       "544      114000   Layer 9-Head 4      9     4  0.006457          True   \n",
       "550      116000   Layer 9-Head 4      9     4  0.004402          True   \n",
       "576      121000   Layer 9-Head 4      9     4  0.008774          True   \n",
       "580      122000   Layer 9-Head 4      9     4  0.006190          True   \n",
       "588      123000  Layer 10-Head 7     10     7  0.008291          True   \n",
       "596      125000   Layer 9-Head 4      9     4  0.003753          True   \n",
       "621      130000   Layer 9-Head 4      9     4  0.004050          True   \n",
       "629      131000  Layer 10-Head 7     10     7  0.007888          True   \n",
       "\n",
       "     Checkpoint_sum  Value_sum  Previous NMH_sum  Top K  \n",
       "72          3135000   0.334368                38   True  \n",
       "97          3135000   0.334368                38   True  \n",
       "121         3135000   0.334368                38   True  \n",
       "149         3135000   0.334368                38   True  \n",
       "156         3135000   0.334368                38   True  \n",
       "171         3135000   0.334368                38   True  \n",
       "197         3135000   0.334368                38   True  \n",
       "202         3135000   0.334368                38   True  \n",
       "207         3135000   0.334368                38   True  \n",
       "212         3135000   0.334368                38   True  \n",
       "234         3135000   0.334368                38   True  \n",
       "276         3135000   0.334368                38   True  \n",
       "283         3135000   0.334368                38   True  \n",
       "287         3135000   0.334368                38   True  \n",
       "291         3135000   0.334368                38   True  \n",
       "295         3135000   0.334368                38   True  \n",
       "300         3135000   0.334368                38   True  \n",
       "301         2216000   0.121681                22  False  \n",
       "310         2216000   0.121681                22  False  \n",
       "316         3135000   0.334368                38   True  \n",
       "320         3135000   0.334368                38   True  \n",
       "326         3135000   0.334368                38   True  \n",
       "330         2216000   0.121681                22  False  \n",
       "335         3135000   0.334368                38   True  \n",
       "340         3135000   0.334368                38   True  \n",
       "345         3135000   0.334368                38   True  \n",
       "351         2216000   0.121681                22  False  \n",
       "355         2216000   0.121681                22  False  \n",
       "360         2216000   0.121681                22  False  \n",
       "368         2216000   0.121681                22  False  \n",
       "385         2216000   0.121681                22  False  \n",
       "390         2216000   0.121681                22  False  \n",
       "400         2216000   0.121681                22  False  \n",
       "415         2216000   0.121681                22  False  \n",
       "420         2216000   0.121681                22  False  \n",
       "440         2216000   0.121681                22  False  \n",
       "500         3135000   0.334368                38   True  \n",
       "506         2216000   0.121681                22  False  \n",
       "515         3135000   0.334368                38   True  \n",
       "520         3135000   0.334368                38   True  \n",
       "526         2216000   0.121681                22  False  \n",
       "536         3135000   0.334368                38   True  \n",
       "544         3135000   0.334368                38   True  \n",
       "550         3135000   0.334368                38   True  \n",
       "576         3135000   0.334368                38   True  \n",
       "580         3135000   0.334368                38   True  \n",
       "588         2216000   0.121681                22  False  \n",
       "596         3135000   0.334368                38   True  \n",
       "621         3135000   0.334368                38   True  \n",
       "629         2216000   0.121681                22  False  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_backup_heads[top_backup_heads['Previous NMH']==True].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4000: set(),\n",
       " 5000: set(),\n",
       " 6000: set(),\n",
       " 7000: set(),\n",
       " 8000: {(8, 2)},\n",
       " 9000: set(),\n",
       " 10000: {(8, 1), (8, 10)},\n",
       " 11000: {(10, 7)},\n",
       " 12000: {(8, 2), (10, 7)},\n",
       " 13000: {(10, 7)},\n",
       " 14000: {(8, 2), (10, 7)},\n",
       " 15000: {(8, 2), (10, 7)},\n",
       " 16000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 17000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 18000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 19000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 20000: {(8, 2), (10, 7)},\n",
       " 21000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 22000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 23000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 24000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 25000: {(8, 2), (10, 7)},\n",
       " 26000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 27000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 28000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 29000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 30000: {(8, 2), (10, 7)},\n",
       " 31000: {(8, 1), (8, 2), (9, 4), (10, 7)},\n",
       " 32000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 33000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 34000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 35000: {(8, 2), (8, 10), (10, 7)},\n",
       " 36000: {(8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 37000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 38000: {(8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 39000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 40000: {(8, 2), (8, 10), (10, 7)},\n",
       " 41000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 42000: {(8, 2), (9, 4), (10, 7)},\n",
       " 43000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 44000: {(8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 45000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 46000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 47000: {(8, 2), (8, 10), (10, 7)},\n",
       " 48000: {(8, 2), (8, 10), (10, 7)},\n",
       " 49000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 50000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 51000: {(8, 2), (9, 4), (10, 7)},\n",
       " 52000: {(8, 2), (8, 10)},\n",
       " 53000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 54000: {(8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 55000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 56000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 57000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 58000: {(8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 59000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 60000: {(8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 61000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 62000: {(8, 2), (8, 10), (10, 7)},\n",
       " 63000: {(8, 2), (8, 10), (10, 7)},\n",
       " 64000: {(8, 2), (8, 10), (10, 7)},\n",
       " 65000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 66000: {(8, 2), (8, 10)},\n",
       " 67000: {(8, 2), (8, 10), (10, 7)},\n",
       " 68000: {(8, 2), (8, 10)},\n",
       " 69000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 70000: {(8, 2), (8, 10), (10, 7)},\n",
       " 71000: {(8, 2), (8, 10), (10, 7)},\n",
       " 72000: {(8, 2), (8, 10)},\n",
       " 73000: {(8, 2), (8, 10), (10, 7)},\n",
       " 74000: {(8, 2), (8, 10)},\n",
       " 75000: {(8, 1), (8, 2), (8, 10)},\n",
       " 76000: {(8, 2), (8, 10)},\n",
       " 77000: {(8, 1), (8, 2), (8, 10)},\n",
       " 78000: {(8, 2), (8, 10)},\n",
       " 79000: {(8, 2), (8, 10)},\n",
       " 80000: {(8, 2), (8, 10)},\n",
       " 81000: {(8, 2), (8, 10)},\n",
       " 82000: {(8, 2), (8, 10)},\n",
       " 83000: {(8, 2), (8, 10)},\n",
       " 84000: {(8, 2), (8, 10)},\n",
       " 85000: {(8, 2), (8, 10)},\n",
       " 86000: {(8, 1), (8, 2), (8, 10)},\n",
       " 87000: {(8, 1), (8, 2), (8, 10)},\n",
       " 88000: {(8, 2), (8, 10)},\n",
       " 89000: {(8, 10)},\n",
       " 90000: {(8, 2), (8, 10)},\n",
       " 91000: {(8, 1), (8, 2), (8, 10)},\n",
       " 92000: {(8, 1), (8, 2), (8, 10)},\n",
       " 93000: {(8, 2), (8, 10)},\n",
       " 94000: {(8, 2), (8, 10)},\n",
       " 95000: {(8, 2), (8, 10)},\n",
       " 96000: {(8, 2), (8, 10)},\n",
       " 97000: {(8, 2), (8, 10)},\n",
       " 98000: {(8, 1), (8, 2), (8, 10)},\n",
       " 99000: {(8, 1), (8, 2), (8, 10)},\n",
       " 100000: {(8, 2), (8, 10)},\n",
       " 101000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 102000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 103000: {(8, 2), (8, 10)},\n",
       " 104000: {(8, 2), (8, 10)},\n",
       " 105000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 106000: {(8, 1), (8, 10), (10, 7)},\n",
       " 107000: {(8, 2), (8, 10)},\n",
       " 108000: {(8, 2), (8, 10)},\n",
       " 109000: {(8, 2), (8, 10)},\n",
       " 110000: {(8, 2), (8, 10), (10, 7)},\n",
       " 111000: {(8, 2), (8, 10)},\n",
       " 112000: {(5, 0), (8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 113000: {(8, 1), (8, 10), (10, 7)},\n",
       " 114000: {(8, 2), (8, 10)},\n",
       " 115000: {(8, 10)},\n",
       " 116000: {(8, 2), (8, 10), (10, 7)},\n",
       " 117000: {(8, 10)},\n",
       " 118000: {(8, 2), (8, 10), (10, 7)},\n",
       " 119000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 120000: {(8, 2), (8, 10), (10, 7)},\n",
       " 121000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 122000: {(5, 0), (8, 2), (8, 10), (10, 7)},\n",
       " 123000: {(8, 2), (8, 10)},\n",
       " 124000: {(5, 0), (8, 2), (8, 10), (10, 7)},\n",
       " 125000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 126000: {(5, 0), (8, 2), (8, 10)},\n",
       " 127000: {(8, 2), (8, 10)},\n",
       " 128000: {(8, 2), (8, 10)},\n",
       " 129000: {(8, 2), (8, 10)},\n",
       " 130000: {(8, 2), (8, 10)},\n",
       " 131000: {(8, 2), (8, 10)},\n",
       " 132000: {(5, 0), (8, 2), (8, 10)},\n",
       " 133000: set(),\n",
       " 134000: {(8, 2), (8, 10)},\n",
       " 135000: {(8, 2), (8, 10)},\n",
       " 136000: {(8, 2), (8, 10)},\n",
       " 137000: {(5, 0), (8, 2), (8, 10)},\n",
       " 138000: {(5, 0), (8, 2), (8, 10)},\n",
       " 139000: {(8, 2), (8, 10)},\n",
       " 140000: {(8, 2), (8, 10)},\n",
       " 141000: {(8, 2), (8, 10)},\n",
       " 142000: {(5, 0), (8, 2), (8, 10)},\n",
       " 143000: {(8, 2), (8, 10)}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_nmhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4000: set(),\n",
       " 5000: set(),\n",
       " 6000: set(),\n",
       " 7000: set(),\n",
       " 8000: {(8, 2)},\n",
       " 9000: {(8, 2)},\n",
       " 10000: {(8, 1), (8, 2), (8, 10)},\n",
       " 11000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 12000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 13000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 14000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 15000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 16000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 17000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 18000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 19000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 20000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 21000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 22000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 23000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 24000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 25000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 26000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 27000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 28000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 29000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 30000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 31000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 32000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 33000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 34000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 35000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 36000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 37000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 38000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 39000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 40000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 41000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 42000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 43000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 44000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 45000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 46000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 47000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 48000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 49000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 50000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 51000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 52000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 53000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 54000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 55000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 56000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 57000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 58000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 59000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 60000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 61000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 62000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 63000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 64000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 65000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 66000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 67000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 68000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 69000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 70000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 71000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 72000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 73000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 74000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 75000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 76000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 77000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 78000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 79000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 80000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 81000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 82000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 83000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 84000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 85000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 86000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 87000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 88000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 89000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 90000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 91000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 92000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 93000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 94000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 95000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 96000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 97000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 98000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 99000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 100000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 101000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 102000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 103000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 104000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 105000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 106000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 107000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 108000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 109000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 110000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 111000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 112000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 113000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 114000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 115000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 116000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 117000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 118000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 119000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 120000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 121000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 122000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 123000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 124000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 125000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 126000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 127000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 128000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 129000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 130000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 131000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 132000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 133000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 134000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 135000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 136000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 137000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 138000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 139000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 140000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 141000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 142000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 143000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative_nmhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Checkpoint=%{x}<br>Number of NMHs=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          4000,
          5000,
          6000,
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          126000,
          127000,
          128000,
          129000,
          130000,
          131000,
          132000,
          133000,
          134000,
          135000,
          136000,
          137000,
          138000,
          139000,
          140000,
          141000,
          142000,
          143000
         ],
         "xaxis": "x",
         "y": [
          0,
          0,
          0,
          0,
          1,
          0,
          2,
          1,
          2,
          1,
          2,
          2,
          5,
          5,
          5,
          5,
          2,
          5,
          5,
          5,
          5,
          2,
          5,
          5,
          5,
          5,
          2,
          4,
          5,
          5,
          5,
          3,
          4,
          4,
          4,
          5,
          3,
          5,
          3,
          5,
          4,
          4,
          4,
          3,
          3,
          5,
          5,
          3,
          2,
          5,
          4,
          5,
          5,
          5,
          4,
          5,
          4,
          4,
          3,
          3,
          3,
          4,
          2,
          3,
          2,
          4,
          3,
          3,
          2,
          3,
          2,
          3,
          2,
          3,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          2,
          1,
          2,
          3,
          3,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          2,
          4,
          4,
          2,
          2,
          5,
          3,
          2,
          2,
          2,
          3,
          2,
          5,
          3,
          2,
          1,
          3,
          1,
          3,
          4,
          3,
          4,
          4,
          2,
          4,
          4,
          3,
          2,
          2,
          2,
          2,
          2,
          3,
          0,
          2,
          2,
          2,
          3,
          3,
          2,
          2,
          2,
          3,
          2
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Number of NMHs Over Time (pythia-160m)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Checkpoint"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Number of NMHs"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"ac1fd460-59e7-451f-812d-e6e46a4d6776\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ac1fd460-59e7-451f-812d-e6e46a4d6776\")) {                    Plotly.newPlot(                        \"ac1fd460-59e7-451f-812d-e6e46a4d6776\",                        [{\"hovertemplate\":\"Checkpoint=%{x}\\u003cbr\\u003eNumber of NMHs=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4000,5000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000],\"xaxis\":\"x\",\"y\":[0,0,0,0,1,0,2,1,2,1,2,2,5,5,5,5,2,5,5,5,5,2,5,5,5,5,2,4,5,5,5,3,4,4,4,5,3,5,3,5,4,4,4,3,3,5,5,3,2,5,4,5,5,5,4,5,4,4,3,3,3,4,2,3,2,4,3,3,2,3,2,3,2,3,2,2,2,2,2,2,2,2,3,3,2,1,2,3,3,2,2,2,2,2,3,3,2,4,4,2,2,5,3,2,2,2,3,2,5,3,2,1,3,1,3,4,3,4,4,2,4,4,3,2,2,2,2,2,3,0,2,2,2,3,3,2,2,2,3,2],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Checkpoint\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Number of NMHs\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Number of NMHs Over Time (pythia-160m)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ac1fd460-59e7-451f-812d-e6e46a4d6776');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot number of nmhs over time\n",
    "fig = px.line(\n",
    "    x=list(checkpoint_nmhs.keys()), \n",
    "    y=list([len(heads) for heads in checkpoint_nmhs.values()]), \n",
    "    title=f\"Number of NMHs Over Time ({MODEL_TO_VIEW})\",\n",
    "    labels={'x': 'Checkpoint', 'y': 'Number of NMHs'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
