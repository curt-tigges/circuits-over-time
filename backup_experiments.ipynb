{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "import einops\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from torch import Tensor\n",
    "from torchtyping import TensorType as TT\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from utils.data_utils import generate_data_and_caches\n",
    "from utils.data_processing import (\n",
    "    load_edge_scores_into_dictionary,\n",
    ")\n",
    "from utils.visualization import plot_attention_heads, imshow_p\n",
    "from utils.backup_analysis import (\n",
    "    load_model,\n",
    "    run_iteration,\n",
    "    process_backup_results,\n",
    "    get_past_nmhs_for_checkpoints,\n",
    "    plot_top_heads\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7ff7c54634c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TASK = 'ioi'\n",
    "PERFORMANCE_METRIC = 'logit_diff'\n",
    "BASE_MODEL = \"pythia-160m\"\n",
    "VARIANT = None\n",
    "CACHE = \"model_cache\"\n",
    "IOI_DATASET_SIZE = 70\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = 'ioi'\n",
    "PERFORMANCE_METRIC = 'logit_diff'\n",
    "BASE_MODEL = \"pythia-160m\"\n",
    "VARIANT = \"EleutherAI/pythia-160m-alldropout\"\n",
    "MODEL_SHORTNAME = BASE_MODEL if not VARIANT else VARIANT[11:]\n",
    "CACHE = \"model_cache\"\n",
    "IOI_DATASET_SIZE = 70\n",
    "COPY_SCORE_THRESHOLD = 75.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/143: results/graphs/pythia-160m-alldropout/ioi/57000.json\n",
      "Processing file 2/143: results/graphs/pythia-160m-alldropout/ioi/141000.json\n",
      "Processing file 3/143: results/graphs/pythia-160m-alldropout/ioi/95000.json\n",
      "Processing file 4/143: results/graphs/pythia-160m-alldropout/ioi/107000.json\n",
      "Processing file 5/143: results/graphs/pythia-160m-alldropout/ioi/34000.json\n",
      "Processing file 6/143: results/graphs/pythia-160m-alldropout/ioi/6000.json\n",
      "Processing file 7/143: results/graphs/pythia-160m-alldropout/ioi/37000.json\n",
      "Processing file 8/143: results/graphs/pythia-160m-alldropout/ioi/39000.json\n",
      "Processing file 9/143: results/graphs/pythia-160m-alldropout/ioi/104000.json\n",
      "Processing file 10/143: results/graphs/pythia-160m-alldropout/ioi/59000.json\n",
      "Processing file 11/143: results/graphs/pythia-160m-alldropout/ioi/67000.json\n",
      "Processing file 12/143: results/graphs/pythia-160m-alldropout/ioi/111000.json\n",
      "Processing file 13/143: results/graphs/pythia-160m-alldropout/ioi/76000.json\n",
      "Processing file 14/143: results/graphs/pythia-160m-alldropout/ioi/5000.json\n",
      "Processing file 15/143: results/graphs/pythia-160m-alldropout/ioi/42000.json\n",
      "Processing file 16/143: results/graphs/pythia-160m-alldropout/ioi/77000.json\n",
      "Processing file 17/143: results/graphs/pythia-160m-alldropout/ioi/86000.json\n",
      "Processing file 18/143: results/graphs/pythia-160m-alldropout/ioi/80000.json\n",
      "Processing file 19/143: results/graphs/pythia-160m-alldropout/ioi/81000.json\n",
      "Processing file 20/143: results/graphs/pythia-160m-alldropout/ioi/63000.json\n",
      "Processing file 21/143: results/graphs/pythia-160m-alldropout/ioi/142000.json\n",
      "Processing file 22/143: results/graphs/pythia-160m-alldropout/ioi/56000.json\n",
      "Processing file 23/143: results/graphs/pythia-160m-alldropout/ioi/8000.json\n",
      "Processing file 24/143: results/graphs/pythia-160m-alldropout/ioi/93000.json\n",
      "Processing file 25/143: results/graphs/pythia-160m-alldropout/ioi/120000.json\n",
      "Processing file 26/143: results/graphs/pythia-160m-alldropout/ioi/62000.json\n",
      "Processing file 27/143: results/graphs/pythia-160m-alldropout/ioi/70000.json\n",
      "Processing file 28/143: results/graphs/pythia-160m-alldropout/ioi/19000.json\n",
      "Processing file 29/143: results/graphs/pythia-160m-alldropout/ioi/121000.json\n",
      "Processing file 30/143: results/graphs/pythia-160m-alldropout/ioi/105000.json\n",
      "Processing file 31/143: results/graphs/pythia-160m-alldropout/ioi/129000.json\n",
      "Processing file 32/143: results/graphs/pythia-160m-alldropout/ioi/2000.json\n",
      "Processing file 33/143: results/graphs/pythia-160m-alldropout/ioi/96000.json\n",
      "Processing file 34/143: results/graphs/pythia-160m-alldropout/ioi/124000.json\n",
      "Processing file 35/143: results/graphs/pythia-160m-alldropout/ioi/143000.json\n",
      "Processing file 36/143: results/graphs/pythia-160m-alldropout/ioi/79000.json\n",
      "Processing file 37/143: results/graphs/pythia-160m-alldropout/ioi/29000.json\n",
      "Processing file 38/143: results/graphs/pythia-160m-alldropout/ioi/137000.json\n",
      "Processing file 39/143: results/graphs/pythia-160m-alldropout/ioi/10000.json\n",
      "Processing file 40/143: results/graphs/pythia-160m-alldropout/ioi/135000.json\n",
      "Processing file 41/143: results/graphs/pythia-160m-alldropout/ioi/65000.json\n",
      "Processing file 42/143: results/graphs/pythia-160m-alldropout/ioi/60000.json\n",
      "Processing file 43/143: results/graphs/pythia-160m-alldropout/ioi/90000.json\n",
      "Processing file 44/143: results/graphs/pythia-160m-alldropout/ioi/106000.json\n",
      "Processing file 45/143: results/graphs/pythia-160m-alldropout/ioi/1000.json\n",
      "Processing file 46/143: results/graphs/pythia-160m-alldropout/ioi/33000.json\n",
      "Processing file 47/143: results/graphs/pythia-160m-alldropout/ioi/103000.json\n",
      "Processing file 48/143: results/graphs/pythia-160m-alldropout/ioi/113000.json\n",
      "Processing file 49/143: results/graphs/pythia-160m-alldropout/ioi/35000.json\n",
      "Processing file 50/143: results/graphs/pythia-160m-alldropout/ioi/133000.json\n",
      "Processing file 51/143: results/graphs/pythia-160m-alldropout/ioi/18000.json\n",
      "Processing file 52/143: results/graphs/pythia-160m-alldropout/ioi/55000.json\n",
      "Processing file 53/143: results/graphs/pythia-160m-alldropout/ioi/102000.json\n",
      "Processing file 54/143: results/graphs/pythia-160m-alldropout/ioi/108000.json\n",
      "Processing file 55/143: results/graphs/pythia-160m-alldropout/ioi/49000.json\n",
      "Processing file 56/143: results/graphs/pythia-160m-alldropout/ioi/130000.json\n",
      "Processing file 57/143: results/graphs/pythia-160m-alldropout/ioi/83000.json\n",
      "Processing file 58/143: results/graphs/pythia-160m-alldropout/ioi/31000.json\n",
      "Processing file 59/143: results/graphs/pythia-160m-alldropout/ioi/46000.json\n",
      "Processing file 60/143: results/graphs/pythia-160m-alldropout/ioi/112000.json\n",
      "Processing file 61/143: results/graphs/pythia-160m-alldropout/ioi/26000.json\n",
      "Processing file 62/143: results/graphs/pythia-160m-alldropout/ioi/78000.json\n",
      "Processing file 63/143: results/graphs/pythia-160m-alldropout/ioi/13000.json\n",
      "Processing file 64/143: results/graphs/pythia-160m-alldropout/ioi/47000.json\n",
      "Processing file 65/143: results/graphs/pythia-160m-alldropout/ioi/58000.json\n",
      "Processing file 66/143: results/graphs/pythia-160m-alldropout/ioi/134000.json\n",
      "Processing file 67/143: results/graphs/pythia-160m-alldropout/ioi/100000.json\n",
      "Processing file 68/143: results/graphs/pythia-160m-alldropout/ioi/138000.json\n",
      "Processing file 69/143: results/graphs/pythia-160m-alldropout/ioi/27000.json\n",
      "Processing file 70/143: results/graphs/pythia-160m-alldropout/ioi/48000.json\n",
      "Processing file 71/143: results/graphs/pythia-160m-alldropout/ioi/91000.json\n",
      "Processing file 72/143: results/graphs/pythia-160m-alldropout/ioi/122000.json\n",
      "Processing file 73/143: results/graphs/pythia-160m-alldropout/ioi/99000.json\n",
      "Processing file 74/143: results/graphs/pythia-160m-alldropout/ioi/32000.json\n",
      "Processing file 75/143: results/graphs/pythia-160m-alldropout/ioi/30000.json\n",
      "Processing file 76/143: results/graphs/pythia-160m-alldropout/ioi/44000.json\n",
      "Processing file 77/143: results/graphs/pythia-160m-alldropout/ioi/136000.json\n",
      "Processing file 78/143: results/graphs/pythia-160m-alldropout/ioi/116000.json\n",
      "Processing file 79/143: results/graphs/pythia-160m-alldropout/ioi/74000.json\n",
      "Processing file 80/143: results/graphs/pythia-160m-alldropout/ioi/118000.json\n",
      "Processing file 81/143: results/graphs/pythia-160m-alldropout/ioi/94000.json\n",
      "Processing file 82/143: results/graphs/pythia-160m-alldropout/ioi/119000.json\n",
      "Processing file 83/143: results/graphs/pythia-160m-alldropout/ioi/64000.json\n",
      "Processing file 84/143: results/graphs/pythia-160m-alldropout/ioi/69000.json\n",
      "Processing file 85/143: results/graphs/pythia-160m-alldropout/ioi/140000.json\n",
      "Processing file 86/143: results/graphs/pythia-160m-alldropout/ioi/72000.json\n",
      "Processing file 87/143: results/graphs/pythia-160m-alldropout/ioi/117000.json\n",
      "Processing file 88/143: results/graphs/pythia-160m-alldropout/ioi/9000.json\n",
      "Processing file 89/143: results/graphs/pythia-160m-alldropout/ioi/98000.json\n",
      "Processing file 90/143: results/graphs/pythia-160m-alldropout/ioi/110000.json\n",
      "Processing file 91/143: results/graphs/pythia-160m-alldropout/ioi/88000.json\n",
      "Processing file 92/143: results/graphs/pythia-160m-alldropout/ioi/16000.json\n",
      "Processing file 93/143: results/graphs/pythia-160m-alldropout/ioi/23000.json\n",
      "Processing file 94/143: results/graphs/pythia-160m-alldropout/ioi/109000.json\n",
      "Processing file 95/143: results/graphs/pythia-160m-alldropout/ioi/22000.json\n",
      "Processing file 96/143: results/graphs/pythia-160m-alldropout/ioi/17000.json\n",
      "Processing file 97/143: results/graphs/pythia-160m-alldropout/ioi/73000.json\n",
      "Processing file 98/143: results/graphs/pythia-160m-alldropout/ioi/3000.json\n",
      "Processing file 99/143: results/graphs/pythia-160m-alldropout/ioi/71000.json\n",
      "Processing file 100/143: results/graphs/pythia-160m-alldropout/ioi/125000.json\n",
      "Processing file 101/143: results/graphs/pythia-160m-alldropout/ioi/84000.json\n",
      "Processing file 102/143: results/graphs/pythia-160m-alldropout/ioi/87000.json\n",
      "Processing file 103/143: results/graphs/pythia-160m-alldropout/ioi/131000.json\n",
      "Processing file 104/143: results/graphs/pythia-160m-alldropout/ioi/24000.json\n",
      "Processing file 105/143: results/graphs/pythia-160m-alldropout/ioi/128000.json\n",
      "Processing file 106/143: results/graphs/pythia-160m-alldropout/ioi/51000.json\n",
      "Processing file 107/143: results/graphs/pythia-160m-alldropout/ioi/52000.json\n",
      "Processing file 108/143: results/graphs/pythia-160m-alldropout/ioi/132000.json\n",
      "Processing file 109/143: results/graphs/pythia-160m-alldropout/ioi/101000.json\n",
      "Processing file 110/143: results/graphs/pythia-160m-alldropout/ioi/89000.json\n",
      "Processing file 111/143: results/graphs/pythia-160m-alldropout/ioi/40000.json\n",
      "Processing file 112/143: results/graphs/pythia-160m-alldropout/ioi/43000.json\n",
      "Processing file 113/143: results/graphs/pythia-160m-alldropout/ioi/54000.json\n",
      "Processing file 114/143: results/graphs/pythia-160m-alldropout/ioi/11000.json\n",
      "Processing file 115/143: results/graphs/pythia-160m-alldropout/ioi/36000.json\n",
      "Processing file 116/143: results/graphs/pythia-160m-alldropout/ioi/45000.json\n",
      "Processing file 117/143: results/graphs/pythia-160m-alldropout/ioi/50000.json\n",
      "Processing file 118/143: results/graphs/pythia-160m-alldropout/ioi/114000.json\n",
      "Processing file 119/143: results/graphs/pythia-160m-alldropout/ioi/66000.json\n",
      "Processing file 120/143: results/graphs/pythia-160m-alldropout/ioi/28000.json\n",
      "Processing file 121/143: results/graphs/pythia-160m-alldropout/ioi/82000.json\n",
      "Processing file 122/143: results/graphs/pythia-160m-alldropout/ioi/127000.json\n",
      "Processing file 123/143: results/graphs/pythia-160m-alldropout/ioi/14000.json\n",
      "Processing file 124/143: results/graphs/pythia-160m-alldropout/ioi/75000.json\n",
      "Processing file 125/143: results/graphs/pythia-160m-alldropout/ioi/115000.json\n",
      "Processing file 126/143: results/graphs/pythia-160m-alldropout/ioi/12000.json\n",
      "Processing file 127/143: results/graphs/pythia-160m-alldropout/ioi/21000.json\n",
      "Processing file 128/143: results/graphs/pythia-160m-alldropout/ioi/38000.json\n",
      "Processing file 129/143: results/graphs/pythia-160m-alldropout/ioi/15000.json\n",
      "Processing file 130/143: results/graphs/pythia-160m-alldropout/ioi/85000.json\n",
      "Processing file 131/143: results/graphs/pythia-160m-alldropout/ioi/139000.json\n",
      "Processing file 132/143: results/graphs/pythia-160m-alldropout/ioi/61000.json\n",
      "Processing file 133/143: results/graphs/pythia-160m-alldropout/ioi/7000.json\n",
      "Processing file 134/143: results/graphs/pythia-160m-alldropout/ioi/68000.json\n",
      "Processing file 135/143: results/graphs/pythia-160m-alldropout/ioi/53000.json\n",
      "Processing file 136/143: results/graphs/pythia-160m-alldropout/ioi/25000.json\n",
      "Processing file 137/143: results/graphs/pythia-160m-alldropout/ioi/97000.json\n",
      "Processing file 138/143: results/graphs/pythia-160m-alldropout/ioi/4000.json\n",
      "Processing file 139/143: results/graphs/pythia-160m-alldropout/ioi/123000.json\n",
      "Processing file 140/143: results/graphs/pythia-160m-alldropout/ioi/20000.json\n",
      "Processing file 141/143: results/graphs/pythia-160m-alldropout/ioi/41000.json\n",
      "Processing file 142/143: results/graphs/pythia-160m-alldropout/ioi/126000.json\n",
      "Processing file 143/143: results/graphs/pythia-160m-alldropout/ioi/92000.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = f'results/graphs/{MODEL_SHORTNAME}/{TASK}'\n",
    "df = load_edge_scores_into_dictionary(folder_path)\n",
    "\n",
    "# filter everything before 1000 steps\n",
    "df = df[df['checkpoint'] >= 1000]\n",
    "\n",
    "df[['source', 'target']] = df['edge'].str.split('->', expand=True)\n",
    "len(df['target'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step143000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "initial_model = load_model(BASE_MODEL, VARIANT, 143000, CACHE, device)\n",
    "size=70\n",
    "ioi_dataset, abc_dataset = generate_data_and_caches(initial_model, size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imshow_p(\n",
    "#     per_head_ablated_logit_diffs,\n",
    "#     title=\"Headwise logit diff contribution, post NMH KO\",\n",
    "#     labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Logit diff attribution\"},\n",
    "#     #coloraxis=dict(colorbar_ticksuffix = \"%\"),\n",
    "#     border=True,\n",
    "#     width=600,\n",
    "#     margin={\"r\": 100, \"l\": 100}\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step4000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 47.61904761904761%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 4000:\n",
      "Heads ablated:            [(9, 11), (9, 4), (7, 6), (7, 10), (8, 9)]\n",
      "Original logit diff:      0.7503997087\n",
      "Post ablation logit diff: 0.8393040299\n",
      "Logit diff % change:      11.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step5000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Checkpoint 5000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      1.5525176525\n",
      "Post ablation logit diff: 1.5525176525\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step6000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 57.14285714285714%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Checkpoint 6000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (7, 6), (9, 11)]\n",
      "Original logit diff:      1.7438025475\n",
      "Post ablation logit diff: 1.5554083586\n",
      "Logit diff % change:      -10.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step7000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 52.38095238095239%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 70.95238095238095%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 66.66666666666666%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 73.80952380952381%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 7000:\n",
      "Heads ablated:            [(9, 11), (7, 10), (9, 4), (8, 9)]\n",
      "Original logit diff:      1.6021207571\n",
      "Post ablation logit diff: 1.9914293289\n",
      "Logit diff % change:      24.30%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step8000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 70.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 68.57142857142857%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 56.19047619047619%\n",
      "Checkpoint 8000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (9, 11)]\n",
      "Original logit diff:      2.1946053505\n",
      "Post ablation logit diff: 2.3080902100\n",
      "Logit diff % change:      5.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step9000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 66.66666666666666%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 61.904761904761905%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 79.04761904761905%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 73.33333333333333%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Checkpoint 9000:\n",
      "Heads ablated:            [(8, 9), (9, 11), (9, 9), (9, 4)]\n",
      "Original logit diff:      2.2502546310\n",
      "Post ablation logit diff: 2.4143366814\n",
      "Logit diff % change:      7.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step10000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 78.57142857142857%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 58.0952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 76.19047619047619%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 63.33333333333333%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Checkpoint 10000:\n",
      "Heads ablated:            [(9, 9), (9, 11), (9, 8), (8, 9), (9, 4)]\n",
      "Original logit diff:      2.0763607025\n",
      "Post ablation logit diff: 2.7305092812\n",
      "Logit diff % change:      31.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step11000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 61.42857142857143%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 77.61904761904762%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 58.57142857142858%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Checkpoint 11000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (8, 9), (9, 4)]\n",
      "Original logit diff:      2.8749055862\n",
      "Post ablation logit diff: 3.3432726860\n",
      "Logit diff % change:      16.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step12000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 61.42857142857143%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 78.0952380952381%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 58.57142857142858%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 12000:\n",
      "Heads ablated:            [(9, 11), (9, 9), (9, 8), (8, 1), (8, 9)]\n",
      "Original logit diff:      2.9505121708\n",
      "Post ablation logit diff: 3.0151007175\n",
      "Logit diff % change:      2.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step13000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 67.61904761904762%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 80.0%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 33.80952380952381%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 18.095238095238095%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Checkpoint 13000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (8, 9)]\n",
      "Original logit diff:      2.6273641586\n",
      "Post ablation logit diff: 2.9601230621\n",
      "Logit diff % change:      12.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step14000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 20.952380952380953%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 38.57142857142858%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 65.71428571428571%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 80.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Checkpoint 14000:\n",
      "Heads ablated:            [(8, 9), (9, 4), (9, 11), (9, 8)]\n",
      "Original logit diff:      2.6580519676\n",
      "Post ablation logit diff: 3.2050671577\n",
      "Logit diff % change:      20.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step15000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 35.23809523809524%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 80.95238095238095%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 67.14285714285714%\n",
      "Checkpoint 15000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (9, 8), (9, 11)]\n",
      "Original logit diff:      2.7998774052\n",
      "Post ablation logit diff: 3.5529823303\n",
      "Logit diff % change:      26.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step16000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 79.52380952380952%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 79.04761904761905%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 70.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 18.095238095238095%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 26.190476190476193%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 21.428571428571427%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 16000:\n",
      "Heads ablated:            [(9, 8), (9, 9), (9, 11), (9, 4), (8, 9)]\n",
      "Original logit diff:      2.5674970150\n",
      "Post ablation logit diff: 3.1832628250\n",
      "Logit diff % change:      23.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step17000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 22.380952380952383%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 73.33333333333333%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 81.9047619047619%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 20.952380952380953%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 23.809523809523807%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 16.19047619047619%\n",
      "Checkpoint 17000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.2116754055\n",
      "Post ablation logit diff: 3.7808332443\n",
      "Logit diff % change:      17.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step18000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 13.80952380952381%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 28.095238095238095%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 26.666666666666668%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 80.47619047619048%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 73.80952380952381%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 71.9047619047619%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 22.857142857142858%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Checkpoint 18000:\n",
      "Heads ablated:            [(8, 9), (8, 2), (9, 8), (9, 11)]\n",
      "Original logit diff:      3.0395021439\n",
      "Post ablation logit diff: 3.7396328449\n",
      "Logit diff % change:      23.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step19000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 32.857142857142854%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 81.42857142857143%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 73.80952380952381%\n",
      "Checkpoint 19000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (9, 8), (9, 11)]\n",
      "Original logit diff:      3.0793039799\n",
      "Post ablation logit diff: 3.6368265152\n",
      "Logit diff % change:      18.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step20000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 24.285714285714285%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 71.42857142857143%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 14.761904761904763%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 30.476190476190478%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Checkpoint 20000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (7, 10), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.4340963364\n",
      "Post ablation logit diff: 3.9933707714\n",
      "Logit diff % change:      16.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step21000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 24.761904761904763%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 72.85714285714285%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 78.0952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Checkpoint 21000:\n",
      "Heads ablated:            [(9, 11), (9, 9), (9, 8), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.5647156239\n",
      "Post ablation logit diff: 4.1162972450\n",
      "Logit diff % change:      15.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step22000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 72.38095238095238%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 18.095238095238095%\n",
      "Checkpoint 22000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.3527474403\n",
      "Post ablation logit diff: 3.5867624283\n",
      "Logit diff % change:      6.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step23000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 72.38095238095238%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 74.28571428571429%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 81.9047619047619%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 36.19047619047619%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Checkpoint 23000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (8, 1), (8, 2), (8, 9)]\n",
      "Original logit diff:      3.6167130470\n",
      "Post ablation logit diff: 4.1623573303\n",
      "Logit diff % change:      15.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step24000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 38.095238095238095%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 18.571428571428573%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 71.42857142857143%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Checkpoint 24000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (7, 10), (9, 8), (9, 11)]\n",
      "Original logit diff:      3.4534935951\n",
      "Post ablation logit diff: 4.0211248398\n",
      "Logit diff % change:      16.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step25000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 29.04761904761905%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 70.0%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 81.42857142857143%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 13.80952380952381%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 33.80952380952381%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 18.571428571428573%\n",
      "Checkpoint 25000:\n",
      "Heads ablated:            [(9, 11), (9, 9), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.8888685703\n",
      "Post ablation logit diff: 4.1350383759\n",
      "Logit diff % change:      6.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step26000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 22.380952380952383%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 40.476190476190474%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 74.28571428571429%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 28.095238095238095%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 26000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (7, 10), (9, 8), (9, 11)]\n",
      "Original logit diff:      3.6521568298\n",
      "Post ablation logit diff: 3.9535486698\n",
      "Logit diff % change:      8.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step27000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 43.80952380952381%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 23.809523809523807%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 77.61904761904762%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 34.76190476190476%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 71.9047619047619%\n",
      "Checkpoint 27000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (9, 8), (9, 9), (9, 11)]\n",
      "Original logit diff:      3.6870427132\n",
      "Post ablation logit diff: 4.1167435646\n",
      "Logit diff % change:      11.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step28000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 44.761904761904766%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 79.04761904761905%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 35.23809523809524%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 77.61904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 25.71428571428571%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Checkpoint 28000:\n",
      "Heads ablated:            [(8, 9), (8, 2), (8, 1), (7, 10), (10, 10), (9, 11), (9, 9), (9, 8), (9, 4)]\n",
      "Original logit diff:      3.8017516136\n",
      "Post ablation logit diff: 3.7000479698\n",
      "Logit diff % change:      -2.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step29000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 35.23809523809524%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 46.666666666666664%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 25.71428571428571%\n",
      "Checkpoint 29000:\n",
      "Heads ablated:            [(10, 10), (9, 11), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.8330845833\n",
      "Post ablation logit diff: 4.2445011139\n",
      "Logit diff % change:      10.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step30000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 24.761904761904763%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 47.14285714285714%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 81.9047619047619%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 37.61904761904762%\n",
      "Checkpoint 30000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 8), (9, 11), (10, 10)]\n",
      "Original logit diff:      4.1986637115\n",
      "Post ablation logit diff: 4.4636082649\n",
      "Logit diff % change:      6.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step31000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 40.476190476190474%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 80.95238095238095%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 73.80952380952381%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 46.19047619047619%\n",
      "Checkpoint 31000:\n",
      "Heads ablated:            [(10, 10), (9, 11), (7, 10), (8, 1), (8, 2), (8, 9), (9, 4), (9, 8)]\n",
      "Original logit diff:      3.7670619488\n",
      "Post ablation logit diff: 4.0057406425\n",
      "Logit diff % change:      6.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step32000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 34.76190476190476%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 80.0%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 75.23809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 55.714285714285715%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 32.38095238095238%\n",
      "Checkpoint 32000:\n",
      "Heads ablated:            [(10, 10), (9, 11), (9, 9), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.2646412849\n",
      "Post ablation logit diff: 4.2193803787\n",
      "Logit diff % change:      -1.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step33000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 50.95238095238095%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 32.857142857142854%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 78.0952380952381%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 38.095238095238095%\n",
      "Checkpoint 33000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (7, 10), (9, 8), (10, 10), (9, 11), (9, 9)]\n",
      "Original logit diff:      4.1034574509\n",
      "Post ablation logit diff: 3.6126854420\n",
      "Logit diff % change:      -11.96%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step34000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 52.38095238095239%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 75.23809523809524%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 79.52380952380952%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 39.523809523809526%\n",
      "Checkpoint 34000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (7, 10), (9, 8), (10, 10), (9, 11), (9, 9)]\n",
      "Original logit diff:      4.5543885231\n",
      "Post ablation logit diff: 4.6401953697\n",
      "Logit diff % change:      1.88%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step35000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 40.476190476190474%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 77.61904761904762%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 73.80952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 52.85714285714286%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Checkpoint 35000:\n",
      "Heads ablated:            [(10, 10), (9, 11), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.9483010769\n",
      "Post ablation logit diff: 4.3063626289\n",
      "Logit diff % change:      9.07%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step36000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 49.523809523809526%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 74.28571428571429%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 41.42857142857143%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 79.04761904761905%\n",
      "Checkpoint 36000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (7, 10), (9, 8), (9, 11), (10, 10)]\n",
      "Original logit diff:      4.1632523537\n",
      "Post ablation logit diff: 4.3864612579\n",
      "Logit diff % change:      5.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step37000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 47.14285714285714%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 80.47619047619048%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 77.61904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 57.14285714285714%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Checkpoint 37000:\n",
      "Heads ablated:            [(10, 10), (9, 11), (9, 9), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.2827534676\n",
      "Post ablation logit diff: 4.3034749031\n",
      "Logit diff % change:      0.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step38000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 47.61904761904761%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 80.95238095238095%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 56.666666666666664%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Checkpoint 38000:\n",
      "Heads ablated:            [(10, 10), (9, 9), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.5856761932\n",
      "Post ablation logit diff: 4.8034510612\n",
      "Logit diff % change:      4.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step39000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 59.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 72.85714285714285%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 48.095238095238095%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 39000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 8), (10, 10), (9, 11)]\n",
      "Original logit diff:      4.0642728806\n",
      "Post ablation logit diff: 4.5869531631\n",
      "Logit diff % change:      12.86%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step40000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 46.666666666666664%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 71.9047619047619%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 60.476190476190474%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Checkpoint 40000:\n",
      "Heads ablated:            [(10, 10), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.2882342339\n",
      "Post ablation logit diff: 4.9187889099\n",
      "Logit diff % change:      14.70%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step41000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 49.523809523809526%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 82.38095238095238%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 73.80952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 62.857142857142854%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Checkpoint 41000:\n",
      "Heads ablated:            [(10, 10), (9, 8), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.5821361542\n",
      "Post ablation logit diff: 5.2278418541\n",
      "Logit diff % change:      14.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step42000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 49.047619047619044%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 78.0952380952381%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 75.23809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 57.61904761904761%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 33.33333333333333%\n",
      "Checkpoint 42000:\n",
      "Heads ablated:            [(10, 10), (9, 9), (9, 8), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.4717941284\n",
      "Post ablation logit diff: 4.9502186775\n",
      "Logit diff % change:      10.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step43000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 49.523809523809526%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 80.95238095238095%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 74.28571428571429%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 58.0952380952381%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 37.142857142857146%\n",
      "Checkpoint 43000:\n",
      "Heads ablated:            [(10, 10), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.4700336456\n",
      "Post ablation logit diff: 5.0850300789\n",
      "Logit diff % change:      13.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step44000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 62.857142857142854%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 75.23809523809524%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 80.95238095238095%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 50.95238095238095%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 44000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 8), (9, 9), (10, 10), (9, 11)]\n",
      "Original logit diff:      4.3092579842\n",
      "Post ablation logit diff: 4.3763160706\n",
      "Logit diff % change:      1.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step45000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 51.42857142857142%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 79.04761904761905%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 57.61904761904761%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 38.095238095238095%\n",
      "Checkpoint 45000:\n",
      "Heads ablated:            [(9, 11), (10, 10), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.6193852425\n",
      "Post ablation logit diff: 5.0978932381\n",
      "Logit diff % change:      10.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step46000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 61.42857142857143%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 79.04761904761905%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 50.95238095238095%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 78.57142857142857%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Checkpoint 46000:\n",
      "Heads ablated:            [(8, 9), (8, 2), (8, 1), (9, 4), (10, 10), (9, 11), (9, 9), (9, 8)]\n",
      "Original logit diff:      4.3712697029\n",
      "Post ablation logit diff: 4.4973921776\n",
      "Logit diff % change:      2.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step47000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 78.0952380952381%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 55.714285714285715%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 79.04761904761905%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 63.33333333333333%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 47000:\n",
      "Heads ablated:            [(9, 8), (9, 9), (9, 11), (10, 10), (9, 4), (8, 1), (8, 9), (8, 2)]\n",
      "Original logit diff:      4.4275708199\n",
      "Post ablation logit diff: 4.5920276642\n",
      "Logit diff % change:      3.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step48000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 63.8095238095238%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 51.90476190476191%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Checkpoint 48000:\n",
      "Heads ablated:            [(8, 9), (8, 2), (9, 4), (10, 10), (9, 9), (9, 8)]\n",
      "Original logit diff:      4.1561117172\n",
      "Post ablation logit diff: 4.8135166168\n",
      "Logit diff % change:      15.82%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step49000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 57.14285714285714%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 72.85714285714285%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 63.8095238095238%\n",
      "Checkpoint 49000:\n",
      "Heads ablated:            [(10, 10), (9, 11), (8, 1), (8, 2), (8, 9), (9, 4), (9, 8)]\n",
      "Original logit diff:      4.7086668015\n",
      "Post ablation logit diff: 5.2478537560\n",
      "Logit diff % change:      11.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step50000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 56.19047619047619%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 80.47619047619048%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 75.23809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 63.8095238095238%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 39.04761904761905%\n",
      "Checkpoint 50000:\n",
      "Heads ablated:            [(9, 11), (10, 10), (9, 9), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.2389316559\n",
      "Post ablation logit diff: 4.5170273781\n",
      "Logit diff % change:      6.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step51000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 69.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 80.95238095238095%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 73.80952380952381%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 54.285714285714285%\n",
      "Checkpoint 51000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 8), (10, 10), (9, 11)]\n",
      "Original logit diff:      4.3836593628\n",
      "Post ablation logit diff: 5.1815490723\n",
      "Logit diff % change:      18.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step52000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 66.19047619047619%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 79.52380952380952%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 60.476190476190474%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Checkpoint 52000:\n",
      "Heads ablated:            [(8, 9), (8, 2), (8, 1), (9, 4), (10, 10), (9, 8)]\n",
      "Original logit diff:      4.3767466545\n",
      "Post ablation logit diff: 5.6696691513\n",
      "Logit diff % change:      29.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step53000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 66.19047619047619%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 37.142857142857146%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 83.80952380952381%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 60.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Checkpoint 53000:\n",
      "Heads ablated:            [(8, 9), (8, 2), (8, 1), (9, 4), (10, 10), (9, 8)]\n",
      "Original logit diff:      4.0848083496\n",
      "Post ablation logit diff: 5.0482902527\n",
      "Logit diff % change:      23.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step54000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 62.38095238095238%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 75.23809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 37.61904761904762%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 67.14285714285714%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Checkpoint 54000:\n",
      "Heads ablated:            [(9, 11), (10, 10), (9, 9), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.7141408920\n",
      "Post ablation logit diff: 4.8273401260\n",
      "Logit diff % change:      2.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step55000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 59.523809523809526%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 82.38095238095238%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 66.66666666666666%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 39.04761904761905%\n",
      "Checkpoint 55000:\n",
      "Heads ablated:            [(9, 11), (10, 10), (9, 8), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.4829554558\n",
      "Post ablation logit diff: 5.2408919334\n",
      "Logit diff % change:      16.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step56000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 63.33333333333333%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 82.38095238095238%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 78.57142857142857%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 66.19047619047619%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 40.0%\n",
      "Checkpoint 56000:\n",
      "Heads ablated:            [(10, 10), (9, 11), (9, 9), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.4195761681\n",
      "Post ablation logit diff: 4.4943790436\n",
      "Logit diff % change:      1.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step57000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 63.33333333333333%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 80.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 68.0952380952381%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 38.57142857142858%\n",
      "Checkpoint 57000:\n",
      "Heads ablated:            [(9, 11), (10, 10), (9, 9), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.5725593567\n",
      "Post ablation logit diff: 4.7409248352\n",
      "Logit diff % change:      3.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step58000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 41.904761904761905%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 67.61904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 80.47619047619048%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 64.76190476190476%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Checkpoint 58000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 8), (10, 10), (9, 11)]\n",
      "Original logit diff:      4.5261950493\n",
      "Post ablation logit diff: 4.8779811859\n",
      "Logit diff % change:      7.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step59000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 82.38095238095238%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 76.19047619047619%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 66.19047619047619%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 80.95238095238095%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 38.57142857142858%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 67.61904761904762%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 59000:\n",
      "Heads ablated:            [(9, 8), (9, 9), (9, 11), (10, 10), (9, 4), (8, 2), (8, 9)]\n",
      "Original logit diff:      4.2955198288\n",
      "Post ablation logit diff: 4.8485035896\n",
      "Logit diff % change:      12.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step60000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 67.14285714285714%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 83.80952380952381%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 81.9047619047619%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 70.0%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 60000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 8), (9, 9), (10, 10), (9, 11)]\n",
      "Original logit diff:      4.5323991776\n",
      "Post ablation logit diff: 4.7737164497\n",
      "Logit diff % change:      5.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step61000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 66.66666666666666%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 80.95238095238095%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 71.9047619047619%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 41.904761904761905%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 68.57142857142857%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Checkpoint 61000:\n",
      "Heads ablated:            [(9, 11), (10, 10), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.6385593414\n",
      "Post ablation logit diff: 5.3187398911\n",
      "Logit diff % change:      14.66%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step62000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 66.66666666666666%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 40.476190476190474%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 73.33333333333333%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Checkpoint 62000:\n",
      "Heads ablated:            [(8, 9), (8, 2), (8, 1), (9, 4), (10, 10), (9, 11), (9, 8)]\n",
      "Original logit diff:      4.3011960983\n",
      "Post ablation logit diff: 4.7344746590\n",
      "Logit diff % change:      10.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step63000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 73.33333333333333%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 81.42857142857143%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 83.80952380952381%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 66.19047619047619%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 41.42857142857143%\n",
      "Checkpoint 63000:\n",
      "Heads ablated:            [(10, 10), (9, 11), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.3011336327\n",
      "Post ablation logit diff: 4.9533047676\n",
      "Logit diff % change:      15.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step64000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 40.95238095238095%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 66.66666666666666%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 75.23809523809524%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 72.38095238095238%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Checkpoint 64000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 8), (9, 9), (10, 10), (9, 11)]\n",
      "Original logit diff:      4.6724824905\n",
      "Post ablation logit diff: 4.9069676399\n",
      "Logit diff % change:      5.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step65000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 73.80952380952381%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 72.38095238095238%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 60.476190476190474%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 40.476190476190474%\n",
      "Checkpoint 65000:\n",
      "Heads ablated:            [(10, 10), (9, 11), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.0308432579\n",
      "Post ablation logit diff: 4.6819267273\n",
      "Logit diff % change:      16.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step66000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 42.38095238095238%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 61.42857142857143%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 72.38095238095238%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 71.42857142857143%\n",
      "Checkpoint 66000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (9, 8), (9, 11), (10, 10)]\n",
      "Original logit diff:      3.9939069748\n",
      "Post ablation logit diff: 4.6598958969\n",
      "Logit diff % change:      16.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step67000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 73.33333333333333%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 81.42857142857143%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 59.04761904761905%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 40.476190476190474%\n",
      "Checkpoint 67000:\n",
      "Heads ablated:            [(10, 10), (9, 11), (9, 8), (8, 1), (8, 2), (8, 9)]\n",
      "Original logit diff:      4.3274483681\n",
      "Post ablation logit diff: 5.3949599266\n",
      "Logit diff % change:      24.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step68000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 39.523809523809526%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 60.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 72.38095238095238%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 81.9047619047619%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 73.33333333333333%\n",
      "Checkpoint 68000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 8), (9, 11), (10, 10)]\n",
      "Original logit diff:      4.4930891991\n",
      "Post ablation logit diff: 5.2709922791\n",
      "Logit diff % change:      17.31%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step69000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 41.42857142857143%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 59.04761904761905%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 76.19047619047619%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 72.85714285714285%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Checkpoint 69000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 1), (9, 8), (9, 9), (10, 10), (9, 11)]\n",
      "Original logit diff:      4.3300323486\n",
      "Post ablation logit diff: 4.4531407356\n",
      "Logit diff % change:      2.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step70000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 57.14285714285714%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 42.38095238095238%\n",
      "Checkpoint 70000:\n",
      "Heads ablated:            [(10, 9), (10, 10), (9, 11), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.8486249447\n",
      "Post ablation logit diff: 4.4910316467\n",
      "Logit diff % change:      16.69%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step71000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 74.28571428571429%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 83.80952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 42.857142857142854%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 60.952380952380956%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Checkpoint 71000:\n",
      "Heads ablated:            [(9, 11), (10, 10), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.9566597939\n",
      "Post ablation logit diff: 4.3486576080\n",
      "Logit diff % change:      9.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step72000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 45.23809523809524%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 57.61904761904761%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Copy circuit for head 10.2 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Checkpoint 72000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 8), (9, 9), (10, 10), (9, 11)]\n",
      "Original logit diff:      4.0465149879\n",
      "Post ablation logit diff: 4.3066534996\n",
      "Logit diff % change:      6.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step73000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 77.14285714285715%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 78.0952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 59.04761904761905%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 44.285714285714285%\n",
      "Checkpoint 73000:\n",
      "Heads ablated:            [(10, 9), (10, 10), (9, 11), (9, 9), (9, 8), (8, 1), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.9963457584\n",
      "Post ablation logit diff: 4.2917528152\n",
      "Logit diff % change:      7.39%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step74000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 79.52380952380952%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 58.0952380952381%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 90.47619047619048%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 48.57142857142857%\n",
      "Checkpoint 74000:\n",
      "Heads ablated:            [(10, 9), (10, 10), (9, 11), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.9810400009\n",
      "Post ablation logit diff: 4.8340358734\n",
      "Logit diff % change:      21.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step75000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 74.28571428571429%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 57.61904761904761%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 49.047619047619044%\n",
      "Checkpoint 75000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.9297153950\n",
      "Post ablation logit diff: 4.7122321129\n",
      "Logit diff % change:      19.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step76000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 74.28571428571429%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 75.23809523809524%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 46.19047619047619%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 58.0952380952381%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Checkpoint 76000:\n",
      "Heads ablated:            [(9, 11), (10, 9), (10, 10), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.9551839828\n",
      "Post ablation logit diff: 4.7231678963\n",
      "Logit diff % change:      19.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step77000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 58.0952380952381%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 79.04761904761905%\n",
      "Checkpoint 77000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 8), (10, 10), (9, 11), (10, 9)]\n",
      "Original logit diff:      4.0902109146\n",
      "Post ablation logit diff: 4.7998328209\n",
      "Logit diff % change:      17.35%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step78000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 79.04761904761905%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 60.476190476190474%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 78000:\n",
      "Heads ablated:            [(9, 4), (9, 8), (9, 9), (9, 11), (10, 9), (10, 10), (8, 1), (8, 9)]\n",
      "Original logit diff:      4.1592407227\n",
      "Post ablation logit diff: 4.5878028870\n",
      "Logit diff % change:      10.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step79000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 47.14285714285714%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 77.61904761904762%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 10.9 (sign=1) : Top 5 accuracy: 76.19047619047619%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 82.38095238095238%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 79000:\n",
      "Heads ablated:            [(9, 4), (9, 8), (9, 9), (9, 11), (10, 9), (10, 10), (8, 1), (8, 2), (8, 9)]\n",
      "Original logit diff:      4.1969671249\n",
      "Post ablation logit diff: 4.3464016914\n",
      "Logit diff % change:      3.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step80000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 61.42857142857143%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 48.57142857142857%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 80.47619047619048%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Checkpoint 80000:\n",
      "Heads ablated:            [(8, 9), (8, 2), (9, 8), (9, 11)]\n",
      "Original logit diff:      3.8981869221\n",
      "Post ablation logit diff: 4.9297637939\n",
      "Logit diff % change:      26.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step81000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 90.47619047619048%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 60.952380952380956%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 55.23809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 80.95238095238095%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 74.28571428571429%\n",
      "Checkpoint 81000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 8), (9, 11)]\n",
      "Original logit diff:      3.7783634663\n",
      "Post ablation logit diff: 4.1968059540\n",
      "Logit diff % change:      11.07%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step82000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 62.38095238095238%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 66.66666666666666%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 71.9047619047619%\n",
      "Checkpoint 82000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 8), (10, 10), (9, 11)]\n",
      "Original logit diff:      3.8537945747\n",
      "Post ablation logit diff: 4.1508545876\n",
      "Logit diff % change:      7.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step83000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 64.76190476190476%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 90.47619047619048%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 70.95238095238095%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Checkpoint 83000:\n",
      "Heads ablated:            [(8, 9), (8, 2), (8, 1), (9, 4), (9, 6), (9, 11), (9, 8)]\n",
      "Original logit diff:      3.9027187824\n",
      "Post ablation logit diff: 3.7222180367\n",
      "Logit diff % change:      -4.63%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step84000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Checkpoint 84000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.7948217392\n",
      "Post ablation logit diff: 3.7924461365\n",
      "Logit diff % change:      -0.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step85000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 55.23809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Checkpoint 85000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4), (9, 6)]\n",
      "Original logit diff:      3.6635603905\n",
      "Post ablation logit diff: 3.2390737534\n",
      "Logit diff % change:      -11.59%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step86000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 51.90476190476191%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Checkpoint 86000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (8, 1), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.8036525249\n",
      "Post ablation logit diff: 4.1245656013\n",
      "Logit diff % change:      8.44%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step87000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 59.523809523809526%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 87000:\n",
      "Heads ablated:            [(9, 8), (9, 11), (9, 6), (8, 1), (8, 9)]\n",
      "Original logit diff:      3.4856913090\n",
      "Post ablation logit diff: 3.3863351345\n",
      "Logit diff % change:      -2.85%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step88000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 78.0952380952381%\n",
      "Checkpoint 88000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (9, 6), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.8774125576\n",
      "Post ablation logit diff: 3.2408926487\n",
      "Logit diff % change:      -16.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step89000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 74.28571428571429%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 45.714285714285715%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Checkpoint 89000:\n",
      "Heads ablated:            [(9, 6), (8, 9), (8, 2), (8, 1), (9, 8), (9, 11)]\n",
      "Original logit diff:      3.6312358379\n",
      "Post ablation logit diff: 3.1907584667\n",
      "Logit diff % change:      -12.13%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step90000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 58.57142857142858%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 53.333333333333336%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 71.42857142857143%\n",
      "Checkpoint 90000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (8, 1), (8, 2), (8, 9)]\n",
      "Original logit diff:      4.0732855797\n",
      "Post ablation logit diff: 3.8399069309\n",
      "Logit diff % change:      -5.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step91000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 79.52380952380952%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 62.857142857142854%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 67.61904761904762%\n",
      "Checkpoint 91000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 1), (9, 6), (9, 8), (9, 11)]\n",
      "Original logit diff:      3.6509788036\n",
      "Post ablation logit diff: 3.2772970200\n",
      "Logit diff % change:      -10.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step92000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 63.8095238095238%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 63.33333333333333%\n",
      "Checkpoint 92000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 1), (9, 8), (9, 11)]\n",
      "Original logit diff:      3.7650799751\n",
      "Post ablation logit diff: 3.8739407063\n",
      "Logit diff % change:      2.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step93000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 62.857142857142854%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 64.76190476190476%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 42.857142857142854%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Checkpoint 93000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (8, 1), (8, 9), (9, 4), (9, 6)]\n",
      "Original logit diff:      3.8510830402\n",
      "Post ablation logit diff: 3.1161036491\n",
      "Logit diff % change:      -19.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step94000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 59.04761904761905%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 63.33333333333333%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 46.19047619047619%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.4 (sign=1) : Top 5 accuracy: 30.952380952380953%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 52.38095238095239%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.3 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 75.23809523809524%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 55.23809523809524%\n",
      "Checkpoint 94000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (9, 6), (7, 10), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.6854364872\n",
      "Post ablation logit diff: 2.7154066563\n",
      "Logit diff % change:      -26.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step95000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 63.8095238095238%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 66.66666666666666%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Checkpoint 95000:\n",
      "Heads ablated:            [(9, 11), (8, 1), (8, 2), (8, 9), (9, 4), (9, 8)]\n",
      "Original logit diff:      3.8157775402\n",
      "Post ablation logit diff: 3.7719552517\n",
      "Logit diff % change:      -1.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step96000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 62.857142857142854%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 66.19047619047619%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Checkpoint 96000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (8, 1), (8, 2), (8, 9)]\n",
      "Original logit diff:      3.3174202442\n",
      "Post ablation logit diff: 3.4195487499\n",
      "Logit diff % change:      3.08%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step97000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 76.19047619047619%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 66.66666666666666%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 68.0952380952381%\n",
      "Checkpoint 97000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 6), (9, 8)]\n",
      "Original logit diff:      3.5353643894\n",
      "Post ablation logit diff: 2.9611625671\n",
      "Logit diff % change:      -16.24%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step98000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 63.8095238095238%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 69.52380952380952%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Checkpoint 98000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (9, 6), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      3.7724151611\n",
      "Post ablation logit diff: 2.7300455570\n",
      "Logit diff % change:      -27.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step99000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 58.57142857142858%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 66.66666666666666%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Checkpoint 99000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (8, 1), (8, 2), (8, 9), (9, 6)]\n",
      "Original logit diff:      3.8813066483\n",
      "Post ablation logit diff: 3.1179172993\n",
      "Logit diff % change:      -19.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step100000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 71.42857142857143%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 70.0%\n",
      "Checkpoint 100000:\n",
      "Heads ablated:            [(9, 6), (8, 9), (8, 2), (8, 1), (9, 8)]\n",
      "Original logit diff:      4.0274939537\n",
      "Post ablation logit diff: 3.1162800789\n",
      "Logit diff % change:      -22.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step101000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 62.38095238095238%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 73.80952380952381%\n",
      "Checkpoint 101000:\n",
      "Heads ablated:            [(9, 6), (8, 9), (8, 2), (8, 1), (9, 8)]\n",
      "Original logit diff:      3.8571720123\n",
      "Post ablation logit diff: 3.1220989227\n",
      "Logit diff % change:      -19.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step102000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 72.85714285714285%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 59.523809523809526%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Checkpoint 102000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (8, 1), (8, 2), (8, 9), (9, 4), (9, 6)]\n",
      "Original logit diff:      4.0581960678\n",
      "Post ablation logit diff: 2.8886754513\n",
      "Logit diff % change:      -28.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step103000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 62.38095238095238%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 80.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 79.04761904761905%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 73.33333333333333%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Checkpoint 103000:\n",
      "Heads ablated:            [(8, 9), (8, 2), (8, 1), (9, 4), (9, 6), (9, 11), (9, 8)]\n",
      "Original logit diff:      4.1358156204\n",
      "Post ablation logit diff: 2.9038691521\n",
      "Logit diff % change:      -29.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step104000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 80.95238095238095%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 64.28571428571429%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 78.57142857142857%\n",
      "Checkpoint 104000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 6), (9, 8), (9, 9)]\n",
      "Original logit diff:      3.7935748100\n",
      "Post ablation logit diff: 2.3894762993\n",
      "Logit diff % change:      -37.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step105000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 81.9047619047619%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 64.76190476190476%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 36.666666666666664%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 80.95238095238095%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 78.57142857142857%\n",
      "Checkpoint 105000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 6), (9, 8), (9, 11), (9, 9)]\n",
      "Original logit diff:      4.0199747086\n",
      "Post ablation logit diff: 2.4916658401\n",
      "Logit diff % change:      -38.02%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step106000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 69.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Checkpoint 106000:\n",
      "Heads ablated:            [(9, 6), (9, 4), (8, 9), (8, 2), (8, 1), (9, 8)]\n",
      "Original logit diff:      4.3689026833\n",
      "Post ablation logit diff: 2.9052648544\n",
      "Logit diff % change:      -33.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step107000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 60.0%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 70.95238095238095%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 61.42857142857143%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.3 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Checkpoint 107000:\n",
      "Heads ablated:            [(9, 9), (9, 8), (9, 6), (7, 10), (8, 1), (8, 2), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.2003755569\n",
      "Post ablation logit diff: 2.6227154732\n",
      "Logit diff % change:      -37.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step108000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 24.285714285714285%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 62.38095238095238%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 70.95238095238095%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 59.04761904761905%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Checkpoint 108000:\n",
      "Heads ablated:            [(9, 11), (9, 9), (9, 8), (7, 10), (8, 1), (8, 2), (8, 9), (9, 4), (9, 6)]\n",
      "Original logit diff:      4.2692241669\n",
      "Post ablation logit diff: 2.6608245373\n",
      "Logit diff % change:      -37.67%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step109000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 62.38095238095238%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 72.38095238095238%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 59.04761904761905%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Checkpoint 109000:\n",
      "Heads ablated:            [(9, 8), (7, 10), (8, 1), (8, 2), (8, 9), (9, 4), (9, 6)]\n",
      "Original logit diff:      4.2349190712\n",
      "Post ablation logit diff: 2.9743044376\n",
      "Logit diff % change:      -29.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step110000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 59.523809523809526%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 71.9047619047619%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Checkpoint 110000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 2), (8, 1), (9, 6), (9, 8)]\n",
      "Original logit diff:      4.3877296448\n",
      "Post ablation logit diff: 2.8735165596\n",
      "Logit diff % change:      -34.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step111000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 65.23809523809524%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Checkpoint 111000:\n",
      "Heads ablated:            [(9, 9), (9, 8), (7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 4), (9, 6)]\n",
      "Original logit diff:      4.1714777946\n",
      "Post ablation logit diff: 2.4911799431\n",
      "Logit diff % change:      -40.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step112000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 65.23809523809524%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 75.23809523809524%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 59.04761904761905%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 90.47619047619048%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Checkpoint 112000:\n",
      "Heads ablated:            [(9, 8), (7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 4), (9, 6)]\n",
      "Original logit diff:      4.2715706825\n",
      "Post ablation logit diff: 2.8390722275\n",
      "Logit diff % change:      -33.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step113000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 65.71428571428571%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 56.666666666666664%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 76.19047619047619%\n",
      "Checkpoint 113000:\n",
      "Heads ablated:            [(9, 11), (9, 9), (7, 10), (8, 1), (8, 2), (8, 9), (9, 4), (8, 5)]\n",
      "Original logit diff:      4.9458160400\n",
      "Post ablation logit diff: 3.6229934692\n",
      "Logit diff % change:      -26.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step114000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 65.71428571428571%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 76.19047619047619%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Checkpoint 114000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (9, 6), (7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.3948512077\n",
      "Post ablation logit diff: 3.0252194405\n",
      "Logit diff % change:      -31.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step115000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 64.28571428571429%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.4 (sign=1) : Top 5 accuracy: 39.523809523809526%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 53.333333333333336%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 115000:\n",
      "Heads ablated:            [(9, 8), (9, 9), (9, 11), (7, 10), (8, 1), (8, 2), (8, 5), (8, 9)]\n",
      "Original logit diff:      4.2679810524\n",
      "Post ablation logit diff: 3.1340122223\n",
      "Logit diff % change:      -26.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step116000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 53.80952380952381%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 78.0952380952381%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 67.14285714285714%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Checkpoint 116000:\n",
      "Heads ablated:            [(8, 9), (8, 5), (8, 2), (8, 1), (7, 10), (9, 4), (9, 6), (9, 9), (9, 8)]\n",
      "Original logit diff:      4.4814324379\n",
      "Post ablation logit diff: 2.8943748474\n",
      "Logit diff % change:      -35.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step117000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 53.333333333333336%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 79.04761904761905%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 117000:\n",
      "Heads ablated:            [(9, 6), (8, 9), (8, 5), (8, 1), (7, 10)]\n",
      "Original logit diff:      4.7434997559\n",
      "Post ablation logit diff: 3.5868151188\n",
      "Logit diff % change:      -24.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step118000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 66.19047619047619%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 79.52380952380952%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 48.095238095238095%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Checkpoint 118000:\n",
      "Heads ablated:            [(9, 11), (9, 8), (7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 4), (9, 6)]\n",
      "Original logit diff:      4.5004386902\n",
      "Post ablation logit diff: 2.9751758575\n",
      "Logit diff % change:      -33.89%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step119000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 49.047619047619044%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 78.0952380952381%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 67.61904761904762%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Checkpoint 119000:\n",
      "Heads ablated:            [(9, 6), (9, 4), (8, 9), (8, 5), (8, 2), (8, 1), (7, 10), (9, 8), (9, 11)]\n",
      "Original logit diff:      4.4462542534\n",
      "Post ablation logit diff: 2.6933503151\n",
      "Logit diff % change:      -39.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step120000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 52.85714285714286%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 80.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 67.61904761904762%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Checkpoint 120000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 5), (8, 2), (8, 1), (7, 10), (9, 6), (9, 9), (9, 11)]\n",
      "Original logit diff:      4.4726037979\n",
      "Post ablation logit diff: 2.5502011776\n",
      "Logit diff % change:      -42.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step121000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 69.04761904761905%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 54.761904761904766%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 78.57142857142857%\n",
      "Checkpoint 121000:\n",
      "Heads ablated:            [(9, 9), (7, 10), (8, 1), (8, 9), (9, 4), (8, 5)]\n",
      "Original logit diff:      4.6029114723\n",
      "Post ablation logit diff: 3.2199792862\n",
      "Logit diff % change:      -30.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step122000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 51.90476190476191%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 80.95238095238095%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 69.04761904761905%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Checkpoint 122000:\n",
      "Heads ablated:            [(9, 9), (9, 6), (9, 4), (8, 9), (8, 5), (8, 1), (7, 10), (9, 11)]\n",
      "Original logit diff:      4.5792465210\n",
      "Post ablation logit diff: 2.8671545982\n",
      "Logit diff % change:      -37.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step123000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 81.42857142857143%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 90.47619047619048%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 67.61904761904762%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 81.9047619047619%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 48.095238095238095%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 123000:\n",
      "Heads ablated:            [(9, 8), (9, 9), (9, 6), (7, 10), (8, 1), (8, 2), (8, 5), (8, 9)]\n",
      "Original logit diff:      4.7253069878\n",
      "Post ablation logit diff: 3.2800340652\n",
      "Logit diff % change:      -30.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step124000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 82.38095238095238%\n",
      "Checkpoint 124000:\n",
      "Heads ablated:            [(7, 10), (8, 1), (8, 9), (8, 5)]\n",
      "Original logit diff:      4.6726117134\n",
      "Post ablation logit diff: 3.3874263763\n",
      "Logit diff % change:      -27.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step125000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 49.047619047619044%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 82.38095238095238%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 66.66666666666666%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Checkpoint 125000:\n",
      "Heads ablated:            [(8, 9), (8, 5), (8, 2), (8, 1), (7, 10), (9, 11)]\n",
      "Original logit diff:      4.6287317276\n",
      "Post ablation logit diff: 3.2932188511\n",
      "Logit diff % change:      -28.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step126000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 83.80952380952381%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 69.52380952380952%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Checkpoint 126000:\n",
      "Heads ablated:            [(9, 6), (9, 4), (8, 9), (8, 5), (8, 1), (7, 10), (9, 9), (9, 11)]\n",
      "Original logit diff:      4.3907866478\n",
      "Post ablation logit diff: 2.3879914284\n",
      "Logit diff % change:      -45.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step127000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 69.04761904761905%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 52.38095238095239%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Checkpoint 127000:\n",
      "Heads ablated:            [(9, 11), (9, 9), (9, 6), (7, 10), (8, 1), (8, 2), (8, 9), (9, 4), (8, 5)]\n",
      "Original logit diff:      4.9994950294\n",
      "Post ablation logit diff: 2.9266698360\n",
      "Logit diff % change:      -41.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step128000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 51.90476190476191%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 83.80952380952381%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 70.95238095238095%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Checkpoint 128000:\n",
      "Heads ablated:            [(9, 6), (9, 4), (8, 9), (8, 5), (8, 1), (7, 10), (9, 11)]\n",
      "Original logit diff:      4.9464960098\n",
      "Post ablation logit diff: 3.1869676113\n",
      "Logit diff % change:      -35.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step129000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 129000:\n",
      "Heads ablated:            [(8, 1), (8, 5), (8, 9)]\n",
      "Original logit diff:      5.0566964149\n",
      "Post ablation logit diff: 3.7143292427\n",
      "Logit diff % change:      -26.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step130000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 51.42857142857142%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 69.52380952380952%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Checkpoint 130000:\n",
      "Heads ablated:            [(9, 6), (8, 9), (8, 5), (8, 2), (8, 1), (7, 10), (9, 9), (9, 11)]\n",
      "Original logit diff:      4.6716670990\n",
      "Post ablation logit diff: 2.9467730522\n",
      "Logit diff % change:      -36.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step131000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 70.47619047619048%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 49.523809523809526%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Checkpoint 131000:\n",
      "Heads ablated:            [(9, 11), (7, 10), (8, 1), (8, 9), (9, 4), (8, 5)]\n",
      "Original logit diff:      4.6185445786\n",
      "Post ablation logit diff: 3.3961455822\n",
      "Logit diff % change:      -26.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step132000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 53.80952380952381%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 69.52380952380952%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Checkpoint 132000:\n",
      "Heads ablated:            [(9, 6), (9, 4), (8, 9), (8, 5), (8, 1), (7, 10), (9, 11)]\n",
      "Original logit diff:      4.8236279488\n",
      "Post ablation logit diff: 3.3111436367\n",
      "Logit diff % change:      -31.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step133000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 70.47619047619048%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 53.333333333333336%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Checkpoint 133000:\n",
      "Heads ablated:            [(9, 11), (9, 9), (7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 4), (9, 6)]\n",
      "Original logit diff:      4.8249945641\n",
      "Post ablation logit diff: 2.7618398666\n",
      "Logit diff % change:      -42.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step134000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 71.42857142857143%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Checkpoint 134000:\n",
      "Heads ablated:            [(9, 11), (7, 10), (8, 1), (8, 5), (8, 9), (9, 4)]\n",
      "Original logit diff:      4.6599798203\n",
      "Post ablation logit diff: 3.4568464756\n",
      "Logit diff % change:      -25.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step135000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 72.85714285714285%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 57.14285714285714%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Checkpoint 135000:\n",
      "Heads ablated:            [(9, 9), (7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 4), (9, 6)]\n",
      "Original logit diff:      4.8110241890\n",
      "Post ablation logit diff: 2.9844381809\n",
      "Logit diff % change:      -37.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step136000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 49.523809523809526%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 71.9047619047619%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Checkpoint 136000:\n",
      "Heads ablated:            [(8, 9), (8, 5), (8, 2), (8, 1), (7, 10), (9, 6), (9, 9), (9, 11)]\n",
      "Original logit diff:      4.2932896614\n",
      "Post ablation logit diff: 2.8151009083\n",
      "Logit diff % change:      -34.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step137000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 74.28571428571429%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 52.38095238095239%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Checkpoint 137000:\n",
      "Heads ablated:            [(9, 9), (7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 4), (9, 6)]\n",
      "Original logit diff:      4.6687407494\n",
      "Post ablation logit diff: 2.9402670860\n",
      "Logit diff % change:      -37.02%\n",
      "Loaded model EleutherAI/pythia-160m-alldropout at step138000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 72.85714285714285%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 51.90476190476191%\n",
      "Checkpoint 138000:\n",
      "Heads ablated:            [(9, 9), (7, 10), (8, 1), (8, 5), (8, 9), (9, 4), (9, 6)]\n",
      "Original logit diff:      4.7791352272\n",
      "Post ablation logit diff: 2.9975485802\n",
      "Logit diff % change:      -37.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step139000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 72.85714285714285%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 51.90476190476191%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Checkpoint 139000:\n",
      "Heads ablated:            [(9, 9), (7, 10), (8, 1), (8, 5), (8, 9), (9, 6)]\n",
      "Original logit diff:      4.6881947517\n",
      "Post ablation logit diff: 3.4748618603\n",
      "Logit diff % change:      -25.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step140000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 53.333333333333336%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 73.33333333333333%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Checkpoint 140000:\n",
      "Heads ablated:            [(9, 6), (9, 4), (8, 9), (8, 5), (8, 2), (8, 1), (7, 10), (9, 9), (9, 11)]\n",
      "Original logit diff:      4.4293336868\n",
      "Post ablation logit diff: 2.7516005039\n",
      "Logit diff % change:      -37.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step141000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 50.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 73.33333333333333%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Checkpoint 141000:\n",
      "Heads ablated:            [(9, 4), (8, 9), (8, 5), (8, 1), (7, 10), (9, 6), (9, 9), (9, 11)]\n",
      "Original logit diff:      4.7626280785\n",
      "Post ablation logit diff: 3.1539742947\n",
      "Logit diff % change:      -33.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step142000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 75.23809523809524%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 52.38095238095239%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 82.38095238095238%\n",
      "Checkpoint 142000:\n",
      "Heads ablated:            [(9, 11), (9, 9), (7, 9), (7, 10), (8, 1), (8, 5), (8, 9), (9, 4), (9, 6)]\n",
      "Original logit diff:      4.9462704659\n",
      "Post ablation logit diff: 3.4209973812\n",
      "Logit diff % change:      -30.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-alldropout at step143000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 5.11 (sign=1) : Top 5 accuracy: 59.523809523809526%\n",
      "Copy circuit for head 9.11 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 75.23809523809524%\n",
      "Copy circuit for head 7.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 52.85714285714286%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Checkpoint 143000:\n",
      "Heads ablated:            [(9, 11), (9, 9), (7, 9), (7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 4), (9, 6)]\n",
      "Original logit diff:      4.8632206917\n",
      "Post ablation logit diff: 3.1182472706\n",
      "Logit diff % change:      -35.88%\n"
     ]
    }
   ],
   "source": [
    "experiment_metrics = dict()\n",
    "# create folder\n",
    "os.makedirs(f'results/backup/{MODEL_SHORTNAME}', exist_ok=True)\n",
    "\n",
    "for checkpoint in range(4000, 144000, 1000):\n",
    "\n",
    "    experiment_metrics = run_iteration(\n",
    "        BASE_MODEL, VARIANT, df, checkpoint=checkpoint, dataset=ioi_dataset, experiment_metrics=experiment_metrics, \n",
    "        threshold=COPY_SCORE_THRESHOLD\n",
    "    )\n",
    "    experiment_metrics = process_backup_results(df, checkpoint, experiment_metrics)\n",
    "\n",
    "    # save to file, using pytorch format\n",
    "    torch.save(experiment_metrics, f'results/backup/{MODEL_SHORTNAME}/nmh_backup_metrics.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pythia 160m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TO_VIEW = \"pythia-160m-alldropout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_metrics = torch.load(f'results/backup/{MODEL_TO_VIEW}/nmh_backup_metrics.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['logit_diff', 'per_head_logit_diffs', 'ablation_targets', 'ablated_logit_diff', 'per_head_ablated_logit_diffs', 'per_head_logit_diff_delta', 'in_circuit_head_delta', 'outside_circuit_head_delta', 'summed_in_circuit_head_delta', 'summed_outside_circuit_head_delta', 'summed_total_head_delta'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_metrics[4000].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_in_circuit_head_deltas = {checkpoint: experiment_metrics[checkpoint][\"summed_in_circuit_head_delta\"] for checkpoint in experiment_metrics.keys()}\n",
    "summed_outside_circuit_head_deltas = {checkpoint: experiment_metrics[checkpoint][\"summed_outside_circuit_head_delta\"] for checkpoint in experiment_metrics.keys()}\n",
    "summed_total_head_deltas = {checkpoint: experiment_metrics[checkpoint][\"summed_total_head_delta\"] for checkpoint in experiment_metrics.keys()}\n",
    "per_head_logit_diff_deltas = {checkpoint: experiment_metrics[checkpoint][\"per_head_logit_diff_delta\"] for checkpoint in experiment_metrics.keys()}\n",
    "total_logit_diff_deltas = {checkpoint: experiment_metrics[checkpoint]['ablated_logit_diff'] - experiment_metrics[checkpoint]['logit_diff'] for checkpoint in experiment_metrics.keys()}\n",
    "\n",
    "for checkpoint in experiment_metrics.keys():\n",
    "    # divide by total original logit diff\n",
    "    summed_in_circuit_head_deltas[checkpoint] = summed_in_circuit_head_deltas[checkpoint] / experiment_metrics[checkpoint][\"logit_diff\"]\n",
    "    summed_outside_circuit_head_deltas[checkpoint] = summed_outside_circuit_head_deltas[checkpoint] / experiment_metrics[checkpoint][\"logit_diff\"]\n",
    "    summed_total_head_deltas[checkpoint] = summed_total_head_deltas[checkpoint] / experiment_metrics[checkpoint][\"logit_diff\"]\n",
    "    per_head_logit_diff_deltas[checkpoint] = per_head_logit_diff_deltas[checkpoint] / experiment_metrics[checkpoint][\"logit_diff\"]\n",
    "    total_logit_diff_deltas[checkpoint] = total_logit_diff_deltas[checkpoint] / experiment_metrics[checkpoint][\"logit_diff\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Checkpoint=%{x}<br>Change as % of original logit diff=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          4000,
          5000,
          6000,
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          126000,
          127000,
          128000,
          129000,
          130000,
          131000,
          132000,
          133000,
          134000,
          135000,
          136000,
          137000,
          138000,
          139000,
          140000,
          141000,
          142000,
          143000
         ],
         "xaxis": "x",
         "y": [
          -0.04874759206355586,
          0,
          -0.004927556561437061,
          0.0006743107202406805,
          0.04036578656102951,
          0.09597346521368576,
          0.10268239756901225,
          0.05690063353421415,
          0.09948137809491371,
          0.07299122642324421,
          0.10252651704533937,
          0.11269554053433817,
          0.10051376825947178,
          0.1088063776437573,
          0.12957015031302271,
          0.10681447081913822,
          0.14895983960725495,
          0.12758877854470932,
          0.13645988726566838,
          0.14865951587102463,
          0.14164626970710809,
          0.12957601809369412,
          0.12878618492862373,
          0.08272382214374036,
          0.10825265798896781,
          0.11976527111009047,
          0.11758727988664926,
          0.12307609275652977,
          0.10645015916966778,
          0.09973174072990366,
          0.1004438753795971,
          0.11348029558509394,
          0.11181337549663709,
          0.08856981395822675,
          0.07580316626216874,
          0.10308891037353882,
          0.08925231558624137,
          0.06620143341399017,
          0.06140121536160252,
          0.09611860008277916,
          0.07933803025040254,
          0.10934576306318014,
          0.07065813294263679,
          0.07085727218614304,
          0.05536279021059895,
          0.09221853705243352,
          0.09588969922890397,
          0.08822287125266501,
          0.06862912712177309,
          0.0877592843441588,
          0.07751692498788725,
          0.0737459113971808,
          0.08568443159713797,
          0.0813632727738016,
          0.08691193583856249,
          0.056643390227164594,
          0.06657101217973874,
          0.10561306791571012,
          0.07475683633136156,
          0.0750266473402659,
          0.07487797617407495,
          0.08496792846714966,
          0.062175367993380956,
          0.06392574909504538,
          0.09451198044168062,
          0.05890794534716129,
          0.09054137132725114,
          0.07413826899615883,
          0.0844143143946076,
          0.06570059019004362,
          0.07920544498006071,
          0.11261990881072628,
          0.09277152714132121,
          0.07376765518034124,
          0.04737410089469147,
          0.07426975146102822,
          0.03485145286770831,
          0.0651146774228661,
          0.07628403816979303,
          0.05445336968129576,
          0.029765411406656533,
          0.05696537247676474,
          0.044782428656720163,
          0.045010959778588974,
          0.05228933268163094,
          0.07097674095813095,
          0.04246955357100003,
          0.06467394143593337,
          0.04563181243663678,
          0.04340673084905829,
          0.04306732583159576,
          0.05211456211993966,
          0.05871719707301907,
          0.05810109109598053,
          0.06079037078225046,
          0.03944882268243631,
          0.04728356991911338,
          0.05733666305026681,
          0.0388132774520031,
          0.04730385263207168,
          0.04114124955393634,
          0.03949183592119944,
          0.03457835370841648,
          0.03459737426985162,
          0.0319678010528512,
          0.03308853582748141,
          0.027661118782368894,
          0.03456853108911376,
          0.02840640385111496,
          0.01930603554786443,
          0.03601782777642817,
          0.023082051785884863,
          0.03125438246715211,
          0.01682396002549297,
          0.020912885085288317,
          0.015418706460666776,
          0.017511601479999958,
          0.009844104245258343,
          0.017460074066661557,
          0.026944187919356082,
          0.014296447739947957,
          0.012739445342321756,
          0.006476104462168121,
          0.009815081547151416,
          0.009769135654954898,
          0.010748837822350003,
          0.010674380710562491,
          0.020791681357431868,
          0.015429756254052416,
          0.007981631458420691,
          0.01217451486085811,
          0.018553015444649157,
          0.015657068481213406,
          0.020563142097295095,
          0.011444063409415926,
          0.011145184411754565,
          0.019839719319202333,
          0.015308745634116293,
          0.009208870476862177,
          0.011535058932260723
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Summed Post-NMH-Ablation In-Circuit Head Logit Diff Change Over Time (Pythia 160m Default)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Checkpoint"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Change as % of original logit diff"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"a8467688-5f3c-4768-a1d7-57091439e0f9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a8467688-5f3c-4768-a1d7-57091439e0f9\")) {                    Plotly.newPlot(                        \"a8467688-5f3c-4768-a1d7-57091439e0f9\",                        [{\"hovertemplate\":\"Checkpoint=%{x}\\u003cbr\\u003eChange as % of original logit diff=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4000,5000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000],\"xaxis\":\"x\",\"y\":[-0.04874759206355586,0.0,-0.004927556561437061,0.0006743107202406805,0.04036578656102951,0.09597346521368576,0.10268239756901225,0.05690063353421415,0.09948137809491371,0.07299122642324421,0.10252651704533937,0.11269554053433817,0.10051376825947178,0.1088063776437573,0.12957015031302271,0.10681447081913822,0.14895983960725495,0.12758877854470932,0.13645988726566838,0.14865951587102463,0.14164626970710809,0.12957601809369412,0.12878618492862373,0.08272382214374036,0.10825265798896781,0.11976527111009047,0.11758727988664926,0.12307609275652977,0.10645015916966778,0.09973174072990366,0.1004438753795971,0.11348029558509394,0.11181337549663709,0.08856981395822675,0.07580316626216874,0.10308891037353882,0.08925231558624137,0.06620143341399017,0.06140121536160252,0.09611860008277916,0.07933803025040254,0.10934576306318014,0.07065813294263679,0.07085727218614304,0.05536279021059895,0.09221853705243352,0.09588969922890397,0.08822287125266501,0.06862912712177309,0.0877592843441588,0.07751692498788725,0.0737459113971808,0.08568443159713797,0.0813632727738016,0.08691193583856249,0.056643390227164594,0.06657101217973874,0.10561306791571012,0.07475683633136156,0.0750266473402659,0.07487797617407495,0.08496792846714966,0.062175367993380956,0.06392574909504538,0.09451198044168062,0.05890794534716129,0.09054137132725114,0.07413826899615883,0.0844143143946076,0.06570059019004362,0.07920544498006071,0.11261990881072628,0.09277152714132121,0.07376765518034124,0.04737410089469147,0.07426975146102822,0.03485145286770831,0.0651146774228661,0.07628403816979303,0.05445336968129576,0.029765411406656533,0.05696537247676474,0.044782428656720163,0.045010959778588974,0.05228933268163094,0.07097674095813095,0.04246955357100003,0.06467394143593337,0.04563181243663678,0.04340673084905829,0.04306732583159576,0.05211456211993966,0.05871719707301907,0.05810109109598053,0.06079037078225046,0.03944882268243631,0.04728356991911338,0.05733666305026681,0.0388132774520031,0.04730385263207168,0.04114124955393634,0.03949183592119944,0.03457835370841648,0.03459737426985162,0.0319678010528512,0.03308853582748141,0.027661118782368894,0.03456853108911376,0.02840640385111496,0.01930603554786443,0.03601782777642817,0.023082051785884863,0.03125438246715211,0.01682396002549297,0.020912885085288317,0.015418706460666776,0.017511601479999958,0.009844104245258343,0.017460074066661557,0.026944187919356082,0.014296447739947957,0.012739445342321756,0.006476104462168121,0.009815081547151416,0.009769135654954898,0.010748837822350003,0.010674380710562491,0.020791681357431868,0.015429756254052416,0.007981631458420691,0.01217451486085811,0.018553015444649157,0.015657068481213406,0.020563142097295095,0.011444063409415926,0.011145184411754565,0.019839719319202333,0.015308745634116293,0.009208870476862177,0.011535058932260723],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Checkpoint\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Change as % of original logit diff\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Summed Post-NMH-Ablation In-Circuit Head Logit Diff Change Over Time (Pythia 160m Default)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a8467688-5f3c-4768-a1d7-57091439e0f9');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot summed_in_circuit_head_deltas with plotly express\n",
    "fig = px.line(\n",
    "    x=list(summed_in_circuit_head_deltas.keys()), \n",
    "    y=list(summed_in_circuit_head_deltas.values()), \n",
    "    title=\"Summed Post-NMH-Ablation In-Circuit Head Logit Diff Change Over Time (Pythia 160m Default)\",\n",
    "    labels={'x': 'Checkpoint', 'y': 'Change as % of original logit diff'} \n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Checkpoint=%{x}<br>Change as % of original logit diff=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          4000,
          5000,
          6000,
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          126000,
          127000,
          128000,
          129000,
          130000,
          131000,
          132000,
          133000,
          134000,
          135000,
          136000,
          137000,
          138000,
          139000,
          140000,
          141000,
          142000,
          143000
         ],
         "xaxis": "x",
         "y": [
          -0.043071704995338224,
          0,
          -0.034838356390819954,
          0.010491199797567822,
          0.010968819453935222,
          0.0015313982563985365,
          0.00532543547393766,
          0.010752839912506215,
          0.02531287764098698,
          0.018958980378701782,
          0.012663146585631049,
          0.015745443908849608,
          0.001253890462658407,
          0.014050342054257883,
          0.01441419142351048,
          0.013618688241534208,
          0.029345154340804314,
          0.00206194990681872,
          0.02779381463404436,
          0.02809484386991504,
          0.030302537238172285,
          0.002692509833182894,
          0.02245335973625196,
          0.0010800536696668995,
          -0.00033378300289789686,
          0.02278989011528307,
          -0.0007894061245561732,
          0.0005445490384878283,
          -0.0007571442634131655,
          -0.000715703684103466,
          -0.000020625481121832807,
          0.0004697040728810489,
          0.0007784223780814578,
          0.012825845535615372,
          0.03468700412608689,
          0.015627993597741183,
          0.0419716025512508,
          0.02877968397416872,
          0.019712446398668718,
          0.026902490602005048,
          0.016781069312192788,
          0.002071077983715289,
          0.018275545416346094,
          0.015047084829562117,
          0.030879653712632793,
          0.016442919292460903,
          0.0044167596571767906,
          0.028159189672801635,
          0.07909375240706666,
          0.05178431847726203,
          0.0018901401182523845,
          0.0034961710715242637,
          0.00292895533018044,
          0.002126415556090234,
          0.0030492390829039333,
          0.0030223033136976794,
          0.02008166412505668,
          0.0035456041883057607,
          0.020610581350076086,
          0.024104055071373132,
          0.0029305579335846266,
          0.004411356993980214,
          0.0036760890413248598,
          0.0552390828331396,
          0.010253089622964395,
          0.003070315727987247,
          0.004598480793756628,
          0.025428259363388748,
          0.004157358861025079,
          0.0033033339611961395,
          0.02958754344612417,
          0.019078342996456648,
          0.005432832619587728,
          0.025313198970504536,
          0.01829353996991709,
          0.0031303388257330022,
          0.04855533403575165,
          0.00331778741172562,
          0.0019858912256388987,
          0.00414795719644388,
          0.04796310680849898,
          0.006522627675811524,
          0.0433516493848596,
          0.026138310138358635,
          0.009362040020852181,
          0.006914942879938271,
          0.042187168378208253,
          0.0037349999310240796,
          0.049817972231316204,
          0.002789436935838183,
          0.002797758411895943,
          0.05204835040152067,
          0.059878940640000565,
          0.0163152878400242,
          0.0022534859389283317,
          0.0319401522048137,
          0.007048238089235162,
          0.0121535512832368,
          0.0038414043481558528,
          0.0037456667069525173,
          0.008391687502348944,
          0.0027589807898952116,
          0.005605328460150576,
          0.0013763728010548737,
          0.0011645933953647026,
          0.008593100898644101,
          0.004654583232310729,
          0.00584879137740316,
          0.0100021931536962,
          0.03065953835606348,
          0.008855759003169365,
          0.019745341236709697,
          0.002796119614619538,
          0.011352454245092403,
          0.00666919775743624,
          0.004675234223743095,
          0.0018308533567863894,
          0.009164939117404373,
          0.004221618018106341,
          0.004134954167825886,
          0.01302100501016565,
          0.012646882327714179,
          0.0011572067096873562,
          0.0007284961634218276,
          0.0018684002774627673,
          0.012999804107363734,
          0.004371371921108856,
          0.01478777802326914,
          0.004974097550796613,
          0.0007935620404013462,
          0.011419992415595876,
          0.00855909908955385,
          0.0055227099616256955,
          0.005477746329584981,
          0.004117184404643398,
          0.005805052026921262,
          0.0030363544844265026,
          0.0010752626343117785,
          0.00492113538277586,
          0.0021855264517527504
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Summed Post-NMH-Ablation Outside-Circuit Head Attribution Change (Pythia 160m Default)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Checkpoint"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Change as % of original logit diff"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"122731fd-c7e2-4acc-b799-98ddc227befb\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"122731fd-c7e2-4acc-b799-98ddc227befb\")) {                    Plotly.newPlot(                        \"122731fd-c7e2-4acc-b799-98ddc227befb\",                        [{\"hovertemplate\":\"Checkpoint=%{x}\\u003cbr\\u003eChange as % of original logit diff=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4000,5000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000],\"xaxis\":\"x\",\"y\":[-0.043071704995338224,0.0,-0.034838356390819954,0.010491199797567822,0.010968819453935222,0.0015313982563985365,0.00532543547393766,0.010752839912506215,0.02531287764098698,0.018958980378701782,0.012663146585631049,0.015745443908849608,0.001253890462658407,0.014050342054257883,0.01441419142351048,0.013618688241534208,0.029345154340804314,0.00206194990681872,0.02779381463404436,0.02809484386991504,0.030302537238172285,0.002692509833182894,0.02245335973625196,0.0010800536696668995,-0.00033378300289789686,0.02278989011528307,-0.0007894061245561732,0.0005445490384878283,-0.0007571442634131655,-0.000715703684103466,-2.0625481121832807e-05,0.0004697040728810489,0.0007784223780814578,0.012825845535615372,0.03468700412608689,0.015627993597741183,0.0419716025512508,0.02877968397416872,0.019712446398668718,0.026902490602005048,0.016781069312192788,0.002071077983715289,0.018275545416346094,0.015047084829562117,0.030879653712632793,0.016442919292460903,0.0044167596571767906,0.028159189672801635,0.07909375240706666,0.05178431847726203,0.0018901401182523845,0.0034961710715242637,0.00292895533018044,0.002126415556090234,0.0030492390829039333,0.0030223033136976794,0.02008166412505668,0.0035456041883057607,0.020610581350076086,0.024104055071373132,0.0029305579335846266,0.004411356993980214,0.0036760890413248598,0.0552390828331396,0.010253089622964395,0.003070315727987247,0.004598480793756628,0.025428259363388748,0.004157358861025079,0.0033033339611961395,0.02958754344612417,0.019078342996456648,0.005432832619587728,0.025313198970504536,0.01829353996991709,0.0031303388257330022,0.04855533403575165,0.00331778741172562,0.0019858912256388987,0.00414795719644388,0.04796310680849898,0.006522627675811524,0.0433516493848596,0.026138310138358635,0.009362040020852181,0.006914942879938271,0.042187168378208253,0.0037349999310240796,0.049817972231316204,0.002789436935838183,0.002797758411895943,0.05204835040152067,0.059878940640000565,0.0163152878400242,0.0022534859389283317,0.0319401522048137,0.007048238089235162,0.0121535512832368,0.0038414043481558528,0.0037456667069525173,0.008391687502348944,0.0027589807898952116,0.005605328460150576,0.0013763728010548737,0.0011645933953647026,0.008593100898644101,0.004654583232310729,0.00584879137740316,0.0100021931536962,0.03065953835606348,0.008855759003169365,0.019745341236709697,0.002796119614619538,0.011352454245092403,0.00666919775743624,0.004675234223743095,0.0018308533567863894,0.009164939117404373,0.004221618018106341,0.004134954167825886,0.01302100501016565,0.012646882327714179,0.0011572067096873562,0.0007284961634218276,0.0018684002774627673,0.012999804107363734,0.004371371921108856,0.01478777802326914,0.004974097550796613,0.0007935620404013462,0.011419992415595876,0.00855909908955385,0.0055227099616256955,0.005477746329584981,0.004117184404643398,0.005805052026921262,0.0030363544844265026,0.0010752626343117785,0.00492113538277586,0.0021855264517527504],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Checkpoint\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Change as % of original logit diff\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Summed Post-NMH-Ablation Outside-Circuit Head Attribution Change (Pythia 160m Default)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('122731fd-c7e2-4acc-b799-98ddc227befb');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot summed_outside_circuit_head_deltas\n",
    "fig = px.line(\n",
    "    x=list(summed_outside_circuit_head_deltas.keys()), \n",
    "    y=list(summed_outside_circuit_head_deltas.values()), \n",
    "    title=\"Summed Post-NMH-Ablation Outside-Circuit Head Attribution Change (Pythia 160m Default)\",\n",
    "    labels={'x': 'Checkpoint', 'y': 'Change as % of original logit diff'} \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Checkpoint=%{x}<br>Change as % of original logit diff=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          4000,
          5000,
          6000,
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          126000,
          127000,
          128000,
          129000,
          130000,
          131000,
          132000,
          133000,
          134000,
          135000,
          136000,
          137000,
          138000,
          139000,
          140000,
          141000,
          142000,
          143000
         ],
         "xaxis": "x",
         "y": [
          -0.09181928713007813,
          0,
          -0.039765914020408424,
          0.01116551022715545,
          0.051334606014964736,
          0.09750486636720443,
          0.10800784066806234,
          0.06765347279882251,
          0.12479425826108283,
          0.09195020963770843,
          0.11518965522190165,
          0.128440988434745,
          0.10176766107991159,
          0.1228567347769904,
          0.1439843356084074,
          0.12043316510958856,
          0.17830498310010995,
          0.12965072871278943,
          0.1642537085664069,
          0.17675435150077187,
          0.17194881988970384,
          0.13226852553203503,
          0.15123954058477676,
          0.08380387909712117,
          0.10791887682335967,
          0.1425551631691294,
          0.11679787032397541,
          0.12362063481083123,
          0.10569302566158754,
          0.09901603454923519,
          0.10042324979623075,
          0.11394998603594424,
          0.11259181107306139,
          0.10139566819219548,
          0.11049018338626121,
          0.11871690580446906,
          0.13122389728812567,
          0.09498112064017077,
          0.0811136667586547,
          0.1230210873512162,
          0.09611909437568336,
          0.11141684850653204,
          0.08893368347231226,
          0.08590435365016641,
          0.08624244750859282,
          0.10866145001564591,
          0.10030645493135638,
          0.11638205412696341,
          0.14772288633808067,
          0.13954360282142084,
          0.0794070589818,
          0.07724207831375601,
          0.08861338397714501,
          0.08348970034680132,
          0.08996116278146113,
          0.059665694624924985,
          0.08665267466094624,
          0.10915866688377719,
          0.09536740728816968,
          0.09913069721492958,
          0.07780853291173524,
          0.08937928730952241,
          0.06585145890019257,
          0.11916483192818499,
          0.10476508250137885,
          0.06197825677345977,
          0.095139840021588,
          0.099566513295163,
          0.08857167325563267,
          0.06900393207472022,
          0.10879298468315261,
          0.1316982499112236,
          0.09820434704559171,
          0.0990808577939736,
          0.06566764086460855,
          0.0774000920619909,
          0.08340680601639823,
          0.06843246187673153,
          0.0782699267371303,
          0.05860133021861987,
          0.07772852410521922,
          0.06348800371155022,
          0.0881340741239871,
          0.07114926564199614,
          0.061651373663250186,
          0.07789168640282297,
          0.08465671829094253,
          0.06840894264239977,
          0.09544978466795298,
          0.0461961634318856,
          0.04586508070564317,
          0.10416291642660444,
          0.11859615118839517,
          0.07441637472111778,
          0.06304386017745693,
          0.07138897488725,
          0.05433181309565516,
          0.06949022205997323,
          0.042654679505241315,
          0.05104951438495987,
          0.04953293803828524,
          0.042250814394357246,
          0.040183682168567054,
          0.03597374950986658,
          0.033132390085256556,
          0.041681634966804854,
          0.03231570583529021,
          0.04041732603867094,
          0.03840859613269863,
          0.0499655723974868,
          0.04487359440843663,
          0.04282739825967072,
          0.03405050291304387,
          0.028176418197317206,
          0.02758208242884376,
          0.02009394152225901,
          0.019342454420329797,
          0.019009043362662716,
          0.0216816916780099,
          0.03107914129881196,
          0.02731745354737444,
          0.025386325255579545,
          0.0076333108536929305,
          0.010543577524289915,
          0.011637534614462893,
          0.023748641929713737,
          0.015045752631671348,
          0.035579460993888985,
          0.020403854963299786,
          0.008775193305801598,
          0.023594506477032005,
          0.027112112985555506,
          0.02117977670743777,
          0.026040886831036357,
          0.015561247034568867,
          0.016950236041370415,
          0.02287607401389129,
          0.016384007975106016,
          0.014130005106486683,
          0.013720585958523226
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Summed Total Post-NMH-Ablation Head Attribution Change (Pythia 160m Default)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Checkpoint"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Change as % of original logit diff"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"2b94b05b-9fde-42a9-b4a1-f783303ea953\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2b94b05b-9fde-42a9-b4a1-f783303ea953\")) {                    Plotly.newPlot(                        \"2b94b05b-9fde-42a9-b4a1-f783303ea953\",                        [{\"hovertemplate\":\"Checkpoint=%{x}\\u003cbr\\u003eChange as % of original logit diff=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4000,5000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000],\"xaxis\":\"x\",\"y\":[-0.09181928713007813,0.0,-0.039765914020408424,0.01116551022715545,0.051334606014964736,0.09750486636720443,0.10800784066806234,0.06765347279882251,0.12479425826108283,0.09195020963770843,0.11518965522190165,0.128440988434745,0.10176766107991159,0.1228567347769904,0.1439843356084074,0.12043316510958856,0.17830498310010995,0.12965072871278943,0.1642537085664069,0.17675435150077187,0.17194881988970384,0.13226852553203503,0.15123954058477676,0.08380387909712117,0.10791887682335967,0.1425551631691294,0.11679787032397541,0.12362063481083123,0.10569302566158754,0.09901603454923519,0.10042324979623075,0.11394998603594424,0.11259181107306139,0.10139566819219548,0.11049018338626121,0.11871690580446906,0.13122389728812567,0.09498112064017077,0.0811136667586547,0.1230210873512162,0.09611909437568336,0.11141684850653204,0.08893368347231226,0.08590435365016641,0.08624244750859282,0.10866145001564591,0.10030645493135638,0.11638205412696341,0.14772288633808067,0.13954360282142084,0.0794070589818,0.07724207831375601,0.08861338397714501,0.08348970034680132,0.08996116278146113,0.059665694624924985,0.08665267466094624,0.10915866688377719,0.09536740728816968,0.09913069721492958,0.07780853291173524,0.08937928730952241,0.06585145890019257,0.11916483192818499,0.10476508250137885,0.06197825677345977,0.095139840021588,0.099566513295163,0.08857167325563267,0.06900393207472022,0.10879298468315261,0.1316982499112236,0.09820434704559171,0.0990808577939736,0.06566764086460855,0.0774000920619909,0.08340680601639823,0.06843246187673153,0.0782699267371303,0.05860133021861987,0.07772852410521922,0.06348800371155022,0.0881340741239871,0.07114926564199614,0.061651373663250186,0.07789168640282297,0.08465671829094253,0.06840894264239977,0.09544978466795298,0.0461961634318856,0.04586508070564317,0.10416291642660444,0.11859615118839517,0.07441637472111778,0.06304386017745693,0.07138897488725,0.05433181309565516,0.06949022205997323,0.042654679505241315,0.05104951438495987,0.04953293803828524,0.042250814394357246,0.040183682168567054,0.03597374950986658,0.033132390085256556,0.041681634966804854,0.03231570583529021,0.04041732603867094,0.03840859613269863,0.0499655723974868,0.04487359440843663,0.04282739825967072,0.03405050291304387,0.028176418197317206,0.02758208242884376,0.02009394152225901,0.019342454420329797,0.019009043362662716,0.0216816916780099,0.03107914129881196,0.02731745354737444,0.025386325255579545,0.0076333108536929305,0.010543577524289915,0.011637534614462893,0.023748641929713737,0.015045752631671348,0.035579460993888985,0.020403854963299786,0.008775193305801598,0.023594506477032005,0.027112112985555506,0.02117977670743777,0.026040886831036357,0.015561247034568867,0.016950236041370415,0.02287607401389129,0.016384007975106016,0.014130005106486683,0.013720585958523226],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Checkpoint\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Change as % of original logit diff\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Summed Total Post-NMH-Ablation Head Attribution Change (Pythia 160m Default)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('2b94b05b-9fde-42a9-b4a1-f783303ea953');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot total_head_deltas\n",
    "fig = px.line(\n",
    "    x=list(summed_total_head_deltas.keys()), \n",
    "    y=list(summed_total_head_deltas.values()), \n",
    "    title=\"Summed Total Post-NMH-Ablation Head Attribution Change (Pythia 160m Default)\",\n",
    "    labels={'x': 'Checkpoint', 'y': 'Change as % of original logit diff'}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(8, 1), (7, 10), (9, 4), (8, 9), (9, 8), (9, 11), (8, 2)}\n"
     ]
    }
   ],
   "source": [
    "cumulative_nmhs, checkpoint_nmhs = get_past_nmhs_for_checkpoints(experiment_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Layer-Head=Layer 10-Head 7<br>Checkpoint=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "Layer 10-Head 7",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Layer 10-Head 7",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          4000,
          6000,
          10000,
          78000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000
         ],
         "xaxis": "x",
         "y": [
          0.0026725521311163902,
          0.000629258924163878,
          0.0005257618031464517,
          0.0015414286172017455,
          0.003221277380362153,
          0.00985526293516159,
          0.0173241775482893,
          0.029232485219836235,
          0.022043347358703613,
          0.028252970427274704,
          0.022193506360054016,
          0.022391492500901222,
          0.03811975196003914,
          0.03271893411874771,
          0.022330261766910553,
          0.033639296889305115,
          0.022757917642593384,
          0.02427608333528042,
          0.029711777344346046,
          0.025612127035856247,
          0.016769522801041603,
          0.022821063175797462,
          0.03760848566889763,
          0.02798982709646225,
          0.027296870946884155,
          0.037040431052446365,
          0.024523871019482613,
          0.020268121734261513,
          0.02189512364566326,
          0.021337129175662994,
          0.022509777918457985,
          0.014212765730917454,
          0.01839056983590126,
          0.01428866945207119,
          0.013621959835290909,
          0.013122176751494408,
          0.01007688045501709,
          0.01160617545247078,
          0.01563279703259468,
          0.0035480703227221966,
          0.011431626044213772,
          0.007735598832368851,
          0.012061845511198044,
          0.005630972795188427,
          0.004875671584159136,
          0.004165762569755316,
          0.002620133338496089,
          0.005015193950384855,
          0.00404421566054225,
          0.0008534201770089567
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Layer-Head=Layer 9-Head 9<br>Checkpoint=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "Layer 9-Head 9",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Layer 9-Head 9",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          7000,
          8000,
          11000,
          13000,
          14000,
          15000,
          17000,
          18000,
          19000,
          20000,
          22000,
          23000,
          24000,
          26000,
          29000,
          30000,
          31000,
          35000,
          36000,
          39000,
          40000,
          41000,
          43000,
          45000,
          49000,
          51000,
          52000,
          53000,
          55000,
          58000,
          61000,
          62000,
          63000,
          65000,
          66000,
          67000,
          68000,
          70000,
          71000,
          74000,
          75000,
          76000,
          77000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          106000,
          109000,
          110000,
          112000,
          114000,
          117000,
          118000,
          119000,
          124000,
          125000,
          128000,
          131000,
          132000,
          134000
         ],
         "xaxis": "x",
         "y": [
          0.0017917121294885874,
          0.005667125340551138,
          0.007299109827727079,
          0.008191687986254692,
          0.012010126374661922,
          0.012112344615161419,
          0.01314469613134861,
          0.012295766733586788,
          0.012480290606617928,
          0.02863914519548416,
          0.024729236960411072,
          0.021760735660791397,
          0.02855166792869568,
          0.02219049260020256,
          0.02279970981180668,
          0.019900595769286156,
          0.02176225557923317,
          0.020162083208560944,
          0.01848527416586876,
          0.019513001665472984,
          0.018014416098594666,
          0.012220167554914951,
          0.017404286190867424,
          0.017807412892580032,
          0.016632266342639923,
          0.02064628154039383,
          0.01906583644449711,
          0.019717315211892128,
          0.011843998916447163,
          0.015505749732255936,
          0.020441871136426926,
          0.017366889864206314,
          0.01892448030412197,
          0.01852373033761978,
          0.013165752403438091,
          0.019930146634578705,
          0.021192502230405807,
          0.022864865139126778,
          0.019892964512109756,
          0.023508219048380852,
          0.025774521753191948,
          0.022104782983660698,
          0.021632587537169456,
          0.02096971869468689,
          0.020188646391034126,
          0.018835877999663353,
          0.016392558813095093,
          0.011681637726724148,
          0.008836541324853897,
          0.004337572958320379,
          0.003690236946567893,
          0.003030212363228202,
          0.0037146529648452997,
          0.0023884151596575975,
          0.004061004612594843,
          0.0021455190144479275,
          0.00363927218131721,
          0.0034062149934470654,
          0.004100973252207041,
          0.004484336357563734,
          0.0040551722049713135,
          0.0038121941033750772,
          0.0022667511366307735,
          0.0052386196330189705,
          0.0036718465853482485,
          0.0033050274942070246,
          0.0027817883528769016,
          0.0023229457437992096,
          0.0021952930837869644,
          0.00239189644344151,
          0.002751966007053852,
          0.0016180173261091113,
          0.0024881938006728888,
          0.0016974697355180979,
          0.00177038146648556,
          0.001340522663667798,
          0.0009367555612698197,
          0.0009817652171477675,
          0.001115450169891119,
          0.0014319070614874363
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Layer-Head=Layer 9-Head 6<br>Checkpoint=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "Layer 9-Head 6",
         "line": {
          "color": "#00cc96",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Layer 9-Head 6",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          84000,
          86000,
          90000,
          92000,
          95000,
          96000,
          113000,
          115000,
          121000,
          124000,
          125000,
          129000,
          131000,
          134000
         ],
         "xaxis": "x",
         "y": [
          0.002411766443401575,
          0.0033179090823978186,
          0.0038747189100831747,
          0.005100138485431671,
          0.0032757434528321028,
          0.005085034761577845,
          0.00363577320240438,
          0.004538726061582565,
          0.004653990268707275,
          0.0029530839528888464,
          0.005190160591155291,
          0.004965000320225954,
          0.005145061761140823,
          0.010231755673885345,
          0.0066286372020840645,
          0.008754000999033451,
          0.009256397373974323,
          0.011626716703176498,
          0.009798008017241955,
          0.008918024599552155,
          0.006602451205253601,
          0.009992163628339767,
          0.012730184011161327,
          0.009922172874212265,
          0.01246168464422226,
          0.012267830781638622,
          0.012358670122921467,
          0.011819890700280666,
          0.009157104417681694,
          0.011974005028605461,
          0.012825957499444485,
          0.010397862643003464,
          0.014318552799522877,
          0.012905200012028217,
          0.009982398711144924,
          0.01014366652816534,
          0.011478029191493988,
          0.014981011860072613,
          0.012985504232347012,
          0.015007461421191692,
          0.012812497094273567,
          0.010753141716122627,
          0.014166788198053837,
          0.016267696395516396,
          0.015970410779118538,
          0.015791580080986023,
          0.016894401982426643,
          0.013483737595379353,
          0.012541297823190689,
          0.015036255121231079,
          0.01540802139788866,
          0.013801447115838528,
          0.010804715566337109,
          0.016004567965865135,
          0.01627776399254799,
          0.015161830931901932,
          0.017550276592373848,
          0.015830403193831444,
          0.015380950644612312,
          0.012211491353809834,
          0.01687275804579258,
          0.017069285735487938,
          0.013757806271314621,
          0.018924430012702942,
          0.019756918773055077,
          0.02118491567671299,
          0.01618352346122265,
          0.020782627165317535,
          0.02340947650372982,
          0.019920840859413147,
          0.02092892676591873,
          0.016276195645332336,
          0.019673574715852737,
          0.01910588890314102,
          0.024919889867305756,
          0.025067415088415146,
          0.03254231810569763,
          0.03709037974476814,
          0.0400158166885376,
          0.039756856858730316,
          0.046799853444099426,
          0.05384105071425438,
          0.01724526286125183,
          0.01298323180526495,
          0.005388436373323202,
          0.003529398702085018,
          0.0063859689980745316,
          0.005692761391401291,
          0.009092289954423904,
          0.005608438979834318
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Layer-Head=Layer 9-Head 10<br>Checkpoint=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "Layer 9-Head 10",
         "line": {
          "color": "#ab63fa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Layer 9-Head 10",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          126000,
          127000,
          128000,
          129000,
          130000,
          131000,
          132000,
          133000,
          134000,
          135000,
          136000,
          137000,
          138000,
          139000,
          140000,
          141000,
          142000,
          143000
         ],
         "xaxis": "x",
         "y": [
          0.03222522884607315,
          0.07681969553232193,
          0.09267992526292801,
          0.050537109375,
          0.09160967916250229,
          0.06424630433320999,
          0.08639313280582428,
          0.09022432565689087,
          0.08270614594221115,
          0.09181993454694748,
          0.09710001945495605,
          0.08648547530174255,
          0.10375629365444183,
          0.09595594555139542,
          0.1080184206366539,
          0.10138272494077682,
          0.10140974074602127,
          0.09014643728733063,
          0.09361708164215088,
          0.062364812940359116,
          0.0844980850815773,
          0.09353552013635635,
          0.07908909767866135,
          0.07943552732467651,
          0.08190986514091492,
          0.07658962160348892,
          0.07329165190458298,
          0.07407847046852112,
          0.07207773625850677,
          0.07527463883161545,
          0.06588879227638245,
          0.07392334192991257,
          0.06360535323619843,
          0.04968547821044922,
          0.04612740874290466,
          0.06416758894920349,
          0.06921551376581192,
          0.07046577334403992,
          0.060766879469156265,
          0.06194257363677025,
          0.04834931716322899,
          0.06553802639245987,
          0.07097754627466202,
          0.06695930659770966,
          0.06924576312303543,
          0.067400261759758,
          0.05826321989297867,
          0.04571755602955818,
          0.06163875758647919,
          0.058743007481098175,
          0.05438004061579704,
          0.04628763347864151,
          0.05936237797141075,
          0.06313302367925644,
          0.054775454103946686,
          0.05461150407791138,
          0.0506063774228096,
          0.050201185047626495,
          0.03528834134340286,
          0.054817210882902145,
          0.05598009377717972,
          0.042144741863012314,
          0.052121851593256,
          0.053592946380376816,
          0.05531100556254387,
          0.04852185770869255,
          0.06011071056127548,
          0.05953570082783699,
          0.05484941974282265,
          0.052112944424152374,
          0.04271329194307327,
          0.05540832132101059,
          0.010691503994166851,
          0.01124979741871357,
          0.013822130858898163,
          0.007997299544513226,
          0.008184021338820457,
          0.017130574211478233,
          0.017278531566262245,
          0.02039288729429245,
          0.013443195261061192,
          0.02441134676337242,
          0.009756485000252724,
          0.024407168850302696,
          0.017793167382478714,
          0.013443860225379467,
          0.006988817825913429,
          0.01977366767823696,
          0.02635638415813446,
          0.0285998173058033,
          0.017801448702812195,
          0.02694586105644703,
          0.012630818411707878,
          0.012424803338944912,
          0.011190235614776611,
          0.023921163752675056,
          0.019014524295926094,
          0.016663946211338043,
          0.011803888715803623,
          0.01717207580804825,
          0.01147424802184105,
          0.01782493107020855,
          0.013697193004190922,
          0.021145636215806007,
          0.016838466748595238,
          0.006195883732289076,
          0.02008146233856678,
          0.016879579052329063,
          0.01805323176085949,
          0.008822017349302769,
          0.008493449538946152,
          0.009293714538216591,
          0.012265020981431007,
          0.005372397601604462,
          0.014539504423737526,
          0.0185600183904171,
          0.010052982717752457,
          0.011502386070787907,
          0.008226056583225727,
          0.009178150445222855,
          0.009944180026650429,
          0.010204619728028774,
          0.009734218940138817,
          0.02032715082168579,
          0.014304188080132008,
          0.006560435984283686,
          0.011993381194770336,
          0.019200043752789497,
          0.01598062738776207,
          0.02094927988946438,
          0.011559292674064636,
          0.010404814966022968,
          0.019887054339051247,
          0.012579450383782387,
          0.008997111581265926,
          0.009807485155761242
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Layer-Head=Layer 9-Head 11<br>Checkpoint=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "Layer 9-Head 11",
         "line": {
          "color": "#FFA15A",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Layer 9-Head 11",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          38000,
          40000,
          41000,
          42000,
          43000,
          48000,
          52000,
          53000,
          97000,
          100000,
          101000,
          104000,
          107000,
          111000,
          112000,
          129000,
          135000,
          137000,
          138000,
          139000
         ],
         "xaxis": "x",
         "y": [
          0.023925190791487694,
          0.027843698859214783,
          0.01776779070496559,
          0.018317172303795815,
          0.025711344555020332,
          0.018526440486311913,
          0.029855648055672646,
          0.02846292406320572,
          0.011950287967920303,
          0.003895883448421955,
          0.003004722995683551,
          0.0018873580265790224,
          0.001349860685877502,
          0.0014797108015045524,
          0.0013560284860432148,
          0.00176145788282156,
          0.002781546674668789,
          0.0021300639491528273,
          0.001709240605123341,
          0.001875362591817975
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "height": 500,
        "legend": {
         "title": {
          "text": "Layer-Head"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Top Backup Heads Attribution Across Checkpoints (Pythia 160m Default)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Checkpoint"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"243ff06f-14da-4046-8309-067bb8ff05ee\" class=\"plotly-graph-div\" style=\"height:500px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"243ff06f-14da-4046-8309-067bb8ff05ee\")) {                    Plotly.newPlot(                        \"243ff06f-14da-4046-8309-067bb8ff05ee\",                        [{\"hovertemplate\":\"Layer-Head=Layer 10-Head 7\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Layer 10-Head 7\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Layer 10-Head 7\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[4000,6000,10000,78000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000],\"xaxis\":\"x\",\"y\":[0.0026725521311163902,0.000629258924163878,0.0005257618031464517,0.0015414286172017455,0.003221277380362153,0.00985526293516159,0.0173241775482893,0.029232485219836235,0.022043347358703613,0.028252970427274704,0.022193506360054016,0.022391492500901222,0.03811975196003914,0.03271893411874771,0.022330261766910553,0.033639296889305115,0.022757917642593384,0.02427608333528042,0.029711777344346046,0.025612127035856247,0.016769522801041603,0.022821063175797462,0.03760848566889763,0.02798982709646225,0.027296870946884155,0.037040431052446365,0.024523871019482613,0.020268121734261513,0.02189512364566326,0.021337129175662994,0.022509777918457985,0.014212765730917454,0.01839056983590126,0.01428866945207119,0.013621959835290909,0.013122176751494408,0.01007688045501709,0.01160617545247078,0.01563279703259468,0.0035480703227221966,0.011431626044213772,0.007735598832368851,0.012061845511198044,0.005630972795188427,0.004875671584159136,0.004165762569755316,0.002620133338496089,0.005015193950384855,0.00404421566054225,0.0008534201770089567],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Layer-Head=Layer 9-Head 9\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Layer 9-Head 9\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Layer 9-Head 9\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[7000,8000,11000,13000,14000,15000,17000,18000,19000,20000,22000,23000,24000,26000,29000,30000,31000,35000,36000,39000,40000,41000,43000,45000,49000,51000,52000,53000,55000,58000,61000,62000,63000,65000,66000,67000,68000,70000,71000,74000,75000,76000,77000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,106000,109000,110000,112000,114000,117000,118000,119000,124000,125000,128000,131000,132000,134000],\"xaxis\":\"x\",\"y\":[0.0017917121294885874,0.005667125340551138,0.007299109827727079,0.008191687986254692,0.012010126374661922,0.012112344615161419,0.01314469613134861,0.012295766733586788,0.012480290606617928,0.02863914519548416,0.024729236960411072,0.021760735660791397,0.02855166792869568,0.02219049260020256,0.02279970981180668,0.019900595769286156,0.02176225557923317,0.020162083208560944,0.01848527416586876,0.019513001665472984,0.018014416098594666,0.012220167554914951,0.017404286190867424,0.017807412892580032,0.016632266342639923,0.02064628154039383,0.01906583644449711,0.019717315211892128,0.011843998916447163,0.015505749732255936,0.020441871136426926,0.017366889864206314,0.01892448030412197,0.01852373033761978,0.013165752403438091,0.019930146634578705,0.021192502230405807,0.022864865139126778,0.019892964512109756,0.023508219048380852,0.025774521753191948,0.022104782983660698,0.021632587537169456,0.02096971869468689,0.020188646391034126,0.018835877999663353,0.016392558813095093,0.011681637726724148,0.008836541324853897,0.004337572958320379,0.003690236946567893,0.003030212363228202,0.0037146529648452997,0.0023884151596575975,0.004061004612594843,0.0021455190144479275,0.00363927218131721,0.0034062149934470654,0.004100973252207041,0.004484336357563734,0.0040551722049713135,0.0038121941033750772,0.0022667511366307735,0.0052386196330189705,0.0036718465853482485,0.0033050274942070246,0.0027817883528769016,0.0023229457437992096,0.0021952930837869644,0.00239189644344151,0.002751966007053852,0.0016180173261091113,0.0024881938006728888,0.0016974697355180979,0.00177038146648556,0.001340522663667798,0.0009367555612698197,0.0009817652171477675,0.001115450169891119,0.0014319070614874363],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Layer-Head=Layer 9-Head 6\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Layer 9-Head 6\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Layer 9-Head 6\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,84000,86000,90000,92000,95000,96000,113000,115000,121000,124000,125000,129000,131000,134000],\"xaxis\":\"x\",\"y\":[0.002411766443401575,0.0033179090823978186,0.0038747189100831747,0.005100138485431671,0.0032757434528321028,0.005085034761577845,0.00363577320240438,0.004538726061582565,0.004653990268707275,0.0029530839528888464,0.005190160591155291,0.004965000320225954,0.005145061761140823,0.010231755673885345,0.0066286372020840645,0.008754000999033451,0.009256397373974323,0.011626716703176498,0.009798008017241955,0.008918024599552155,0.006602451205253601,0.009992163628339767,0.012730184011161327,0.009922172874212265,0.01246168464422226,0.012267830781638622,0.012358670122921467,0.011819890700280666,0.009157104417681694,0.011974005028605461,0.012825957499444485,0.010397862643003464,0.014318552799522877,0.012905200012028217,0.009982398711144924,0.01014366652816534,0.011478029191493988,0.014981011860072613,0.012985504232347012,0.015007461421191692,0.012812497094273567,0.010753141716122627,0.014166788198053837,0.016267696395516396,0.015970410779118538,0.015791580080986023,0.016894401982426643,0.013483737595379353,0.012541297823190689,0.015036255121231079,0.01540802139788866,0.013801447115838528,0.010804715566337109,0.016004567965865135,0.01627776399254799,0.015161830931901932,0.017550276592373848,0.015830403193831444,0.015380950644612312,0.012211491353809834,0.01687275804579258,0.017069285735487938,0.013757806271314621,0.018924430012702942,0.019756918773055077,0.02118491567671299,0.01618352346122265,0.020782627165317535,0.02340947650372982,0.019920840859413147,0.02092892676591873,0.016276195645332336,0.019673574715852737,0.01910588890314102,0.024919889867305756,0.025067415088415146,0.03254231810569763,0.03709037974476814,0.0400158166885376,0.039756856858730316,0.046799853444099426,0.05384105071425438,0.01724526286125183,0.01298323180526495,0.005388436373323202,0.003529398702085018,0.0063859689980745316,0.005692761391401291,0.009092289954423904,0.005608438979834318],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Layer-Head=Layer 9-Head 10\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Layer 9-Head 10\",\"line\":{\"color\":\"#ab63fa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Layer 9-Head 10\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000],\"xaxis\":\"x\",\"y\":[0.03222522884607315,0.07681969553232193,0.09267992526292801,0.050537109375,0.09160967916250229,0.06424630433320999,0.08639313280582428,0.09022432565689087,0.08270614594221115,0.09181993454694748,0.09710001945495605,0.08648547530174255,0.10375629365444183,0.09595594555139542,0.1080184206366539,0.10138272494077682,0.10140974074602127,0.09014643728733063,0.09361708164215088,0.062364812940359116,0.0844980850815773,0.09353552013635635,0.07908909767866135,0.07943552732467651,0.08190986514091492,0.07658962160348892,0.07329165190458298,0.07407847046852112,0.07207773625850677,0.07527463883161545,0.06588879227638245,0.07392334192991257,0.06360535323619843,0.04968547821044922,0.04612740874290466,0.06416758894920349,0.06921551376581192,0.07046577334403992,0.060766879469156265,0.06194257363677025,0.04834931716322899,0.06553802639245987,0.07097754627466202,0.06695930659770966,0.06924576312303543,0.067400261759758,0.05826321989297867,0.04571755602955818,0.06163875758647919,0.058743007481098175,0.05438004061579704,0.04628763347864151,0.05936237797141075,0.06313302367925644,0.054775454103946686,0.05461150407791138,0.0506063774228096,0.050201185047626495,0.03528834134340286,0.054817210882902145,0.05598009377717972,0.042144741863012314,0.052121851593256,0.053592946380376816,0.05531100556254387,0.04852185770869255,0.06011071056127548,0.05953570082783699,0.05484941974282265,0.052112944424152374,0.04271329194307327,0.05540832132101059,0.010691503994166851,0.01124979741871357,0.013822130858898163,0.007997299544513226,0.008184021338820457,0.017130574211478233,0.017278531566262245,0.02039288729429245,0.013443195261061192,0.02441134676337242,0.009756485000252724,0.024407168850302696,0.017793167382478714,0.013443860225379467,0.006988817825913429,0.01977366767823696,0.02635638415813446,0.0285998173058033,0.017801448702812195,0.02694586105644703,0.012630818411707878,0.012424803338944912,0.011190235614776611,0.023921163752675056,0.019014524295926094,0.016663946211338043,0.011803888715803623,0.01717207580804825,0.01147424802184105,0.01782493107020855,0.013697193004190922,0.021145636215806007,0.016838466748595238,0.006195883732289076,0.02008146233856678,0.016879579052329063,0.01805323176085949,0.008822017349302769,0.008493449538946152,0.009293714538216591,0.012265020981431007,0.005372397601604462,0.014539504423737526,0.0185600183904171,0.010052982717752457,0.011502386070787907,0.008226056583225727,0.009178150445222855,0.009944180026650429,0.010204619728028774,0.009734218940138817,0.02032715082168579,0.014304188080132008,0.006560435984283686,0.011993381194770336,0.019200043752789497,0.01598062738776207,0.02094927988946438,0.011559292674064636,0.010404814966022968,0.019887054339051247,0.012579450383782387,0.008997111581265926,0.009807485155761242],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Layer-Head=Layer 9-Head 11\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Layer 9-Head 11\",\"line\":{\"color\":\"#FFA15A\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Layer 9-Head 11\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[38000,40000,41000,42000,43000,48000,52000,53000,97000,100000,101000,104000,107000,111000,112000,129000,135000,137000,138000,139000],\"xaxis\":\"x\",\"y\":[0.023925190791487694,0.027843698859214783,0.01776779070496559,0.018317172303795815,0.025711344555020332,0.018526440486311913,0.029855648055672646,0.02846292406320572,0.011950287967920303,0.003895883448421955,0.003004722995683551,0.0018873580265790224,0.001349860685877502,0.0014797108015045524,0.0013560284860432148,0.00176145788282156,0.002781546674668789,0.0021300639491528273,0.001709240605123341,0.001875362591817975],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Checkpoint\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Value\"}},\"legend\":{\"title\":{\"text\":\"Layer-Head\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Top Backup Heads Attribution Across Checkpoints (Pythia 160m Default)\"},\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('243ff06f-14da-4046-8309-067bb8ff05ee');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_backup_heads = plot_top_heads(MODEL_TO_VIEW, checkpoint_dict=per_head_logit_diff_deltas, cumulative_nmhs=cumulative_nmhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Checkpoint</th>\n",
       "      <th>Layer-Head</th>\n",
       "      <th>Layer</th>\n",
       "      <th>Head</th>\n",
       "      <th>Value</th>\n",
       "      <th>Previous NMH</th>\n",
       "      <th>Checkpoint_sum</th>\n",
       "      <th>Value_sum</th>\n",
       "      <th>Previous NMH_sum</th>\n",
       "      <th>Top K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>11000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>12000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.020281</td>\n",
       "      <td>True</td>\n",
       "      <td>2013000</td>\n",
       "      <td>0.203263</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>13000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.008192</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>13000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>True</td>\n",
       "      <td>2013000</td>\n",
       "      <td>0.203263</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>14000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.012010</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>15000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.012112</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>17000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.013145</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>18000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>True</td>\n",
       "      <td>2013000</td>\n",
       "      <td>0.203263</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>18000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.012296</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>19000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.012480</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>20000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.028639</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>22000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.024729</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>23000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.021761</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>23000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.025838</td>\n",
       "      <td>True</td>\n",
       "      <td>2013000</td>\n",
       "      <td>0.203263</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>24000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.028552</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>26000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.022190</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>29000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>30000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.019901</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>31000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.021762</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>35000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.020162</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>36000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.018485</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>38000</td>\n",
       "      <td>Layer 9-Head 11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.023925</td>\n",
       "      <td>True</td>\n",
       "      <td>1767000</td>\n",
       "      <td>0.225592</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>39000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.019513</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>40000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.018014</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>40000</td>\n",
       "      <td>Layer 9-Head 11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.027844</td>\n",
       "      <td>True</td>\n",
       "      <td>1767000</td>\n",
       "      <td>0.225592</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>41000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.012220</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>41000</td>\n",
       "      <td>Layer 9-Head 11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.017768</td>\n",
       "      <td>True</td>\n",
       "      <td>1767000</td>\n",
       "      <td>0.225592</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>42000</td>\n",
       "      <td>Layer 9-Head 11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.018317</td>\n",
       "      <td>True</td>\n",
       "      <td>1767000</td>\n",
       "      <td>0.225592</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>43000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.017404</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>43000</td>\n",
       "      <td>Layer 9-Head 11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.025711</td>\n",
       "      <td>True</td>\n",
       "      <td>1767000</td>\n",
       "      <td>0.225592</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>45000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.017807</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>48000</td>\n",
       "      <td>Layer 9-Head 11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.018526</td>\n",
       "      <td>True</td>\n",
       "      <td>1767000</td>\n",
       "      <td>0.225592</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>49000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.016632</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>51000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.020646</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>52000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.019066</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>52000</td>\n",
       "      <td>Layer 9-Head 11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.029856</td>\n",
       "      <td>True</td>\n",
       "      <td>1767000</td>\n",
       "      <td>0.225592</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>53000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.019717</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>53000</td>\n",
       "      <td>Layer 9-Head 11</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.028463</td>\n",
       "      <td>True</td>\n",
       "      <td>1767000</td>\n",
       "      <td>0.225592</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>55000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.011844</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>58000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.015506</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>61000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.020442</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>62000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.017367</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>63000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.018924</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>65000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.018524</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>66000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.013166</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>67000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.031035</td>\n",
       "      <td>True</td>\n",
       "      <td>2013000</td>\n",
       "      <td>0.203263</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>67000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.019930</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>68000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.021193</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>70000</td>\n",
       "      <td>Layer 9-Head 9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.022865</td>\n",
       "      <td>True</td>\n",
       "      <td>5602000</td>\n",
       "      <td>0.951745</td>\n",
       "      <td>78</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>71000</td>\n",
       "      <td>Layer 10-Head 9</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>True</td>\n",
       "      <td>1574000</td>\n",
       "      <td>0.126401</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Checkpoint       Layer-Head  Layer  Head     Value  Previous NMH  \\\n",
       "36        11000   Layer 9-Head 9      9     9  0.007299          True   \n",
       "39        12000   Layer 9-Head 4      9     4  0.020281          True   \n",
       "44        13000   Layer 9-Head 9      9     9  0.008192          True   \n",
       "46        13000   Layer 9-Head 4      9     4  0.010593          True   \n",
       "50        14000   Layer 9-Head 9      9     9  0.012010          True   \n",
       "57        15000   Layer 9-Head 9      9     9  0.012112          True   \n",
       "67        17000   Layer 9-Head 9      9     9  0.013145          True   \n",
       "71        18000   Layer 9-Head 4      9     4  0.013086          True   \n",
       "72        18000   Layer 9-Head 9      9     9  0.012296          True   \n",
       "77        19000   Layer 9-Head 9      9     9  0.012480          True   \n",
       "82        20000   Layer 9-Head 9      9     9  0.028639          True   \n",
       "91        22000   Layer 9-Head 9      9     9  0.024729          True   \n",
       "96        23000   Layer 9-Head 9      9     9  0.021761          True   \n",
       "97        23000   Layer 9-Head 4      9     4  0.025838          True   \n",
       "99        24000   Layer 9-Head 9      9     9  0.028552          True   \n",
       "111       26000   Layer 9-Head 9      9     9  0.022190          True   \n",
       "125       29000   Layer 9-Head 9      9     9  0.022800          True   \n",
       "131       30000   Layer 9-Head 9      9     9  0.019901          True   \n",
       "137       31000   Layer 9-Head 9      9     9  0.021762          True   \n",
       "155       35000   Layer 9-Head 9      9     9  0.020162          True   \n",
       "159       36000   Layer 9-Head 9      9     9  0.018485          True   \n",
       "171       38000  Layer 9-Head 11      9    11  0.023925          True   \n",
       "175       39000   Layer 9-Head 9      9     9  0.019513          True   \n",
       "180       40000   Layer 9-Head 9      9     9  0.018014          True   \n",
       "182       40000  Layer 9-Head 11      9    11  0.027844          True   \n",
       "185       41000   Layer 9-Head 9      9     9  0.012220          True   \n",
       "186       41000  Layer 9-Head 11      9    11  0.017768          True   \n",
       "191       42000  Layer 9-Head 11      9    11  0.018317          True   \n",
       "195       43000   Layer 9-Head 9      9     9  0.017404          True   \n",
       "196       43000  Layer 9-Head 11      9    11  0.025711          True   \n",
       "205       45000   Layer 9-Head 9      9     9  0.017807          True   \n",
       "221       48000  Layer 9-Head 11      9    11  0.018526          True   \n",
       "225       49000   Layer 9-Head 9      9     9  0.016632          True   \n",
       "235       51000   Layer 9-Head 9      9     9  0.020646          True   \n",
       "241       52000   Layer 9-Head 9      9     9  0.019066          True   \n",
       "242       52000  Layer 9-Head 11      9    11  0.029856          True   \n",
       "246       53000   Layer 9-Head 9      9     9  0.019717          True   \n",
       "247       53000  Layer 9-Head 11      9    11  0.028463          True   \n",
       "256       55000   Layer 9-Head 9      9     9  0.011844          True   \n",
       "270       58000   Layer 9-Head 9      9     9  0.015506          True   \n",
       "287       61000   Layer 9-Head 9      9     9  0.020442          True   \n",
       "291       62000   Layer 9-Head 9      9     9  0.017367          True   \n",
       "297       63000   Layer 9-Head 9      9     9  0.018924          True   \n",
       "307       65000   Layer 9-Head 9      9     9  0.018524          True   \n",
       "312       66000   Layer 9-Head 9      9     9  0.013166          True   \n",
       "314       67000   Layer 9-Head 4      9     4  0.031035          True   \n",
       "315       67000   Layer 9-Head 9      9     9  0.019930          True   \n",
       "322       68000   Layer 9-Head 9      9     9  0.021193          True   \n",
       "332       70000   Layer 9-Head 9      9     9  0.022865          True   \n",
       "336       71000  Layer 10-Head 9     10     9  0.003092          True   \n",
       "\n",
       "     Checkpoint_sum  Value_sum  Previous NMH_sum  Top K  \n",
       "36          5602000   0.951745                78   True  \n",
       "39          2013000   0.203263                22  False  \n",
       "44          5602000   0.951745                78   True  \n",
       "46          2013000   0.203263                22  False  \n",
       "50          5602000   0.951745                78   True  \n",
       "57          5602000   0.951745                78   True  \n",
       "67          5602000   0.951745                78   True  \n",
       "71          2013000   0.203263                22  False  \n",
       "72          5602000   0.951745                78   True  \n",
       "77          5602000   0.951745                78   True  \n",
       "82          5602000   0.951745                78   True  \n",
       "91          5602000   0.951745                78   True  \n",
       "96          5602000   0.951745                78   True  \n",
       "97          2013000   0.203263                22  False  \n",
       "99          5602000   0.951745                78   True  \n",
       "111         5602000   0.951745                78   True  \n",
       "125         5602000   0.951745                78   True  \n",
       "131         5602000   0.951745                78   True  \n",
       "137         5602000   0.951745                78   True  \n",
       "155         5602000   0.951745                78   True  \n",
       "159         5602000   0.951745                78   True  \n",
       "171         1767000   0.225592                20   True  \n",
       "175         5602000   0.951745                78   True  \n",
       "180         5602000   0.951745                78   True  \n",
       "182         1767000   0.225592                20   True  \n",
       "185         5602000   0.951745                78   True  \n",
       "186         1767000   0.225592                20   True  \n",
       "191         1767000   0.225592                20   True  \n",
       "195         5602000   0.951745                78   True  \n",
       "196         1767000   0.225592                20   True  \n",
       "205         5602000   0.951745                78   True  \n",
       "221         1767000   0.225592                20   True  \n",
       "225         5602000   0.951745                78   True  \n",
       "235         5602000   0.951745                78   True  \n",
       "241         5602000   0.951745                78   True  \n",
       "242         1767000   0.225592                20   True  \n",
       "246         5602000   0.951745                78   True  \n",
       "247         1767000   0.225592                20   True  \n",
       "256         5602000   0.951745                78   True  \n",
       "270         5602000   0.951745                78   True  \n",
       "287         5602000   0.951745                78   True  \n",
       "291         5602000   0.951745                78   True  \n",
       "297         5602000   0.951745                78   True  \n",
       "307         5602000   0.951745                78   True  \n",
       "312         5602000   0.951745                78   True  \n",
       "314         2013000   0.203263                22  False  \n",
       "315         5602000   0.951745                78   True  \n",
       "322         5602000   0.951745                78   True  \n",
       "332         5602000   0.951745                78   True  \n",
       "336         1574000   0.126401                 2  False  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_backup_heads[top_backup_heads['Previous NMH']==True].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4000: {(7, 6), (7, 10), (8, 9), (9, 4), (9, 11)},\n",
       " 5000: set(),\n",
       " 6000: {(7, 6), (8, 9), (9, 4), (9, 11)},\n",
       " 7000: {(7, 10), (8, 9), (9, 4), (9, 11)},\n",
       " 8000: {(8, 9), (9, 4), (9, 11)},\n",
       " 9000: {(8, 9), (9, 4), (9, 9), (9, 11)},\n",
       " 10000: {(8, 9), (9, 4), (9, 8), (9, 9), (9, 11)},\n",
       " 11000: {(8, 9), (9, 4), (9, 8), (9, 11)},\n",
       " 12000: {(8, 1), (8, 9), (9, 8), (9, 9), (9, 11)},\n",
       " 13000: {(8, 9), (9, 8), (9, 11)},\n",
       " 14000: {(8, 9), (9, 4), (9, 8), (9, 11)},\n",
       " 15000: {(8, 2), (8, 9), (9, 4), (9, 8), (9, 11)},\n",
       " 16000: {(8, 9), (9, 4), (9, 8), (9, 9), (9, 11)},\n",
       " 17000: {(8, 2), (8, 9), (9, 4), (9, 8), (9, 11)},\n",
       " 18000: {(8, 2), (8, 9), (9, 8), (9, 11)},\n",
       " 19000: {(8, 2), (8, 9), (9, 4), (9, 8), (9, 11)},\n",
       " 20000: {(7, 10), (8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11)},\n",
       " 21000: {(8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11)},\n",
       " 22000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11)},\n",
       " 23000: {(8, 1), (8, 2), (8, 9), (9, 8), (9, 11)},\n",
       " 24000: {(7, 10), (8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11)},\n",
       " 25000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11)},\n",
       " 26000: {(7, 10), (8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11)},\n",
       " 27000: {(8, 9), (9, 4), (9, 8), (9, 9), (9, 11)},\n",
       " 28000: {(7, 10),\n",
       "  (8, 1),\n",
       "  (8, 2),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 8),\n",
       "  (9, 9),\n",
       "  (9, 11),\n",
       "  (10, 10)},\n",
       " 29000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 30000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 31000: {(7, 10), (8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 32000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 10)},\n",
       " 33000: {(7, 10),\n",
       "  (8, 1),\n",
       "  (8, 2),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 8),\n",
       "  (9, 9),\n",
       "  (9, 11),\n",
       "  (10, 10)},\n",
       " 34000: {(7, 10),\n",
       "  (8, 1),\n",
       "  (8, 2),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 8),\n",
       "  (9, 9),\n",
       "  (9, 11),\n",
       "  (10, 10)},\n",
       " 35000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 36000: {(7, 10), (8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 37000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 10)},\n",
       " 38000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (10, 10)},\n",
       " 39000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 40000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (10, 10)},\n",
       " 41000: {(8, 2), (8, 9), (9, 4), (9, 8), (10, 10)},\n",
       " 42000: {(8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (10, 10)},\n",
       " 43000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (10, 10)},\n",
       " 44000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 10)},\n",
       " 45000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 46000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 10)},\n",
       " 47000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 10)},\n",
       " 48000: {(8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (10, 10)},\n",
       " 49000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 50000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 10)},\n",
       " 51000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 52000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (10, 10)},\n",
       " 53000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (10, 10)},\n",
       " 54000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 10)},\n",
       " 55000: {(8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 56000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 10)},\n",
       " 57000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 10)},\n",
       " 58000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 59000: {(8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 10)},\n",
       " 60000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 10)},\n",
       " 61000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 62000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 63000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 64000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 10)},\n",
       " 65000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 66000: {(8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 67000: {(8, 1), (8, 2), (8, 9), (9, 8), (9, 11), (10, 10)},\n",
       " 68000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 69000: {(8, 1), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 10)},\n",
       " 70000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 9), (10, 10)},\n",
       " 71000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 72000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 10)},\n",
       " 73000: {(8, 1), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 9), (10, 10)},\n",
       " 74000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 9), (10, 10)},\n",
       " 75000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11)},\n",
       " 76000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 9), (10, 10)},\n",
       " 77000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 9), (10, 10)},\n",
       " 78000: {(8, 1), (8, 9), (9, 4), (9, 8), (9, 9), (9, 11), (10, 9), (10, 10)},\n",
       " 79000: {(8, 1),\n",
       "  (8, 2),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 8),\n",
       "  (9, 9),\n",
       "  (9, 11),\n",
       "  (10, 9),\n",
       "  (10, 10)},\n",
       " 80000: {(8, 2), (8, 9), (9, 8), (9, 11)},\n",
       " 81000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11)},\n",
       " 82000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11), (10, 10)},\n",
       " 83000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 6), (9, 8), (9, 11)},\n",
       " 84000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11)},\n",
       " 85000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 6), (9, 8), (9, 11)},\n",
       " 86000: {(8, 1), (8, 9), (9, 4), (9, 8), (9, 11)},\n",
       " 87000: {(8, 1), (8, 9), (9, 6), (9, 8), (9, 11)},\n",
       " 88000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 6), (9, 8), (9, 11)},\n",
       " 89000: {(8, 1), (8, 2), (8, 9), (9, 6), (9, 8), (9, 11)},\n",
       " 90000: {(8, 1), (8, 2), (8, 9), (9, 8), (9, 11)},\n",
       " 91000: {(8, 1), (8, 9), (9, 4), (9, 6), (9, 8), (9, 11)},\n",
       " 92000: {(8, 1), (8, 9), (9, 4), (9, 8), (9, 11)},\n",
       " 93000: {(8, 1), (8, 9), (9, 4), (9, 6), (9, 8), (9, 11)},\n",
       " 94000: {(7, 10), (8, 1), (8, 2), (8, 9), (9, 4), (9, 6), (9, 8), (9, 11)},\n",
       " 95000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 8), (9, 11)},\n",
       " 96000: {(8, 1), (8, 2), (8, 9), (9, 8), (9, 11)},\n",
       " 97000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 6), (9, 8)},\n",
       " 98000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 6), (9, 8), (9, 11)},\n",
       " 99000: {(8, 1), (8, 2), (8, 9), (9, 6), (9, 8), (9, 11)},\n",
       " 100000: {(8, 1), (8, 2), (8, 9), (9, 6), (9, 8)},\n",
       " 101000: {(8, 1), (8, 2), (8, 9), (9, 6), (9, 8)},\n",
       " 102000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 6), (9, 8), (9, 11)},\n",
       " 103000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 6), (9, 8), (9, 11)},\n",
       " 104000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 6), (9, 8), (9, 9)},\n",
       " 105000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 6), (9, 8), (9, 9), (9, 11)},\n",
       " 106000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 6), (9, 8)},\n",
       " 107000: {(7, 10), (8, 1), (8, 2), (8, 9), (9, 4), (9, 6), (9, 8), (9, 9)},\n",
       " 108000: {(7, 10),\n",
       "  (8, 1),\n",
       "  (8, 2),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 6),\n",
       "  (9, 8),\n",
       "  (9, 9),\n",
       "  (9, 11)},\n",
       " 109000: {(7, 10), (8, 1), (8, 2), (8, 9), (9, 4), (9, 6), (9, 8)},\n",
       " 110000: {(8, 1), (8, 2), (8, 9), (9, 4), (9, 6), (9, 8)},\n",
       " 111000: {(7, 10),\n",
       "  (8, 1),\n",
       "  (8, 2),\n",
       "  (8, 5),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 6),\n",
       "  (9, 8),\n",
       "  (9, 9)},\n",
       " 112000: {(7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 4), (9, 6), (9, 8)},\n",
       " 113000: {(7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 4), (9, 9), (9, 11)},\n",
       " 114000: {(7, 10),\n",
       "  (8, 1),\n",
       "  (8, 2),\n",
       "  (8, 5),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 6),\n",
       "  (9, 8),\n",
       "  (9, 11)},\n",
       " 115000: {(7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 8), (9, 9), (9, 11)},\n",
       " 116000: {(7, 10),\n",
       "  (8, 1),\n",
       "  (8, 2),\n",
       "  (8, 5),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 6),\n",
       "  (9, 8),\n",
       "  (9, 9)},\n",
       " 117000: {(7, 10), (8, 1), (8, 5), (8, 9), (9, 6)},\n",
       " 118000: {(7, 10),\n",
       "  (8, 1),\n",
       "  (8, 2),\n",
       "  (8, 5),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 6),\n",
       "  (9, 8),\n",
       "  (9, 11)},\n",
       " 119000: {(7, 10),\n",
       "  (8, 1),\n",
       "  (8, 2),\n",
       "  (8, 5),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 6),\n",
       "  (9, 8),\n",
       "  (9, 11)},\n",
       " 120000: {(7, 10),\n",
       "  (8, 1),\n",
       "  (8, 2),\n",
       "  (8, 5),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 6),\n",
       "  (9, 9),\n",
       "  (9, 11)},\n",
       " 121000: {(7, 10), (8, 1), (8, 5), (8, 9), (9, 4), (9, 9)},\n",
       " 122000: {(7, 10), (8, 1), (8, 5), (8, 9), (9, 4), (9, 6), (9, 9), (9, 11)},\n",
       " 123000: {(7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 6), (9, 8), (9, 9)},\n",
       " 124000: {(7, 10), (8, 1), (8, 5), (8, 9)},\n",
       " 125000: {(7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 11)},\n",
       " 126000: {(7, 10), (8, 1), (8, 5), (8, 9), (9, 4), (9, 6), (9, 9), (9, 11)},\n",
       " 127000: {(7, 10),\n",
       "  (8, 1),\n",
       "  (8, 2),\n",
       "  (8, 5),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 6),\n",
       "  (9, 9),\n",
       "  (9, 11)},\n",
       " 128000: {(7, 10), (8, 1), (8, 5), (8, 9), (9, 4), (9, 6), (9, 11)},\n",
       " 129000: {(8, 1), (8, 5), (8, 9)},\n",
       " 130000: {(7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 6), (9, 9), (9, 11)},\n",
       " 131000: {(7, 10), (8, 1), (8, 5), (8, 9), (9, 4), (9, 11)},\n",
       " 132000: {(7, 10), (8, 1), (8, 5), (8, 9), (9, 4), (9, 6), (9, 11)},\n",
       " 133000: {(7, 10),\n",
       "  (8, 1),\n",
       "  (8, 2),\n",
       "  (8, 5),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 6),\n",
       "  (9, 9),\n",
       "  (9, 11)},\n",
       " 134000: {(7, 10), (8, 1), (8, 5), (8, 9), (9, 4), (9, 11)},\n",
       " 135000: {(7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 4), (9, 6), (9, 9)},\n",
       " 136000: {(7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 6), (9, 9), (9, 11)},\n",
       " 137000: {(7, 10), (8, 1), (8, 2), (8, 5), (8, 9), (9, 4), (9, 6), (9, 9)},\n",
       " 138000: {(7, 10), (8, 1), (8, 5), (8, 9), (9, 4), (9, 6), (9, 9)},\n",
       " 139000: {(7, 10), (8, 1), (8, 5), (8, 9), (9, 6), (9, 9)},\n",
       " 140000: {(7, 10),\n",
       "  (8, 1),\n",
       "  (8, 2),\n",
       "  (8, 5),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 6),\n",
       "  (9, 9),\n",
       "  (9, 11)},\n",
       " 141000: {(7, 10), (8, 1), (8, 5), (8, 9), (9, 4), (9, 6), (9, 9), (9, 11)},\n",
       " 142000: {(7, 9),\n",
       "  (7, 10),\n",
       "  (8, 1),\n",
       "  (8, 5),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 6),\n",
       "  (9, 9),\n",
       "  (9, 11)},\n",
       " 143000: {(7, 9),\n",
       "  (7, 10),\n",
       "  (8, 1),\n",
       "  (8, 2),\n",
       "  (8, 5),\n",
       "  (8, 9),\n",
       "  (9, 4),\n",
       "  (9, 6),\n",
       "  (9, 9),\n",
       "  (9, 11)}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_nmhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
