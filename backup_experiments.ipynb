{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "import einops\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from torch import Tensor\n",
    "from torchtyping import TensorType as TT\n",
    "from jaxtyping import Float\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as tl_utils\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "import transformer_lens.patching as patching\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.data_utils import generate_data_and_caches\n",
    "from utils.data_processing import (\n",
    "    load_edge_scores_into_dictionary,\n",
    "    read_json_file,\n",
    "    get_ckpts,\n",
    "    load_metrics,\n",
    "    compute_ged,\n",
    "    compute_weighted_ged,\n",
    "    compute_gtd,\n",
    "    compute_jaccard_similarity_to_reference,\n",
    "    compute_jaccard_similarity,\n",
    "    aggregate_metrics_to_tensors_step_number,\n",
    "    get_ckpts\n",
    ")\n",
    "from utils.metrics import compute_logit_diff, _logits_to_mean_logit_diff\n",
    "from utils.visualization import plot_attention_heads, imshow_p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f1b66d2bd00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TASK = 'ioi'\n",
    "PERFORMANCE_METRIC = 'logit_diff'\n",
    "BASE_MODEL = \"pythia-160m\"\n",
    "VARIANT = None\n",
    "CACHE = \"model_cache\"\n",
    "IOI_DATASET_SIZE = 70\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(BASE_MODEL, VARIANT, CHECKPOINT, CACHE, device):\n",
    "    if not VARIANT:\n",
    "        model = HookedTransformer.from_pretrained(\n",
    "            BASE_MODEL,\n",
    "            checkpoint_value=CHECKPOINT,\n",
    "            center_unembed=True,\n",
    "            center_writing_weights=True,\n",
    "            fold_ln=True,\n",
    "            refactor_factored_attn_matrices=False,\n",
    "            #dtype=torch.bfloat16,\n",
    "            **{\"cache_dir\": CACHE},\n",
    "        )\n",
    "    else:\n",
    "        revision = f\"step{CHECKPOINT}\"\n",
    "        source_model = AutoModelForCausalLM.from_pretrained(\n",
    "           VARIANT, revision=revision, cache_dir=CACHE\n",
    "        ).to(device) #.to(torch.bfloat16)\n",
    "\n",
    "        model = HookedTransformer.from_pretrained(\n",
    "            BASE_MODEL,\n",
    "            hf_model=source_model,\n",
    "            center_unembed=False,\n",
    "            center_writing_weights=False,\n",
    "            fold_ln=False,\n",
    "            #dtype=torch.bfloat16,\n",
    "            **{\"cache_dir\": CACHE},\n",
    "        )\n",
    "\n",
    "    model.cfg.use_split_qkv_input = True\n",
    "    model.cfg.use_attn_result = True\n",
    "    model.cfg.use_hook_mlp_in = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_head_names_to_tuple(head_name):\n",
    "    head_name = head_name.replace('a', '')\n",
    "    head_name = head_name.replace('h', '')\n",
    "    layer, head = head_name.split('.')\n",
    "    return (int(layer), int(head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablate_top_head_hook(z: TT[\"batch\", \"pos\", \"head_index\", \"d_head\"], hook, head_idx=0):\n",
    "    z[:, :, head_idx, :] = 0\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_copy_score(model, layer, head, ioi_dataset, verbose=False, neg=False):\n",
    "\n",
    "    # get the activation cache from IOI dataset\n",
    "    logits, cache = model.run_with_cache(ioi_dataset.toks.long())\n",
    "    \n",
    "    # sign adjustment, optional\n",
    "    if neg:\n",
    "        sign = -1\n",
    "    else:\n",
    "        sign = 1\n",
    "\n",
    "    # pass the activations through the first layernorm for block 1 (effectively the result of layer 0's embedding behavior)\n",
    "    z_0 = cache[\"blocks.0.hook_resid_post\"]\n",
    "\n",
    "    # pass the activations through the attention weights (values) for the head and add the bias\n",
    "    v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n",
    "    v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # pass the activations through the attention weights (output only) for the head\n",
    "    o = sign * torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n",
    "\n",
    "    # unembed the activations (layernorm already folded in, so no need to pass through that)\n",
    "    logits = model.unembed(o)\n",
    "\n",
    "    k = 5\n",
    "    n_right = 0\n",
    "\n",
    "    for seq_idx, prompt in enumerate(ioi_dataset.ioi_prompts):\n",
    "        for word in [\"IO\", \"S1\", \"S2\"]:\n",
    "            pred_tokens = [\n",
    "                model.tokenizer.decode(token)\n",
    "                for token in torch.topk(\n",
    "                    logits[seq_idx, ioi_dataset.word_idx[word][seq_idx]], k\n",
    "                ).indices\n",
    "            ]\n",
    "            if \"S\" in word:\n",
    "                name = \"S\"\n",
    "            else:\n",
    "                name = word\n",
    "            if \" \" + prompt[name] in pred_tokens:\n",
    "                n_right += 1\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"-------\")\n",
    "                    print(\"Seq: \" + ioi_dataset.sentences[seq_idx])\n",
    "                    print(\"Target: \" + ioi_dataset.ioi_prompts[seq_idx][name])\n",
    "                    print(\n",
    "                        \" \".join(\n",
    "                            [\n",
    "                                f\"({i+1}):{model.tokenizer.decode(token)}\"\n",
    "                                for i, token in enumerate(\n",
    "                                    torch.topk(\n",
    "                                        logits[\n",
    "                                            seq_idx, ioi_dataset.word_idx[word][seq_idx]\n",
    "                                        ],\n",
    "                                        k,\n",
    "                                    ).indices\n",
    "                                )\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "    percent_right = (n_right / (ioi_dataset.N * 3)) * 100\n",
    "    if percent_right > 0:\n",
    "        print(\n",
    "            f\"Copy circuit for head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%\"\n",
    "        )\n",
    "    model.reset_hooks()\n",
    "    return percent_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_stack_to_logit_diff(\n",
    "    residual_stack: Float[Tensor, \"... batch d_model\"],\n",
    "    cache: ActivationCache,\n",
    "    logit_diff_directions: Float[Tensor, \"batch d_model\"],\n",
    ") -> Float[Tensor, \"...\"]:\n",
    "    '''\n",
    "    Gets the avg logit difference between the correct and incorrect answer for a given\n",
    "    stack of components in the residual stream.\n",
    "    '''\n",
    "    batch_size = residual_stack.size(-2)\n",
    "    scaled_residual_stack = cache.apply_ln_to_stack(residual_stack, layer=-1, pos_slice=-1)\n",
    "    return einops.einsum(\n",
    "        scaled_residual_stack, logit_diff_directions,\n",
    "        \"... batch d_model, batch d_model -> ...\"\n",
    "    ) / batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve & Process Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/153: results/graphs/pythia-160m/ioi/57000.json\n",
      "Processing file 2/153: results/graphs/pythia-160m/ioi/141000.json\n",
      "Processing file 3/153: results/graphs/pythia-160m/ioi/95000.json\n",
      "Processing file 4/153: results/graphs/pythia-160m/ioi/107000.json\n",
      "Processing file 5/153: results/graphs/pythia-160m/ioi/34000.json\n",
      "Processing file 6/153: results/graphs/pythia-160m/ioi/6000.json\n",
      "Processing file 7/153: results/graphs/pythia-160m/ioi/37000.json\n",
      "Processing file 8/153: results/graphs/pythia-160m/ioi/39000.json\n",
      "Processing file 9/153: results/graphs/pythia-160m/ioi/104000.json\n",
      "Processing file 10/153: results/graphs/pythia-160m/ioi/59000.json\n",
      "Processing file 11/153: results/graphs/pythia-160m/ioi/67000.json\n",
      "Processing file 12/153: results/graphs/pythia-160m/ioi/111000.json\n",
      "Processing file 13/153: results/graphs/pythia-160m/ioi/16.json\n",
      "Processing file 14/153: results/graphs/pythia-160m/ioi/76000.json\n",
      "Processing file 15/153: results/graphs/pythia-160m/ioi/1.json\n",
      "Processing file 16/153: results/graphs/pythia-160m/ioi/5000.json\n",
      "Processing file 17/153: results/graphs/pythia-160m/ioi/42000.json\n",
      "Processing file 18/153: results/graphs/pythia-160m/ioi/77000.json\n",
      "Processing file 19/153: results/graphs/pythia-160m/ioi/86000.json\n",
      "Processing file 20/153: results/graphs/pythia-160m/ioi/80000.json\n",
      "Processing file 21/153: results/graphs/pythia-160m/ioi/81000.json\n",
      "Processing file 22/153: results/graphs/pythia-160m/ioi/63000.json\n",
      "Processing file 23/153: results/graphs/pythia-160m/ioi/142000.json\n",
      "Processing file 24/153: results/graphs/pythia-160m/ioi/56000.json\n",
      "Processing file 25/153: results/graphs/pythia-160m/ioi/8000.json\n",
      "Processing file 26/153: results/graphs/pythia-160m/ioi/93000.json\n",
      "Processing file 27/153: results/graphs/pythia-160m/ioi/120000.json\n",
      "Processing file 28/153: results/graphs/pythia-160m/ioi/62000.json\n",
      "Processing file 29/153: results/graphs/pythia-160m/ioi/70000.json\n",
      "Processing file 30/153: results/graphs/pythia-160m/ioi/19000.json\n",
      "Processing file 31/153: results/graphs/pythia-160m/ioi/121000.json\n",
      "Processing file 32/153: results/graphs/pythia-160m/ioi/105000.json\n",
      "Processing file 33/153: results/graphs/pythia-160m/ioi/129000.json\n",
      "Processing file 34/153: results/graphs/pythia-160m/ioi/2000.json\n",
      "Processing file 35/153: results/graphs/pythia-160m/ioi/96000.json\n",
      "Processing file 36/153: results/graphs/pythia-160m/ioi/124000.json\n",
      "Processing file 37/153: results/graphs/pythia-160m/ioi/143000.json\n",
      "Processing file 38/153: results/graphs/pythia-160m/ioi/79000.json\n",
      "Processing file 39/153: results/graphs/pythia-160m/ioi/29000.json\n",
      "Processing file 40/153: results/graphs/pythia-160m/ioi/137000.json\n",
      "Processing file 41/153: results/graphs/pythia-160m/ioi/10000.json\n",
      "Processing file 42/153: results/graphs/pythia-160m/ioi/135000.json\n",
      "Processing file 43/153: results/graphs/pythia-160m/ioi/65000.json\n",
      "Processing file 44/153: results/graphs/pythia-160m/ioi/60000.json\n",
      "Processing file 45/153: results/graphs/pythia-160m/ioi/90000.json\n",
      "Processing file 46/153: results/graphs/pythia-160m/ioi/106000.json\n",
      "Processing file 47/153: results/graphs/pythia-160m/ioi/1000.json\n",
      "Processing file 48/153: results/graphs/pythia-160m/ioi/33000.json\n",
      "Processing file 49/153: results/graphs/pythia-160m/ioi/103000.json\n",
      "Processing file 50/153: results/graphs/pythia-160m/ioi/113000.json\n",
      "Processing file 51/153: results/graphs/pythia-160m/ioi/35000.json\n",
      "Processing file 52/153: results/graphs/pythia-160m/ioi/133000.json\n",
      "Processing file 53/153: results/graphs/pythia-160m/ioi/18000.json\n",
      "Processing file 54/153: results/graphs/pythia-160m/ioi/4.json\n",
      "Processing file 55/153: results/graphs/pythia-160m/ioi/55000.json\n",
      "Processing file 56/153: results/graphs/pythia-160m/ioi/102000.json\n",
      "Processing file 57/153: results/graphs/pythia-160m/ioi/108000.json\n",
      "Processing file 58/153: results/graphs/pythia-160m/ioi/49000.json\n",
      "Processing file 59/153: results/graphs/pythia-160m/ioi/130000.json\n",
      "Processing file 60/153: results/graphs/pythia-160m/ioi/83000.json\n",
      "Processing file 61/153: results/graphs/pythia-160m/ioi/31000.json\n",
      "Processing file 62/153: results/graphs/pythia-160m/ioi/46000.json\n",
      "Processing file 63/153: results/graphs/pythia-160m/ioi/112000.json\n",
      "Processing file 64/153: results/graphs/pythia-160m/ioi/26000.json\n",
      "Processing file 65/153: results/graphs/pythia-160m/ioi/78000.json\n",
      "Processing file 66/153: results/graphs/pythia-160m/ioi/13000.json\n",
      "Processing file 67/153: results/graphs/pythia-160m/ioi/47000.json\n",
      "Processing file 68/153: results/graphs/pythia-160m/ioi/58000.json\n",
      "Processing file 69/153: results/graphs/pythia-160m/ioi/134000.json\n",
      "Processing file 70/153: results/graphs/pythia-160m/ioi/32.json\n",
      "Processing file 71/153: results/graphs/pythia-160m/ioi/128.json\n",
      "Processing file 72/153: results/graphs/pythia-160m/ioi/100000.json\n",
      "Processing file 73/153: results/graphs/pythia-160m/ioi/138000.json\n",
      "Processing file 74/153: results/graphs/pythia-160m/ioi/27000.json\n",
      "Processing file 75/153: results/graphs/pythia-160m/ioi/48000.json\n",
      "Processing file 76/153: results/graphs/pythia-160m/ioi/91000.json\n",
      "Processing file 77/153: results/graphs/pythia-160m/ioi/122000.json\n",
      "Processing file 78/153: results/graphs/pythia-160m/ioi/99000.json\n",
      "Processing file 79/153: results/graphs/pythia-160m/ioi/32000.json\n",
      "Processing file 80/153: results/graphs/pythia-160m/ioi/30000.json\n",
      "Processing file 81/153: results/graphs/pythia-160m/ioi/44000.json\n",
      "Processing file 82/153: results/graphs/pythia-160m/ioi/136000.json\n",
      "Processing file 83/153: results/graphs/pythia-160m/ioi/116000.json\n",
      "Processing file 84/153: results/graphs/pythia-160m/ioi/74000.json\n",
      "Processing file 85/153: results/graphs/pythia-160m/ioi/118000.json\n",
      "Processing file 86/153: results/graphs/pythia-160m/ioi/94000.json\n",
      "Processing file 87/153: results/graphs/pythia-160m/ioi/119000.json\n",
      "Processing file 88/153: results/graphs/pythia-160m/ioi/64000.json\n",
      "Processing file 89/153: results/graphs/pythia-160m/ioi/2.json\n",
      "Processing file 90/153: results/graphs/pythia-160m/ioi/69000.json\n",
      "Processing file 91/153: results/graphs/pythia-160m/ioi/140000.json\n",
      "Processing file 92/153: results/graphs/pythia-160m/ioi/72000.json\n",
      "Processing file 93/153: results/graphs/pythia-160m/ioi/117000.json\n",
      "Processing file 94/153: results/graphs/pythia-160m/ioi/9000.json\n",
      "Processing file 95/153: results/graphs/pythia-160m/ioi/98000.json\n",
      "Processing file 96/153: results/graphs/pythia-160m/ioi/110000.json\n",
      "Processing file 97/153: results/graphs/pythia-160m/ioi/88000.json\n",
      "Processing file 98/153: results/graphs/pythia-160m/ioi/16000.json\n",
      "Processing file 99/153: results/graphs/pythia-160m/ioi/23000.json\n",
      "Processing file 100/153: results/graphs/pythia-160m/ioi/109000.json\n",
      "Processing file 101/153: results/graphs/pythia-160m/ioi/22000.json\n",
      "Processing file 102/153: results/graphs/pythia-160m/ioi/17000.json\n",
      "Processing file 103/153: results/graphs/pythia-160m/ioi/73000.json\n",
      "Processing file 104/153: results/graphs/pythia-160m/ioi/3000.json\n",
      "Processing file 105/153: results/graphs/pythia-160m/ioi/71000.json\n",
      "Processing file 106/153: results/graphs/pythia-160m/ioi/125000.json\n",
      "Processing file 107/153: results/graphs/pythia-160m/ioi/84000.json\n",
      "Processing file 108/153: results/graphs/pythia-160m/ioi/87000.json\n",
      "Processing file 109/153: results/graphs/pythia-160m/ioi/131000.json\n",
      "Processing file 110/153: results/graphs/pythia-160m/ioi/24000.json\n",
      "Processing file 111/153: results/graphs/pythia-160m/ioi/128000.json\n",
      "Processing file 112/153: results/graphs/pythia-160m/ioi/51000.json\n",
      "Processing file 113/153: results/graphs/pythia-160m/ioi/52000.json\n",
      "Processing file 114/153: results/graphs/pythia-160m/ioi/132000.json\n",
      "Processing file 115/153: results/graphs/pythia-160m/ioi/101000.json\n",
      "Processing file 116/153: results/graphs/pythia-160m/ioi/89000.json\n",
      "Processing file 117/153: results/graphs/pythia-160m/ioi/40000.json\n",
      "Processing file 118/153: results/graphs/pythia-160m/ioi/8.json\n",
      "Processing file 119/153: results/graphs/pythia-160m/ioi/43000.json\n",
      "Processing file 120/153: results/graphs/pythia-160m/ioi/54000.json\n",
      "Processing file 121/153: results/graphs/pythia-160m/ioi/11000.json\n",
      "Processing file 122/153: results/graphs/pythia-160m/ioi/36000.json\n",
      "Processing file 123/153: results/graphs/pythia-160m/ioi/45000.json\n",
      "Processing file 124/153: results/graphs/pythia-160m/ioi/50000.json\n",
      "Processing file 125/153: results/graphs/pythia-160m/ioi/114000.json\n",
      "Processing file 126/153: results/graphs/pythia-160m/ioi/64.json\n",
      "Processing file 127/153: results/graphs/pythia-160m/ioi/66000.json\n",
      "Processing file 128/153: results/graphs/pythia-160m/ioi/28000.json\n",
      "Processing file 129/153: results/graphs/pythia-160m/ioi/82000.json\n",
      "Processing file 130/153: results/graphs/pythia-160m/ioi/127000.json\n",
      "Processing file 131/153: results/graphs/pythia-160m/ioi/14000.json\n",
      "Processing file 132/153: results/graphs/pythia-160m/ioi/75000.json\n",
      "Processing file 133/153: results/graphs/pythia-160m/ioi/115000.json\n",
      "Processing file 134/153: results/graphs/pythia-160m/ioi/12000.json\n",
      "Processing file 135/153: results/graphs/pythia-160m/ioi/256.json\n",
      "Processing file 136/153: results/graphs/pythia-160m/ioi/21000.json\n",
      "Processing file 137/153: results/graphs/pythia-160m/ioi/38000.json\n",
      "Processing file 138/153: results/graphs/pythia-160m/ioi/15000.json\n",
      "Processing file 139/153: results/graphs/pythia-160m/ioi/85000.json\n",
      "Processing file 140/153: results/graphs/pythia-160m/ioi/139000.json\n",
      "Processing file 141/153: results/graphs/pythia-160m/ioi/61000.json\n",
      "Processing file 142/153: results/graphs/pythia-160m/ioi/7000.json\n",
      "Processing file 143/153: results/graphs/pythia-160m/ioi/512.json\n",
      "Processing file 144/153: results/graphs/pythia-160m/ioi/68000.json\n",
      "Processing file 145/153: results/graphs/pythia-160m/ioi/53000.json\n",
      "Processing file 146/153: results/graphs/pythia-160m/ioi/25000.json\n",
      "Processing file 147/153: results/graphs/pythia-160m/ioi/97000.json\n",
      "Processing file 148/153: results/graphs/pythia-160m/ioi/4000.json\n",
      "Processing file 149/153: results/graphs/pythia-160m/ioi/123000.json\n",
      "Processing file 150/153: results/graphs/pythia-160m/ioi/20000.json\n",
      "Processing file 151/153: results/graphs/pythia-160m/ioi/41000.json\n",
      "Processing file 152/153: results/graphs/pythia-160m/ioi/126000.json\n",
      "Processing file 153/153: results/graphs/pythia-160m/ioi/92000.json\n"
     ]
    }
   ],
   "source": [
    "folder_path = f'results/graphs/pythia-160m/{TASK}'\n",
    "df = load_edge_scores_into_dictionary(folder_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path = 'results'\n",
    "perf_metrics = load_metrics(directory_path)\n",
    "\n",
    "ckpts = get_ckpts(schedule=\"exp_plus_detail\")\n",
    "#pythia_evals = aggregate_metrics_to_tensors_step_number(\"results/pythia-evals/pythia-v1\")\n",
    "\n",
    "# filter everything before 1000 steps\n",
    "df = df[df['checkpoint'] >= 1000]\n",
    "\n",
    "df[['source', 'target']] = df['edge'].str.split('->', expand=True)\n",
    "len(df['target'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_metric = perf_metrics['pythia-160m'][TASK][PERFORMANCE_METRIC]\n",
    "\n",
    "perf_metric = [x.item() for x in perf_metric]\n",
    "\n",
    "# zip into dictionary with ckpts as key\n",
    "perf_metric_dict = dict(zip(ckpts, perf_metric))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e897f3da6a0452a8ee97bd61987f18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69f2021c0834a749991deb7b6a95364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f550d9b5d9724717a2524a54118e7b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "initial_model = load_model(BASE_MODEL, VARIANT, 143000, CACHE, device)\n",
    "size=70\n",
    "ioi_dataset, abc_dataset = generate_data_and_caches(initial_model, size, verbose=True)\n",
    "\n",
    "answer_tokens = torch.cat((torch.Tensor(ioi_dataset.io_tokenIDs).unsqueeze(1), torch.Tensor(ioi_dataset.s_tokenIDs).unsqueeze(1)), dim=1).to(device)\n",
    "answer_tokens = answer_tokens.long()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Experimental Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTAL_CHECKPOINT = 143000\n",
    "COPY_SCORE_THRESHOLD = 75.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "experimental_model = load_model(BASE_MODEL, VARIANT, EXPERIMENTAL_CHECKPOINT, CACHE, device)\n",
    "orig_logits, orig_cache = experimental_model.run_with_cache(ioi_dataset.toks.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer residual directions shape: torch.Size([70, 2, 768])\n",
      "Logit difference directions shape: torch.Size([70, 768])\n",
      "Final residual stream shape: torch.Size([70, 21, 768])\n",
      "Final token residual stream shape: torch.Size([70, 768])\n"
     ]
    }
   ],
   "source": [
    "answer_residual_directions = experimental_model.tokens_to_residual_directions(answer_tokens)\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "\n",
    "logit_diff_directions = answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n",
    "print(\"Logit difference directions shape:\", logit_diff_directions.shape)\n",
    "\n",
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream: Float[Tensor, \"batch seq d_model\"] = orig_cache[\"resid_post\", -1]\n",
    "print(f\"Final residual stream shape: {final_residual_stream.shape}\")\n",
    "\n",
    "final_token_residual_stream: Float[Tensor, \"batch d_model\"] = final_residual_stream[torch.arange(final_residual_stream.size(0)), ioi_dataset.word_idx[\"end\"]]\n",
    "print(f\"Final token residual stream shape: {final_token_residual_stream.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(final_token_residual_stream.shape[0]):\n",
    "    scaled_final_token_residual_stream = orig_cache.apply_ln_to_stack(final_token_residual_stream[idx].unsqueeze(0), layer=-1, pos_slice=(ioi_dataset.word_idx[\"end\"][idx]-21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 768])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_example = final_token_residual_stream[0].unsqueeze(0)\n",
    "orig_cache.apply_ln_to_stack(first_example, layer=-1, pos_slice=(ioi_dataset.word_idx[\"end\"][0]-21)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 768])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_final_token_residual_stream.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled residual stream shape: torch.Size([70, 768])\n",
      "Calculated average logit diff: 3.2230978012\n",
      "Original logit difference:     4.1640057564\n"
     ]
    }
   ],
   "source": [
    "scaled_final_token_residual_stream = orig_cache.apply_ln_to_stack(final_token_residual_stream, layer=-1, pos_slice=-2)\n",
    "print(f\"Scaled residual stream shape: {scaled_final_token_residual_stream.shape}\")\n",
    "\n",
    "# scaled_final_token_residual_stream: Float[Tensor, \"batch d_model\"] = scaled_residual_stream[torch.arange(final_residual_stream.size(0)), ioi_dataset.word_idx[\"end\"]]\n",
    "# print(f\"Final token residual stream shape: {scaled_final_token_residual_stream.shape}\")\n",
    "\n",
    "average_logit_diff = einops.einsum(\n",
    "    scaled_final_token_residual_stream, logit_diff_directions,\n",
    "    \"batch d_model, batch d_model ->\"\n",
    ") / 70\n",
    "\n",
    "print(f\"Calculated average logit diff: {average_logit_diff:.10f}\")\n",
    "print(f\"Original logit difference:     {_logits_to_mean_logit_diff(orig_logits, ioi_dataset).item():.10f}\")\n",
    "\n",
    "#torch.testing.assert_close(average_logit_diff, clean_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0morig_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_ln_to_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mresidual_stack\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Float[torch.Tensor, 'num_components *batch_and_pos_dims d_model']\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[int]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmlp_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpos_slice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Union[Slice, SliceInput]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_slice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Union[Slice, SliceInput]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhas_batch_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Float[torch.Tensor, 'num_components *batch_and_pos_dims_out d_model']\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Apply Layer Norm to a Stack.\n",
      "\n",
      "Takes a stack of components of the residual stream (eg outputs of decompose_resid or\n",
      "accumulated_resid), treats them as the input to a specific layer, and applies the layer norm\n",
      "scaling of that layer to them, using the cached scale factors - simulating what that\n",
      "component of the residual stream contributes to that layer's input.\n",
      "\n",
      "The layernorm scale is global across the entire residual stream for each layer, batch\n",
      "element and position, which is why we need to use the cached scale factors rather than just\n",
      "applying a new LayerNorm.\n",
      "\n",
      "If the model does not use LayerNorm, it returns the residual stack unchanged.\n",
      "\n",
      "Args:\n",
      "    residual_stack:\n",
      "        A tensor, whose final dimension is d_model. The other trailing dimensions are\n",
      "        assumed to be the same as the stored hook_scale - which may or may not include batch\n",
      "        or position dimensions.\n",
      "    layer:\n",
      "        The layer we're taking the input to. In [0, n_layers], n_layers means the unembed.\n",
      "        None maps to the n_layers case, ie the unembed.\n",
      "    mlp_input:\n",
      "        Whether the input is to the MLP or attn (ie ln2 vs ln1). Defaults to False, ie ln1.\n",
      "        If layer==n_layers, must be False, and we use ln_final\n",
      "    pos_slice:\n",
      "        The slice to take of positions, if residual_stack is not over the full context, None\n",
      "        means do nothing. It is assumed that pos_slice has already been applied to\n",
      "        residual_stack, and this is only applied to the scale. See utils.Slice for details.\n",
      "        Defaults to None, do nothing.\n",
      "    batch_slice:\n",
      "        The slice to take on the batch dimension. Defaults to None, do nothing.\n",
      "    has_batch_dim:\n",
      "        Whether residual_stack has a batch dimension.\n",
      "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.10/site-packages/transformer_lens/ActivationCache.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "orig_cache.apply_ln_to_stack?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of per head residual: torch.Size([144, 70, 21, 768])\n",
      "Shape of per head residual: torch.Size([144, 70, 768])\n",
      "Shape of per head residual: torch.Size([12, 12, 70, 768])\n"
     ]
    }
   ],
   "source": [
    "per_head_residual, labels = orig_cache.stack_head_results(layer=-1, return_labels=True)\n",
    "print(f\"Shape of per head residual: {per_head_residual.shape}\")\n",
    "per_head_residual_final_token = per_head_residual[:, torch.arange(per_head_residual.size(1)), ioi_dataset.word_idx[\"end\"]]\n",
    "print(f\"Shape of per head residual: {per_head_residual_final_token.shape}\")\n",
    "per_head_residual_final_token = einops.rearrange(\n",
    "    per_head_residual_final_token,\n",
    "    \"(layer head) ... -> layer head ...\",\n",
    "    layer=experimental_model.cfg.n_layers\n",
    ")\n",
    "print(f\"Shape of per head residual: {per_head_residual_final_token.shape}\")\n",
    "per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual_final_token, orig_cache, logit_diff_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Head: %{x}<br>Layer: %{y}<br>Logit diff attribution: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           -0.00353040243498981,
           0.0007185207796283066,
           0.0031008741352707148,
           -0.006916897837072611,
           0.000759218935854733,
           -0.005153461825102568,
           0.0013390592066571116,
           0.012127075344324112,
           0.002531958045437932,
           0.0025704270228743553,
           -0.007824940606951714,
           -0.0023916540667414665
          ],
          [
           -0.003171959426254034,
           -0.002859662286937237,
           -0.0004827241937164217,
           0.0037371551152318716,
           -0.001791481045074761,
           -0.005779072176665068,
           0.022090008482336998,
           0.0013964659301564097,
           -0.0049690864980220795,
           -0.00927679892629385,
           0.0007367721409536898,
           0.007490493822842836
          ],
          [
           0.000126502345665358,
           -0.005411976482719183,
           0.0067047057673335075,
           0.008251587860286236,
           -0.017084352672100067,
           -0.0006540719768963754,
           -0.007523450069129467,
           -0.0048188501968979836,
           0.008622019551694393,
           -0.006772941909730434,
           -0.002296942751854658,
           0.008380663581192493
          ],
          [
           0.0066599720157682896,
           -0.008690684102475643,
           0.022082556039094925,
           -0.015535545535385609,
           0.01639663428068161,
           0.000301676569506526,
           0.002868451876565814,
           0.002903458895161748,
           0.02133089117705822,
           -0.0022550378926098347,
           -0.016087770462036133,
           -0.0063632396049797535
          ],
          [
           -0.003294780384749174,
           -0.011619511991739273,
           0.003662675153464079,
           -0.010764486156404018,
           0.002810998586937785,
           -0.009455016814172268,
           -0.00812145322561264,
           0.001911377883516252,
           0.0036615219432860613,
           -0.007698732428252697,
           -0.005430304445326328,
           0.0014411888550966978
          ],
          [
           -0.00994666013866663,
           -0.0055040218867361546,
           0.00007881649798946455,
           -0.011019675992429256,
           -0.006340444087982178,
           -0.0006710263551212847,
           -0.007358909584581852,
           -0.0015520993620157242,
           -0.012985344976186752,
           -0.0015901544829830527,
           0.001399886328727007,
           -0.015415185131132603
          ],
          [
           -0.0048906030133366585,
           0.001012565684504807,
           0.0004405445361044258,
           -0.0024342627730220556,
           -0.002237266395241022,
           0.03366245701909065,
           -0.07484538853168488,
           0.015777355059981346,
           -0.003362591378390789,
           0.0005520747508853674,
           -0.0012542579788714647,
           -0.010192112997174263
          ],
          [
           -0.0011480296961963177,
           -0.00023611901269759983,
           0.0459333136677742,
           -0.008924638852477074,
           -0.006622329354286194,
           0.002337366109713912,
           0.010484561324119568,
           -0.015335122123360634,
           -0.06539850682020187,
           -0.01495426893234253,
           0.005874264519661665,
           -0.007116608787328005
          ],
          [
           0.0004332825483288616,
           0.01984461024403572,
           0.3365021347999573,
           0.0007362865726463497,
           -0.02819938212633133,
           0.00031533651053905487,
           0.006775828544050455,
           0.011143194511532784,
           0.09098780900239944,
           1.1158113479614258,
           0.8828595876693726,
           0.05158009007573128
          ],
          [
           0.0027228263206779957,
           -0.29871734976768494,
           0.015210369601845741,
           -0.009616434574127197,
           0.003825068473815918,
           -0.1574462354183197,
           0.07153177261352539,
           0.09709110856056213,
           0.04153860732913017,
           -0.005306946579366922,
           -0.0010722405277192593,
           0.0012984763598069549
          ],
          [
           0.00342934625223279,
           0.12363655865192413,
           -0.013303964398801327,
           0.0008960453560575843,
           0.004474788438528776,
           -0.014977107755839825,
           0.041657544672489166,
           0.09216608107089996,
           -0.008593717589974403,
           -0.0025540401693433523,
           0.0023165924940258265,
           -0.019147414714097977
          ],
          [
           0.007343484088778496,
           -0.0008345740498043597,
           0.0007495795143768191,
           -0.0019901995547115803,
           -0.00624826131388545,
           0.03643522411584854,
           0.013599702157080173,
           -0.0017384752864018083,
           0.045191690325737,
           0.012262697331607342,
           0.0812177062034607,
           0.002616856712847948
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorbar": {
          "title": {
           "text": "Logit diff attribution"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "margin": {
         "l": 100,
         "r": 100
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Headwise logit diff contribution"
        },
        "width": 600,
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "scaleanchor": "y",
         "showline": true,
         "title": {
          "text": "Head"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "showline": true,
         "title": {
          "text": "Layer"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"d44c1d16-5554-4770-b61d-ae3099ec5f66\" class=\"plotly-graph-div\" style=\"height:525px; width:600px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d44c1d16-5554-4770-b61d-ae3099ec5f66\")) {                    Plotly.newPlot(                        \"d44c1d16-5554-4770-b61d-ae3099ec5f66\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[-0.00353040243498981,0.0007185207796283066,0.0031008741352707148,-0.006916897837072611,0.000759218935854733,-0.005153461825102568,0.0013390592066571116,0.012127075344324112,0.002531958045437932,0.0025704270228743553,-0.007824940606951714,-0.0023916540667414665],[-0.003171959426254034,-0.002859662286937237,-0.0004827241937164217,0.0037371551152318716,-0.001791481045074761,-0.005779072176665068,0.022090008482336998,0.0013964659301564097,-0.0049690864980220795,-0.00927679892629385,0.0007367721409536898,0.007490493822842836],[0.000126502345665358,-0.005411976482719183,0.0067047057673335075,0.008251587860286236,-0.017084352672100067,-0.0006540719768963754,-0.007523450069129467,-0.0048188501968979836,0.008622019551694393,-0.006772941909730434,-0.002296942751854658,0.008380663581192493],[0.0066599720157682896,-0.008690684102475643,0.022082556039094925,-0.015535545535385609,0.01639663428068161,0.000301676569506526,0.002868451876565814,0.002903458895161748,0.02133089117705822,-0.0022550378926098347,-0.016087770462036133,-0.0063632396049797535],[-0.003294780384749174,-0.011619511991739273,0.003662675153464079,-0.010764486156404018,0.002810998586937785,-0.009455016814172268,-0.00812145322561264,0.001911377883516252,0.0036615219432860613,-0.007698732428252697,-0.005430304445326328,0.0014411888550966978],[-0.00994666013866663,-0.0055040218867361546,7.881649798946455e-05,-0.011019675992429256,-0.006340444087982178,-0.0006710263551212847,-0.007358909584581852,-0.0015520993620157242,-0.012985344976186752,-0.0015901544829830527,0.001399886328727007,-0.015415185131132603],[-0.0048906030133366585,0.001012565684504807,0.0004405445361044258,-0.0024342627730220556,-0.002237266395241022,0.03366245701909065,-0.07484538853168488,0.015777355059981346,-0.003362591378390789,0.0005520747508853674,-0.0012542579788714647,-0.010192112997174263],[-0.0011480296961963177,-0.00023611901269759983,0.0459333136677742,-0.008924638852477074,-0.006622329354286194,0.002337366109713912,0.010484561324119568,-0.015335122123360634,-0.06539850682020187,-0.01495426893234253,0.005874264519661665,-0.007116608787328005],[0.0004332825483288616,0.01984461024403572,0.3365021347999573,0.0007362865726463497,-0.02819938212633133,0.00031533651053905487,0.006775828544050455,0.011143194511532784,0.09098780900239944,1.1158113479614258,0.8828595876693726,0.05158009007573128],[0.0027228263206779957,-0.29871734976768494,0.015210369601845741,-0.009616434574127197,0.003825068473815918,-0.1574462354183197,0.07153177261352539,0.09709110856056213,0.04153860732913017,-0.005306946579366922,-0.0010722405277192593,0.0012984763598069549],[0.00342934625223279,0.12363655865192413,-0.013303964398801327,0.0008960453560575843,0.004474788438528776,-0.014977107755839825,0.041657544672489166,0.09216608107089996,-0.008593717589974403,-0.0025540401693433523,0.0023165924940258265,-0.019147414714097977],[0.007343484088778496,-0.0008345740498043597,0.0007495795143768191,-0.0019901995547115803,-0.00624826131388545,0.03643522411584854,0.013599702157080173,-0.0017384752864018083,0.045191690325737,0.012262697331607342,0.0812177062034607,0.002616856712847948]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003eLogit diff attribution: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Logit diff attribution\"}},\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Headwise logit diff contribution\"},\"width\":600,\"margin\":{\"r\":100,\"l\":100}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('d44c1d16-5554-4770-b61d-ae3099ec5f66');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow_p(\n",
    "    per_head_logit_diffs,\n",
    "    title=\"Headwise logit diff contribution\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Logit diff attribution\"},\n",
    "    #coloraxis=dict(colorbar_ticksuffix = \"%\"),\n",
    "    border=True,\n",
    "    width=600,\n",
    "    margin={\"r\": 100, \"l\": 100}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_nmh = df[df['target']=='logits']\n",
    "candidate_nmh = candidate_nmh[candidate_nmh['in_circuit'] == True]\n",
    "\n",
    "candidate_list = candidate_nmh[candidate_nmh['checkpoint']==EXPERIMENTAL_CHECKPOINT]['source'].unique().tolist()\n",
    "candidate_list = [convert_head_names_to_tuple(c) for c in candidate_list if (c[0] != 'm' and c != 'input')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 16.666666666666664%\n",
      "Copy circuit for head 9.1 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 8.4 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 36.666666666666664%\n",
      "Copy circuit for head 7.11 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 29.523809523809526%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 11.10 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 11.9 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 11.8 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 11.6 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 11.5 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 7.6 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 10.8 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 60.476190476190474%\n",
      "Copy circuit for head 10.2 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 10.1 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 7.3 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 5.0 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.2 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 6.5 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 5.8 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 5.4 (sign=1) : Top 5 accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "NMHs = []\n",
    "\n",
    "for layer, head in candidate_list:\n",
    "    copy_score = compute_copy_score(experimental_model, layer, head, ioi_dataset, verbose=False, neg=False)\n",
    "    NMHs.append((layer, head, copy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heads to ablate: [(8, 10), (8, 2), (5, 0)]\n",
      "Original IOI Metric: 4.1640\n",
      "Post ablation IOI Metric: 4.042059421539307\n"
     ]
    }
   ],
   "source": [
    "heads_to_ablate = [x[:2] for x in NMHs if x[2] >= COPY_SCORE_THRESHOLD]\n",
    "head_labels = [f\"L{l}H{h}\" for l in range(experimental_model.cfg.n_layers) for h in range(experimental_model.cfg.n_heads)]\n",
    "\n",
    "print(f\"Heads to ablate: {heads_to_ablate}\")\n",
    "\n",
    "\n",
    "for layer, head in heads_to_ablate:\n",
    "    ablate_head_hook = partial(ablate_top_head_hook, head_idx=head)\n",
    "    experimental_model.blocks[layer].attn.hook_z.add_hook(ablate_head_hook)\n",
    "\n",
    "ablated_logits, ablated_cache = experimental_model.run_with_cache(ioi_dataset.toks)\n",
    "print(f\"Original IOI Metric: {_logits_to_mean_logit_diff(orig_logits, ioi_dataset).item():.4f}\")\n",
    "print(f\"Post ablation IOI Metric: {_logits_to_mean_logit_diff(ablated_logits, ioi_dataset).item()}\")\n",
    "\n",
    "experimental_model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf59dc2772064aef821cc0bcf9ab4099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results['z'].shape = (layer=12, head=12)\n"
     ]
    }
   ],
   "source": [
    "# from path_patching_cm.path_patching import Node, IterNode, path_patch, act_patch\n",
    "# from path_patching_cm.ioi_dataset import IOIDataset, NAMES\n",
    "# experimental_model.reset_hooks()\n",
    "# pm = partial(_logits_to_mean_logit_diff, ioi_dataset=ioi_dataset)\n",
    "\n",
    "# path_patch_resid_post = path_patch(\n",
    "#     experimental_model,\n",
    "#     orig_input=ioi_dataset.toks,\n",
    "#     new_input=abc_dataset.toks,\n",
    "#     sender_nodes=IterNode('z'), # This means iterate over all heads in all layers\n",
    "#     receiver_nodes=Node('resid_post', 11), # This is resid_post at layer 11\n",
    "#     patching_metric=pm,\n",
    "#     verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Head: %{x}<br>Layer: %{y}<br>Logit diff variation: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           242.1990203857422,
           242.28713989257812,
           242.2535400390625,
           242.52687072753906,
           242.17135620117188,
           242.14999389648438,
           242.20278930664062,
           242.1701202392578,
           242.230224609375,
           242.27658081054688,
           242.37025451660156,
           242.33055114746094
          ],
          [
           242.2139129638672,
           242.3118896484375,
           242.5273895263672,
           242.18406677246094,
           242.1522674560547,
           242.28268432617188,
           242.1565704345703,
           242.21690368652344,
           242.17190551757812,
           242.0604248046875,
           242.23907470703125,
           242.15151977539062
          ],
          [
           242.2222442626953,
           242.38812255859375,
           242.1926727294922,
           242.51451110839844,
           242.02584838867188,
           242.26878356933594,
           242.00469970703125,
           242.5017852783203,
           242.16229248046875,
           241.89398193359375,
           242.76126098632812,
           241.700439453125
          ],
          [
           242.14434814453125,
           242.15003967285156,
           242.01171875,
           242.05064392089844,
           242.08815002441406,
           242.11099243164062,
           242.30906677246094,
           242.04425048828125,
           241.89605712890625,
           242.35784912109375,
           242.49575805664062,
           242.2029266357422
          ],
          [
           242.38331604003906,
           242.13543701171875,
           242.27406311035156,
           242.2615966796875,
           242.1264190673828,
           242.31710815429688,
           242.47088623046875,
           242.28062438964844,
           242.65225219726562,
           242.7348175048828,
           242.35987854003906,
           242.80026245117188
          ],
          [
           247.32022094726562,
           242.25390625,
           242.3491973876953,
           242.29345703125,
           242.78201293945312,
           243.029052734375,
           242.71205139160156,
           242.1822967529297,
           242.63194274902344,
           243.51107788085938,
           242.23883056640625,
           245.75123596191406
          ],
          [
           242.6409912109375,
           242.159912109375,
           242.84925842285156,
           243.34071350097656,
           242.05178833007812,
           237.7662353515625,
           248.99420166015625,
           242.12095642089844,
           241.8871612548828,
           241.77792358398438,
           242.2219696044922,
           243.99502563476562
          ],
          [
           242.11341857910156,
           242.24365234375,
           237.72230529785156,
           242.02931213378906,
           242.04693603515625,
           242.16067504882812,
           241.86875915527344,
           242.30845642089844,
           243.74916076660156,
           243.69134521484375,
           240.61990356445312,
           245.95663452148438
          ],
          [
           242.12464904785156,
           270.3601989746094,
           237.65953063964844,
           242.05618286132812,
           244.89393615722656,
           243.04974365234375,
           241.77944946289062,
           238.54933166503906,
           242.06480407714844,
           -348.4947204589844,
           259.9991149902344,
           242.2898712158203
          ],
          [
           242.29368591308594,
           343.5326843261719,
           241.94082641601562,
           255.43548583984375,
           248.7864227294922,
           241.63055419921875,
           242.3332977294922,
           230.95362854003906,
           237.04710388183594,
           241.76390075683594,
           242.16444396972656,
           242.3466033935547
          ],
          [
           242.18067932128906,
           224.69642639160156,
           244.78392028808594,
           242.0959930419922,
           242.11573791503906,
           241.417724609375,
           241.80856323242188,
           234.50160217285156,
           241.85374450683594,
           241.20204162597656,
           240.1172637939453,
           241.561767578125
          ],
          [
           234.96363830566406,
           242.19607543945312,
           229.6427764892578,
           240.44422912597656,
           242.84381103515625,
           242.24700927734375,
           232.50186157226562,
           237.68801879882812,
           240.07225036621094,
           242.89645385742188,
           242.13710021972656,
           240.91098022460938
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorbar": {
          "ticksuffix": "%",
          "title": {
           "text": "Logit diff variation"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "margin": {
         "l": 100,
         "r": 100
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Patching output of attention heads (corrupted -> clean)"
        },
        "width": 600,
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "scaleanchor": "y",
         "showline": true,
         "title": {
          "text": "Head"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "showline": true,
         "title": {
          "text": "Layer"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"84272e94-ba52-477f-98a7-59590eef2302\" class=\"plotly-graph-div\" style=\"height:525px; width:600px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"84272e94-ba52-477f-98a7-59590eef2302\")) {                    Plotly.newPlot(                        \"84272e94-ba52-477f-98a7-59590eef2302\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[242.1990203857422,242.28713989257812,242.2535400390625,242.52687072753906,242.17135620117188,242.14999389648438,242.20278930664062,242.1701202392578,242.230224609375,242.27658081054688,242.37025451660156,242.33055114746094],[242.2139129638672,242.3118896484375,242.5273895263672,242.18406677246094,242.1522674560547,242.28268432617188,242.1565704345703,242.21690368652344,242.17190551757812,242.0604248046875,242.23907470703125,242.15151977539062],[242.2222442626953,242.38812255859375,242.1926727294922,242.51451110839844,242.02584838867188,242.26878356933594,242.00469970703125,242.5017852783203,242.16229248046875,241.89398193359375,242.76126098632812,241.700439453125],[242.14434814453125,242.15003967285156,242.01171875,242.05064392089844,242.08815002441406,242.11099243164062,242.30906677246094,242.04425048828125,241.89605712890625,242.35784912109375,242.49575805664062,242.2029266357422],[242.38331604003906,242.13543701171875,242.27406311035156,242.2615966796875,242.1264190673828,242.31710815429688,242.47088623046875,242.28062438964844,242.65225219726562,242.7348175048828,242.35987854003906,242.80026245117188],[247.32022094726562,242.25390625,242.3491973876953,242.29345703125,242.78201293945312,243.029052734375,242.71205139160156,242.1822967529297,242.63194274902344,243.51107788085938,242.23883056640625,245.75123596191406],[242.6409912109375,242.159912109375,242.84925842285156,243.34071350097656,242.05178833007812,237.7662353515625,248.99420166015625,242.12095642089844,241.8871612548828,241.77792358398438,242.2219696044922,243.99502563476562],[242.11341857910156,242.24365234375,237.72230529785156,242.02931213378906,242.04693603515625,242.16067504882812,241.86875915527344,242.30845642089844,243.74916076660156,243.69134521484375,240.61990356445312,245.95663452148438],[242.12464904785156,270.3601989746094,237.65953063964844,242.05618286132812,244.89393615722656,243.04974365234375,241.77944946289062,238.54933166503906,242.06480407714844,-348.4947204589844,259.9991149902344,242.2898712158203],[242.29368591308594,343.5326843261719,241.94082641601562,255.43548583984375,248.7864227294922,241.63055419921875,242.3332977294922,230.95362854003906,237.04710388183594,241.76390075683594,242.16444396972656,242.3466033935547],[242.18067932128906,224.69642639160156,244.78392028808594,242.0959930419922,242.11573791503906,241.417724609375,241.80856323242188,234.50160217285156,241.85374450683594,241.20204162597656,240.1172637939453,241.561767578125],[234.96363830566406,242.19607543945312,229.6427764892578,240.44422912597656,242.84381103515625,242.24700927734375,232.50186157226562,237.68801879882812,240.07225036621094,242.89645385742188,242.13710021972656,240.91098022460938]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003eLogit diff variation: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Logit diff variation\"},\"ticksuffix\":\"%\"},\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Patching output of attention heads (corrupted -\\u003e clean)\"},\"width\":600,\"margin\":{\"r\":100,\"l\":100}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('84272e94-ba52-477f-98a7-59590eef2302');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imshow_p(\n",
    "#     path_patch_resid_post['z'] * 100,\n",
    "#     title=\"Patching output of attention heads (corrupted -> clean)\",\n",
    "#     labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Logit diff variation\"},\n",
    "#     coloraxis=dict(colorbar_ticksuffix = \"%\"),\n",
    "#     border=True,\n",
    "#     width=600,\n",
    "#     margin={\"r\": 100, \"l\": 100}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of per head residual: torch.Size([144, 70, 21, 768])\n",
      "Shape of per head residual: torch.Size([144, 70, 768])\n",
      "Shape of per head residual: torch.Size([12, 12, 70, 768])\n"
     ]
    }
   ],
   "source": [
    "# per_head_ablated_residual, labels = ablated_cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n",
    "# per_head_ablated_logit_diffs = residual_stack_to_logit_diff(per_head_ablated_residual, ablated_cache, logit_diff_directions)\n",
    "# per_head_ablated_logit_diffs = per_head_ablated_logit_diffs.reshape(experimental_model.cfg.n_layers, experimental_model.cfg.n_heads)\n",
    "\n",
    "per_head_ablated_residual, labels = ablated_cache.stack_head_results(layer=-1, return_labels=True)\n",
    "print(f\"Shape of per head residual: {per_head_ablated_residual.shape}\")\n",
    "per_head_ablated_residual_final_token = per_head_ablated_residual[:, torch.arange(per_head_ablated_residual.size(1)), ioi_dataset.word_idx[\"end\"]]\n",
    "print(f\"Shape of per head residual: {per_head_ablated_residual_final_token.shape}\")\n",
    "per_head_ablated_residual_final_token = einops.rearrange(\n",
    "    per_head_ablated_residual_final_token,\n",
    "    \"(layer head) ... -> layer head ...\",\n",
    "    layer=experimental_model.cfg.n_layers\n",
    ")\n",
    "print(f\"Shape of per head residual: {per_head_ablated_residual_final_token.shape}\")\n",
    "per_head_ablated_logit_diffs = residual_stack_to_logit_diff(per_head_ablated_residual_final_token, orig_cache, logit_diff_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total logit diff contribution above threshold: 0.15\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Logit Diff=%{x}<br>Attention Head=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "h",
         "showlegend": false,
         "textposition": "auto",
         "type": "bar",
         "x": [
          0.0006616362370550632,
          0.0007483839872293174,
          0.0008059411775320768,
          0.0009081465541385114,
          0.0009465010371059179,
          0.0013774322578683496,
          0.0016423200722783804,
          0.0017768099205568433,
          0.0027155012357980013,
          0.0028473525308072567,
          0.003500519320368767,
          0.005796987563371658,
          0.013224068097770214,
          0.0445730984210968,
          0.07181618362665176
         ],
         "xaxis": "x",
         "y": [
          "Layer 8, Head 11",
          "Layer 10, Head 5",
          "Layer 9, Head 3",
          "Layer 6, Head 6",
          "Layer 9, Head 9",
          "Layer 8, Head 9",
          "Layer 10, Head 8",
          "Layer 10, Head 11",
          "Layer 11, Head 5",
          "Layer 10, Head 2",
          "Layer 9, Head 4",
          "Layer 9, Head 7",
          "Layer 10, Head 1",
          "Layer 9, Head 5",
          "Layer 9, Head 1"
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Logit Diff Contribution From Backup Heads (as percentage of original total)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "range": [
          0,
          0.5
         ],
         "title": {
          "text": "Logit Diff"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Attention Head"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"f5e8a3b0-f073-4cc9-8b0a-d90842bf4542\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f5e8a3b0-f073-4cc9-8b0a-d90842bf4542\")) {                    Plotly.newPlot(                        \"f5e8a3b0-f073-4cc9-8b0a-d90842bf4542\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Logit Diff=%{x}\\u003cbr\\u003eAttention Head=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"h\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[0.0006616362370550632,0.0007483839872293174,0.0008059411775320768,0.0009081465541385114,0.0009465010371059179,0.0013774322578683496,0.0016423200722783804,0.0017768099205568433,0.0027155012357980013,0.0028473525308072567,0.003500519320368767,0.005796987563371658,0.013224068097770214,0.0445730984210968,0.07181618362665176],\"xaxis\":\"x\",\"y\":[\"Layer 8, Head 11\",\"Layer 10, Head 5\",\"Layer 9, Head 3\",\"Layer 6, Head 6\",\"Layer 9, Head 9\",\"Layer 8, Head 9\",\"Layer 10, Head 8\",\"Layer 10, Head 11\",\"Layer 11, Head 5\",\"Layer 10, Head 2\",\"Layer 9, Head 4\",\"Layer 9, Head 7\",\"Layer 10, Head 1\",\"Layer 9, Head 5\",\"Layer 9, Head 1\"],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Logit Diff\"},\"range\":[0,0.5]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Attention Head\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Logit Diff Contribution From Backup Heads (as percentage of original total)\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('f5e8a3b0-f073-4cc9-8b0a-d90842bf4542');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#exclusions = [(6, 6), (7, 9), (8, 9)] + [(9, 1), (9, 5)]\n",
    "delta = per_head_ablated_logit_diffs - per_head_logit_diffs\n",
    "for layer, head in heads_to_ablate:\n",
    "    delta[layer, head] = 0\n",
    "\n",
    "plot_attention_heads(\n",
    "    delta/_logits_to_mean_logit_diff(orig_logits, ioi_dataset).item(), \n",
    "    title=\"Logit Diff Contribution From Backup Heads (as percentage of original total)\", \n",
    "    top_n=15, \n",
    "    range_x=[0, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.2894, device='cuda:0'), tensor(-0.1355, device='cuda:0'))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_head_logit_diffs[9, 1], per_head_ablated_logit_diffs[9, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Head: %{x}<br>Layer: %{y}<br>Logit diff attribution: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           -0.0030651704873889685,
           -0.00018763920525088906,
           0.0012753525516018271,
           -0.004400459583848715,
           0.0009994939900934696,
           -0.002835894003510475,
           -0.004618068691343069,
           0.0069204396568238735,
           -0.0027617074083536863,
           0.0029780580662190914,
           -0.002168485661968589,
           -0.005571810062974691
          ],
          [
           -0.011033998802304268,
           0.0047179413959383965,
           0.003635819535702467,
           -0.0020299276802688837,
           0.0005161264562048018,
           -0.007470051757991314,
           0.00515884580090642,
           -0.003160597989335656,
           -0.00196765991859138,
           -0.012529917992651463,
           -0.0018335480708628893,
           -0.0029942442197352648
          ],
          [
           -0.002956810174509883,
           -0.0046282801777124405,
           -0.005846661515533924,
           -0.01057657040655613,
           -0.0028505439404398203,
           0.0014209776418283582,
           -0.009748047217726707,
           -0.0012730362359434366,
           -0.0010421598562970757,
           0.0005243506748229265,
           -0.0013727664481848478,
           0.0007802103064022958
          ],
          [
           -0.013450264930725098,
           -0.0030189654789865017,
           -0.0017619499703869224,
           0.00814393162727356,
           0.005805967375636101,
           0.0030205915682017803,
           -0.0010881758062168956,
           0.0015763117698952556,
           0.004451892338693142,
           0.007576756179332733,
           -0.0056414357386529446,
           -0.00041984193376265466
          ],
          [
           0.012580253183841705,
           -0.003874491900205612,
           0.000671358488034457,
           0.00040195786277763546,
           0.0022752019576728344,
           -0.008450833149254322,
           -0.008694819174706936,
           -0.001614491455256939,
           0.006938043050467968,
           -0.007487870287150145,
           0.0021330066956579685,
           0.00034814313403330743
          ],
          [
           -0.013180006295442581,
           0.005201607942581177,
           0.00007334873225772753,
           0.005019006785005331,
           -0.0009269529255107045,
           -0.00033274421002715826,
           0.002180515555664897,
           0.0007368932128883898,
           -0.003285542596131563,
           -0.005113814026117325,
           0.0021272075828164816,
           -0.01588211953639984
          ],
          [
           -0.011750993318855762,
           -0.0006951751420274377,
           -0.007922150194644928,
           0.0002609642397146672,
           -0.0009789415635168552,
           0.010834249667823315,
           -0.02371041290462017,
           0.00040381518192589283,
           -0.0007509009446948767,
           0.0002696873852983117,
           0.0014316472224891186,
           -0.01007047574967146
          ],
          [
           0.0026797503232955933,
           -0.0030628966633230448,
           0.019301079213619232,
           0.0015588332898914814,
           -0.0017020211089402437,
           0.003527675289660692,
           0.0007981866365298629,
           -0.006541634444147348,
           -0.017747191712260246,
           -0.00781435426324606,
           0.0031581316143274307,
           -0.016779299825429916
          ],
          [
           0.000991969252936542,
           0,
           -0.012864683754742146,
           0.0003436965052969754,
           -0.015037504024803638,
           -0.0025242131669074297,
           0.000835645420011133,
           0.009080670773983002,
           -0.023636238649487495,
           1.6285923719406128,
           0,
           0.0010341823799535632
          ],
          [
           -0.00155819789506495,
           -0.13547541201114655,
           0.003250113921239972,
           -0.05823265388607979,
           -0.001756189507432282,
           0.002010508207604289,
           0.014558189548552036,
           0.028808362782001495,
           0.028839554637670517,
           0.0010353752877563238,
           0.0018928860081359744,
           -0.00034109799889847636
          ],
          [
           -0.00015180648188106716,
           0.1069013774394989,
           -0.0048538632690906525,
           -0.0036925848107784986,
           -0.0000959969765972346,
           0.000037386267649708316,
           0.014028188772499561,
           -0.010380343534052372,
           -0.018337760120630264,
           -0.0012587904930114746,
           0.007216106168925762,
           -0.04812674596905708
          ],
          [
           0.03535592183470726,
           0.0030812681652605534,
           0.06211071088910103,
           0.010899793356657028,
           -0.010648534633219242,
           -0.0022867172956466675,
           0.021818416193127632,
           0.001782512292265892,
           0.014486820437014103,
           -0.012507002800703049,
           -0.003728351555764675,
           0.006146071944385767
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorbar": {
          "title": {
           "text": "Logit diff attribution"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "margin": {
         "l": 100,
         "r": 100
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Headwise logit diff contribution, post NMH KO"
        },
        "width": 600,
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "scaleanchor": "y",
         "showline": true,
         "title": {
          "text": "Head"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "showline": true,
         "title": {
          "text": "Layer"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"0760a75d-05ac-4678-a98d-b6360adb35ca\" class=\"plotly-graph-div\" style=\"height:525px; width:600px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0760a75d-05ac-4678-a98d-b6360adb35ca\")) {                    Plotly.newPlot(                        \"0760a75d-05ac-4678-a98d-b6360adb35ca\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[-0.0030651704873889685,-0.00018763920525088906,0.0012753525516018271,-0.004400459583848715,0.0009994939900934696,-0.002835894003510475,-0.004618068691343069,0.0069204396568238735,-0.0027617074083536863,0.0029780580662190914,-0.002168485661968589,-0.005571810062974691],[-0.011033998802304268,0.0047179413959383965,0.003635819535702467,-0.0020299276802688837,0.0005161264562048018,-0.007470051757991314,0.00515884580090642,-0.003160597989335656,-0.00196765991859138,-0.012529917992651463,-0.0018335480708628893,-0.0029942442197352648],[-0.002956810174509883,-0.0046282801777124405,-0.005846661515533924,-0.01057657040655613,-0.0028505439404398203,0.0014209776418283582,-0.009748047217726707,-0.0012730362359434366,-0.0010421598562970757,0.0005243506748229265,-0.0013727664481848478,0.0007802103064022958],[-0.013450264930725098,-0.0030189654789865017,-0.0017619499703869224,0.00814393162727356,0.005805967375636101,0.0030205915682017803,-0.0010881758062168956,0.0015763117698952556,0.004451892338693142,0.007576756179332733,-0.0056414357386529446,-0.00041984193376265466],[0.012580253183841705,-0.003874491900205612,0.000671358488034457,0.00040195786277763546,0.0022752019576728344,-0.008450833149254322,-0.008694819174706936,-0.001614491455256939,0.006938043050467968,-0.007487870287150145,0.0021330066956579685,0.00034814313403330743],[-0.013180006295442581,0.005201607942581177,7.334873225772753e-05,0.005019006785005331,-0.0009269529255107045,-0.00033274421002715826,0.002180515555664897,0.0007368932128883898,-0.003285542596131563,-0.005113814026117325,0.0021272075828164816,-0.01588211953639984],[-0.011750993318855762,-0.0006951751420274377,-0.007922150194644928,0.0002609642397146672,-0.0009789415635168552,0.010834249667823315,-0.02371041290462017,0.00040381518192589283,-0.0007509009446948767,0.0002696873852983117,0.0014316472224891186,-0.01007047574967146],[0.0026797503232955933,-0.0030628966633230448,0.019301079213619232,0.0015588332898914814,-0.0017020211089402437,0.003527675289660692,0.0007981866365298629,-0.006541634444147348,-0.017747191712260246,-0.00781435426324606,0.0031581316143274307,-0.016779299825429916],[0.000991969252936542,0.0,-0.012864683754742146,0.0003436965052969754,-0.015037504024803638,-0.0025242131669074297,0.000835645420011133,0.009080670773983002,-0.023636238649487495,1.6285923719406128,0.0,0.0010341823799535632],[-0.00155819789506495,-0.13547541201114655,0.003250113921239972,-0.05823265388607979,-0.001756189507432282,0.002010508207604289,0.014558189548552036,0.028808362782001495,0.028839554637670517,0.0010353752877563238,0.0018928860081359744,-0.00034109799889847636],[-0.00015180648188106716,0.1069013774394989,-0.0048538632690906525,-0.0036925848107784986,-9.59969765972346e-05,3.7386267649708316e-05,0.014028188772499561,-0.010380343534052372,-0.018337760120630264,-0.0012587904930114746,0.007216106168925762,-0.04812674596905708],[0.03535592183470726,0.0030812681652605534,0.06211071088910103,0.010899793356657028,-0.010648534633219242,-0.0022867172956466675,0.021818416193127632,0.001782512292265892,0.014486820437014103,-0.012507002800703049,-0.003728351555764675,0.006146071944385767]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003eLogit diff attribution: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Logit diff attribution\"}},\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Headwise logit diff contribution, post NMH KO\"},\"width\":600,\"margin\":{\"r\":100,\"l\":100}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('0760a75d-05ac-4678-a98d-b6360adb35ca');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow_p(\n",
    "    per_head_ablated_logit_diffs,\n",
    "    title=\"Headwise logit diff contribution, post NMH KO\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Logit diff attribution\"},\n",
    "    #coloraxis=dict(colorbar_ticksuffix = \"%\"),\n",
    "    border=True,\n",
    "    width=600,\n",
    "    margin={\"r\": 100, \"l\": 100}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Head: %{x}<br>Layer: %{y}<br>Logit diff attribution: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0.000027093105018138885,
           0.1539498120546341,
           -0.0001407854724675417,
           -0.004561979323625565,
           -0.006723001599311829,
           -0.002499745460227132,
           0.0028177108615636826,
           -0.006024852395057678,
           0.006472060456871986,
           -0.000525142066180706,
           0.0000584764638915658,
           0.000045055465307086706
          ],
          [
           0.00010516730253584683,
           0.04415109008550644,
           0.0026953741908073425,
           0.000053674448281526566,
           0.0006167539395391941,
           0.00032050118898041546,
           -0.00014609284698963165,
           0.011807234026491642,
           -0.002611663192510605,
           0.0011455330532044172,
           0.0002598976716399193,
           -0.005920968949794769
          ],
          [
           0.007757442072033882,
           -0.00018734880723059177,
           0.006160605698823929,
           0.00015823449939489365,
           -0.0002480708062648773,
           0.00024061650037765503,
           0.001951729878783226,
           -0.0012303742114454508,
           0.003356942906975746,
           0.001268681138753891,
           -0.0015929648652672768,
           0.0005253194831311703
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorbar": {
          "title": {
           "text": "Logit diff attribution"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "margin": {
         "l": 100,
         "r": 100
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Change in headwise logit diff contribution, post NMH KO"
        },
        "width": 600,
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "scaleanchor": "y",
         "showline": true,
         "title": {
          "text": "Head"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "showline": true,
         "title": {
          "text": "Layer"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"a53b81c6-2cd2-46a9-ad81-99c8a18601ef\" class=\"plotly-graph-div\" style=\"height:525px; width:600px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a53b81c6-2cd2-46a9-ad81-99c8a18601ef\")) {                    Plotly.newPlot(                        \"a53b81c6-2cd2-46a9-ad81-99c8a18601ef\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[2.7093105018138885e-05,0.1539498120546341,-0.0001407854724675417,-0.004561979323625565,-0.006723001599311829,-0.002499745460227132,0.0028177108615636826,-0.006024852395057678,0.006472060456871986,-0.000525142066180706,5.84764638915658e-05,4.5055465307086706e-05],[0.00010516730253584683,0.04415109008550644,0.0026953741908073425,5.3674448281526566e-05,0.0006167539395391941,0.00032050118898041546,-0.00014609284698963165,0.011807234026491642,-0.002611663192510605,0.0011455330532044172,0.0002598976716399193,-0.005920968949794769],[0.007757442072033882,-0.00018734880723059177,0.006160605698823929,0.00015823449939489365,-0.0002480708062648773,0.00024061650037765503,0.001951729878783226,-0.0012303742114454508,0.003356942906975746,0.001268681138753891,-0.0015929648652672768,0.0005253194831311703]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003eLogit diff attribution: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Logit diff attribution\"}},\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Change in headwise logit diff contribution, post NMH KO\"},\"width\":600,\"margin\":{\"r\":100,\"l\":100}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('a53b81c6-2cd2-46a9-ad81-99c8a18601ef');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow_p(\n",
    "    delta,\n",
    "    title=\"Change in headwise logit diff contribution, post NMH KO\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Logit diff attribution\"},\n",
    "    #coloraxis=dict(colorbar_ticksuffix = \"%\"),\n",
    "    border=True,\n",
    "    width=600,\n",
    "    margin={\"r\": 100, \"l\": 100}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should track three things in response to NMH KO:\n",
    "# 1. The change in the IOI metric\n",
    "# 2. The change in the logit diff contribution for heads in the circuit at this checkpoint\n",
    "# 3. The change in the logit diff contribution for heads not in the circuit at this checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2135, device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formalized Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTAL_CHECKPOINT = 142000\n",
    "COPY_SCORE_THRESHOLD = 75.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model\n",
    "def setup(checkpoint=143000):\n",
    "    model = load_model(BASE_MODEL, VARIANT, checkpoint, CACHE, device)\n",
    "\n",
    "    answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
    "    logit_diff_directions = answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n",
    "\n",
    "    # Test logit_diff_directions with logit diff calculation\n",
    "    # final_residual_stream: Float[Tensor, \"batch seq d_model\"] = orig_cache[\"resid_post\", -1]\n",
    "\n",
    "    # scaled_residual_stream = orig_cache.apply_ln_to_stack(final_residual_stream, layer=-1)\n",
    "    # scaled_final_token_residual_stream: Float[Tensor, \"batch d_model\"] = scaled_residual_stream[torch.arange(scaled_residual_stream.size(0)), ioi_dataset.word_idx[\"end\"]]\n",
    "    \n",
    "    # batch_size = ioi_dataset.toks.shape[0]\n",
    "\n",
    "    # average_logit_diff = einops.einsum(\n",
    "    #     scaled_final_token_residual_stream, logit_diff_directions,\n",
    "    #     \"batch d_model, batch d_model ->\"\n",
    "    # ) / batch_size\n",
    "\n",
    "    # print(f\"Calculated logit diff: {average_logit_diff:.10f}\")\n",
    "\n",
    "    return model, logit_diff_directions\n",
    "\n",
    "# Get metrics & attribution scores\n",
    "def get_metrics_and_attributions(model, logits, cache, dataset=ioi_dataset, logit_diff_directions=logit_diff_directions):\n",
    "\n",
    "    logit_diff = _logits_to_mean_logit_diff(logits, dataset).item()\n",
    "\n",
    "    per_head_residual, labels = cache.stack_head_results(layer=-1, return_labels=True)\n",
    "    per_head_residual_final_token = per_head_residual[:, torch.arange(per_head_residual.size(1)), dataset.word_idx[\"end\"]]\n",
    "    per_head_residual_final_token = einops.rearrange(\n",
    "        per_head_residual_final_token,\n",
    "        \"(layer head) ... -> layer head ...\",\n",
    "        layer=model.cfg.n_layers\n",
    "    )\n",
    "    per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual_final_token, orig_cache, logit_diff_directions)\n",
    "\n",
    "    return logit_diff, per_head_logit_diffs\n",
    "\n",
    "# Get copy scores from circuit members\n",
    "def get_ablation_targets(model, checkpoint, edge_df, dataset=ioi_dataset, threshold=75.0):\n",
    "    candidate_nmh = edge_df[edge_df['target']=='logits']\n",
    "    candidate_nmh = candidate_nmh[candidate_nmh['in_circuit'] == True]\n",
    "\n",
    "    candidate_list = candidate_nmh[candidate_nmh['checkpoint']==checkpoint]['source'].unique().tolist()\n",
    "    candidate_list = [convert_head_names_to_tuple(c) for c in candidate_list if (c[0] != 'm' and c != 'input')]\n",
    "\n",
    "    NMHs = []\n",
    "\n",
    "    for layer, head in candidate_list:\n",
    "        copy_score = compute_copy_score(model, layer, head, dataset, verbose=False, neg=False)\n",
    "        NMHs.append((layer, head, copy_score))\n",
    "\n",
    "    heads_to_ablate = [x[:2] for x in NMHs if x[2] >= threshold]\n",
    "\n",
    "    return NMHs, heads_to_ablate\n",
    "\n",
    "# Run ablation experiment\n",
    "def run_ablated_model(model, dataset=ioi_dataset, ablation_targets=None):\n",
    "    if ablation_targets is None:\n",
    "        ablation_targets = get_ablation_targets(model, dataset)\n",
    "\n",
    "    for layer, head in ablation_targets:\n",
    "        ablate_head_hook = partial(ablate_top_head_hook, head_idx=head)\n",
    "        model.blocks[layer].attn.hook_z.add_hook(ablate_head_hook)\n",
    "\n",
    "    ablated_logits, ablated_cache = model.run_with_cache(dataset.toks)\n",
    "    \n",
    "    model.reset_hooks()\n",
    "\n",
    "    return ablated_logits, ablated_cache\n",
    "\n",
    "# Run experiment\n",
    "def run_iteration(edge_df, checkpoint, experiment_metrics):\n",
    "\n",
    "    model, logit_diff_directions = setup(checkpoint=checkpoint)\n",
    "    orig_logits, orig_cache = model.run_with_cache(ioi_dataset.toks.long())\n",
    "    logit_diff, per_head_logit_diffs = get_metrics_and_attributions(model, orig_logits, orig_cache, logit_diff_directions=logit_diff_directions)\n",
    "\n",
    "    NMHs, ablation_targets = get_ablation_targets(model, checkpoint=checkpoint, edge_df=edge_df, dataset=ioi_dataset, threshold=COPY_SCORE_THRESHOLD)\n",
    "    ablated_logits, ablated_cache = run_ablated_model(model, ioi_dataset, ablation_targets)\n",
    "    ablated_logit_diff, per_head_ablated_logit_diffs = get_metrics_and_attributions(model, ablated_logits, ablated_cache, logit_diff_directions=logit_diff_directions)\n",
    "    \n",
    "    print(f\"Checkpoint {checkpoint}:\")\n",
    "    print(f\"Heads ablated:            {ablation_targets}\")\n",
    "    print(f\"Original logit diff:      {logit_diff:.10f}\")\n",
    "    print(f\"Post ablation logit diff: {ablated_logit_diff:.10f}\")\n",
    "    print(f\"Logit diff % change:      {((ablated_logit_diff - logit_diff) / logit_diff) * 100:.2f}%\")\n",
    "\n",
    "    experiment_metrics[checkpoint] = {\n",
    "        \"logit_diff\": logit_diff,\n",
    "        \"per_head_logit_diffs\": per_head_logit_diffs,\n",
    "        \"ablation_targets\": ablation_targets,\n",
    "        \"ablated_logit_diff\": ablated_logit_diff,\n",
    "        \"per_head_ablated_logit_diffs\": per_head_ablated_logit_diffs,\n",
    "        \"per_head_logit_diff_delta\": per_head_ablated_logit_diffs - per_head_logit_diffs\n",
    "    }\n",
    "\n",
    "    return experiment_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_backup_results(edge_df, checkpoint, experiment_metrics):\n",
    "\n",
    "    # exclude the delta of the ablated heads\n",
    "    for layer, head in experiment_metrics[checkpoint][\"ablation_targets\"]:\n",
    "        experiment_metrics[checkpoint][\"per_head_logit_diff_delta\"][layer, head] = 0\n",
    "\n",
    "    # get the list of heads in the circuit\n",
    "    circuit_heads = edge_df[edge_df['in_circuit'] == True]\n",
    "    circuit_heads = circuit_heads[circuit_heads['checkpoint']==checkpoint]['source'].unique().tolist()\n",
    "    circuit_heads = [convert_head_names_to_tuple(c) for c in circuit_heads if (c[0] != 'm' and c != 'input')]    \n",
    "\n",
    "    in_circuit_head_delta = torch.zeros_like(experiment_metrics[checkpoint][\"per_head_logit_diffs\"])\n",
    "    outside_circuit_head_delta = torch.zeros_like(experiment_metrics[checkpoint][\"per_head_logit_diffs\"])\n",
    "\n",
    "    for layer in range(in_circuit_head_delta.shape[0]):\n",
    "        for head in range(in_circuit_head_delta.shape[1]):\n",
    "            if (layer, head) in circuit_heads:\n",
    "                in_circuit_head_delta[layer, head] = experiment_metrics[checkpoint][\"per_head_logit_diff_delta\"][layer, head]\n",
    "            else:\n",
    "                outside_circuit_head_delta[layer, head] = experiment_metrics[checkpoint][\"per_head_logit_diff_delta\"][layer, head]\n",
    "\n",
    "    experiment_metrics[checkpoint][\"in_circuit_head_delta\"] = in_circuit_head_delta\n",
    "    experiment_metrics[checkpoint][\"outside_circuit_head_delta\"] = outside_circuit_head_delta\n",
    "\n",
    "    experiment_metrics[checkpoint][\"summed_in_circuit_head_delta\"] = in_circuit_head_delta.sum().item()\n",
    "    experiment_metrics[checkpoint][\"summed_outside_circuit_head_delta\"] = outside_circuit_head_delta.sum().item()\n",
    "    experiment_metrics[checkpoint][\"summed_total_head_delta\"] = experiment_metrics[checkpoint][\"per_head_logit_diff_delta\"].sum().item()\n",
    "\n",
    "    # convert tensors to numpy cpu arrays\n",
    "    experiment_metrics[checkpoint][\"per_head_logit_diffs\"] = experiment_metrics[checkpoint][\"per_head_logit_diffs\"].cpu().numpy()\n",
    "    experiment_metrics[checkpoint][\"per_head_ablated_logit_diffs\"] = experiment_metrics[checkpoint][\"per_head_ablated_logit_diffs\"].cpu().numpy()\n",
    "    experiment_metrics[checkpoint][\"per_head_logit_diff_delta\"] = experiment_metrics[checkpoint][\"per_head_logit_diff_delta\"].cpu().numpy()\n",
    "    experiment_metrics[checkpoint][\"in_circuit_head_delta\"] = experiment_metrics[checkpoint][\"in_circuit_head_delta\"].cpu().numpy()\n",
    "    experiment_metrics[checkpoint][\"outside_circuit_head_delta\"] = experiment_metrics[checkpoint][\"outside_circuit_head_delta\"].cpu().numpy()\n",
    "    \n",
    "\n",
    "    return experiment_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 4000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      0.4079501033\n",
      "Post ablation logit diff: 0.4079501033\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Checkpoint 5000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      1.1939518452\n",
      "Post ablation logit diff: 1.1939518452\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 6000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      1.8560795784\n",
      "Post ablation logit diff: 1.8560795784\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 30.952380952380953%\n",
      "Checkpoint 7000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      1.8438247442\n",
      "Post ablation logit diff: 1.8438247442\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 13.80952380952381%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 30.952380952380953%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Checkpoint 8000:\n",
      "Heads ablated:            [(8, 2)]\n",
      "Original logit diff:      2.3623378277\n",
      "Post ablation logit diff: 2.5049126148\n",
      "Logit diff % change:      6.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 27.142857142857142%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 9000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      1.9961944818\n",
      "Post ablation logit diff: 1.9961944818\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Checkpoint 10000:\n",
      "Heads ablated:            [(8, 1), (8, 10)]\n",
      "Original logit diff:      2.4216914177\n",
      "Post ablation logit diff: 2.9410898685\n",
      "Logit diff % change:      21.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 33.33333333333333%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 29.523809523809526%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 82.85714285714286%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 11000:\n",
      "Heads ablated:            [(10, 7)]\n",
      "Original logit diff:      2.5263617039\n",
      "Post ablation logit diff: 2.5399458408\n",
      "Logit diff % change:      0.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 28.095238095238095%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 33.33333333333333%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 98.09523809523809%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 12000:\n",
      "Heads ablated:            [(10, 7), (8, 2)]\n",
      "Original logit diff:      2.4225206375\n",
      "Post ablation logit diff: 2.5701422691\n",
      "Logit diff % change:      6.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 27.61904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 36.666666666666664%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 13000:\n",
      "Heads ablated:            [(10, 7)]\n",
      "Original logit diff:      2.1553795338\n",
      "Post ablation logit diff: 2.1475479603\n",
      "Logit diff % change:      -0.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 27.61904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 30.476190476190478%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Checkpoint 14000:\n",
      "Heads ablated:            [(8, 2), (10, 7)]\n",
      "Original logit diff:      2.3155550957\n",
      "Post ablation logit diff: 2.3603396416\n",
      "Logit diff % change:      1.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 29.04761904761905%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 30.476190476190478%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 15000:\n",
      "Heads ablated:            [(10, 7), (8, 2)]\n",
      "Original logit diff:      2.7272334099\n",
      "Post ablation logit diff: 2.7891819477\n",
      "Logit diff % change:      2.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 79.52380952380952%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 27.142857142857142%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 29.04761904761905%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 26.190476190476193%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 16000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1), (9, 4), (10, 7)]\n",
      "Original logit diff:      2.5029132366\n",
      "Post ablation logit diff: 3.2023310661\n",
      "Logit diff % change:      27.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 20.952380952380953%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 25.71428571428571%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 29.523809523809526%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 81.9047619047619%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 17000:\n",
      "Heads ablated:            [(10, 7), (9, 4), (8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      2.7875125408\n",
      "Post ablation logit diff: 3.0608634949\n",
      "Logit diff % change:      9.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 18.095238095238095%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 29.523809523809526%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 7.142857142857142%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Checkpoint 18000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10), (9, 4)]\n",
      "Original logit diff:      2.5325748920\n",
      "Post ablation logit diff: 2.8281438351\n",
      "Logit diff % change:      11.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 83.80952380952381%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 27.142857142857142%\n",
      "Checkpoint 19000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10), (9, 4)]\n",
      "Original logit diff:      2.8950085640\n",
      "Post ablation logit diff: 2.9904725552\n",
      "Logit diff % change:      3.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.952380952380953%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 29.523809523809526%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 20000:\n",
      "Heads ablated:            [(10, 7), (8, 2)]\n",
      "Original logit diff:      2.7564985752\n",
      "Post ablation logit diff: 2.7654702663\n",
      "Logit diff % change:      0.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 82.38095238095238%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 25.71428571428571%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 28.57142857142857%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 15.238095238095239%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 21000:\n",
      "Heads ablated:            [(9, 4), (8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      2.5499055386\n",
      "Post ablation logit diff: 2.6521060467\n",
      "Logit diff % change:      4.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 9.523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.857142857142854%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 82.85714285714286%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 21.904761904761905%\n",
      "Checkpoint 22000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10), (9, 4)]\n",
      "Original logit diff:      3.0459568501\n",
      "Post ablation logit diff: 3.3277535439\n",
      "Logit diff % change:      9.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 16.666666666666664%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 34.285714285714285%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 80.0%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 32.857142857142854%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Checkpoint 23000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10), (9, 4)]\n",
      "Original logit diff:      3.2300357819\n",
      "Post ablation logit diff: 3.3783571720\n",
      "Logit diff % change:      4.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 20.0%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.952380952380953%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.4 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 25.238095238095237%\n",
      "Checkpoint 24000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10), (9, 4)]\n",
      "Original logit diff:      2.8624346256\n",
      "Post ablation logit diff: 2.9770069122\n",
      "Logit diff % change:      4.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 33.33333333333333%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 7.142857142857142%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 25.238095238095237%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 25000:\n",
      "Heads ablated:            [(10, 7), (8, 2)]\n",
      "Original logit diff:      2.9707119465\n",
      "Post ablation logit diff: 2.9417746067\n",
      "Logit diff % change:      -0.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.4 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.3 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 80.47619047619048%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.857142857142854%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 24.285714285714285%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 11.904761904761903%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 26000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1), (9, 4), (10, 7)]\n",
      "Original logit diff:      3.0810892582\n",
      "Post ablation logit diff: 3.2433164120\n",
      "Logit diff % change:      5.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 12.380952380952381%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 25.238095238095237%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 78.57142857142857%\n",
      "Checkpoint 27000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10), (9, 4)]\n",
      "Original logit diff:      2.7954201698\n",
      "Post ablation logit diff: 2.9526798725\n",
      "Logit diff % change:      5.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 30.952380952380953%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.9047619047619%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 18.571428571428573%\n",
      "Checkpoint 28000:\n",
      "Heads ablated:            [(9, 4), (8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      2.8104608059\n",
      "Post ablation logit diff: 2.9006824493\n",
      "Logit diff % change:      3.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 28.57142857142857%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 34.76190476190476%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 16.19047619047619%\n",
      "Checkpoint 29000:\n",
      "Heads ablated:            [(9, 4), (8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      2.8960051537\n",
      "Post ablation logit diff: 3.0313863754\n",
      "Logit diff % change:      4.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 34.76190476190476%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 7.142857142857142%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 30000:\n",
      "Heads ablated:            [(10, 7), (8, 2)]\n",
      "Original logit diff:      3.0919957161\n",
      "Post ablation logit diff: 3.0055809021\n",
      "Logit diff % change:      -2.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 80.95238095238095%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 21.904761904761905%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 16.19047619047619%\n",
      "Checkpoint 31000:\n",
      "Heads ablated:            [(9, 4), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      2.8969213963\n",
      "Post ablation logit diff: 3.1643812656\n",
      "Logit diff % change:      9.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 15.238095238095239%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.9047619047619%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 22.380952380952383%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 81.42857142857143%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Checkpoint 32000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10), (9, 4)]\n",
      "Original logit diff:      2.9847354889\n",
      "Post ablation logit diff: 3.0086522102\n",
      "Logit diff % change:      0.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 81.42857142857143%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 28.095238095238095%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 7.142857142857142%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.38095238095238%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 14.761904761904763%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 33000:\n",
      "Heads ablated:            [(9, 4), (8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.0808959007\n",
      "Post ablation logit diff: 3.2220525742\n",
      "Logit diff % change:      4.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 20.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 83.80952380952381%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Checkpoint 34000:\n",
      "Heads ablated:            [(10, 7), (9, 4), (8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.1579537392\n",
      "Post ablation logit diff: 3.2764453888\n",
      "Logit diff % change:      3.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.9047619047619%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 35000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.3524379730\n",
      "Post ablation logit diff: 3.2118079662\n",
      "Logit diff % change:      -4.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 13.333333333333334%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 90.47619047619048%\n",
      "Copy circuit for head 10.8 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 33.33333333333333%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 16.666666666666664%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Checkpoint 36000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10), (9, 4)]\n",
      "Original logit diff:      3.3508515358\n",
      "Post ablation logit diff: 3.2988898754\n",
      "Logit diff % change:      -1.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.857142857142854%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Checkpoint 37000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.2621905804\n",
      "Post ablation logit diff: 3.1055059433\n",
      "Logit diff % change:      -4.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 12.380952380952381%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 36.19047619047619%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 25.71428571428571%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Checkpoint 38000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10), (9, 4)]\n",
      "Original logit diff:      3.6383097172\n",
      "Post ablation logit diff: 3.4925708771\n",
      "Logit diff % change:      -4.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.857142857142854%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 13.333333333333334%\n",
      "Checkpoint 39000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1), (9, 4), (10, 7)]\n",
      "Original logit diff:      2.9229447842\n",
      "Post ablation logit diff: 2.9698035717\n",
      "Logit diff % change:      1.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 34.285714285714285%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.666666666666667%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 40000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.3140232563\n",
      "Post ablation logit diff: 3.1424901485\n",
      "Logit diff % change:      -5.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 14.761904761904763%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 84.28571428571429%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 34.285714285714285%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Checkpoint 41000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10), (9, 4)]\n",
      "Original logit diff:      2.9152984619\n",
      "Post ablation logit diff: 3.2460811138\n",
      "Logit diff % change:      11.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.857142857142854%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Checkpoint 42000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (9, 4)]\n",
      "Original logit diff:      2.9047389030\n",
      "Post ablation logit diff: 3.1141977310\n",
      "Logit diff % change:      7.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 33.33333333333333%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 10.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 16.666666666666664%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 43000:\n",
      "Heads ablated:            [(9, 4), (8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.2269480228\n",
      "Post ablation logit diff: 3.4437601566\n",
      "Logit diff % change:      6.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 90.47619047619048%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 30.952380952380953%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 9.523809523809524%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 28.57142857142857%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 12.380952380952381%\n",
      "Checkpoint 44000:\n",
      "Heads ablated:            [(9, 4), (8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.4973435402\n",
      "Post ablation logit diff: 3.5292413235\n",
      "Logit diff % change:      0.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 78.57142857142857%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 28.57142857142857%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 9.523809523809524%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 32.38095238095238%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Checkpoint 45000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.5383903980\n",
      "Post ablation logit diff: 3.4805328846\n",
      "Logit diff % change:      -1.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 8.4 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 33.33333333333333%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 25.238095238095237%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 80.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 9.523809523809524%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 6.666666666666667%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 46000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.1439352036\n",
      "Post ablation logit diff: 3.3119297028\n",
      "Logit diff % change:      5.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 79.04761904761905%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 9.523809523809524%\n",
      "Checkpoint 47000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.8844797611\n",
      "Post ablation logit diff: 3.8075649738\n",
      "Logit diff % change:      -1.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 30.952380952380953%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.476190476190476%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 78.57142857142857%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Checkpoint 48000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.0278050900\n",
      "Post ablation logit diff: 3.0812087059\n",
      "Logit diff % change:      1.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 34.285714285714285%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 21.428571428571427%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 49000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1), (9, 4), (10, 7)]\n",
      "Original logit diff:      3.4284999371\n",
      "Post ablation logit diff: 3.6376180649\n",
      "Logit diff % change:      6.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 21.904761904761905%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 77.61904761904762%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 9.523809523809524%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 29.523809523809526%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 9.523809523809524%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 50000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1), (9, 4), (10, 7)]\n",
      "Original logit diff:      3.2632486820\n",
      "Post ablation logit diff: 3.3333494663\n",
      "Logit diff % change:      2.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 79.52380952380952%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Checkpoint 51000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (9, 4)]\n",
      "Original logit diff:      2.9528236389\n",
      "Post ablation logit diff: 3.0399122238\n",
      "Logit diff % change:      2.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 43.333333333333336%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 9.523809523809524%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 52000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      3.8311219215\n",
      "Post ablation logit diff: 3.9337983131\n",
      "Logit diff % change:      2.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 37.61904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 20.952380952380953%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 5.238095238095238%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 53000:\n",
      "Heads ablated:            [(9, 4), (8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.4068624973\n",
      "Post ablation logit diff: 3.8794670105\n",
      "Logit diff % change:      13.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 20.0%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 76.19047619047619%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 40.476190476190474%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 12.380952380952381%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 54000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (9, 4), (10, 7)]\n",
      "Original logit diff:      3.7859270573\n",
      "Post ablation logit diff: 3.8625731468\n",
      "Logit diff % change:      2.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 11.904761904761903%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 40.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 94.28571428571428%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Checkpoint 55000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10), (9, 4)]\n",
      "Original logit diff:      3.9617173672\n",
      "Post ablation logit diff: 4.1993093491\n",
      "Logit diff % change:      6.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 20.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 12.380952380952381%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 37.61904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Checkpoint 56000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10), (9, 4)]\n",
      "Original logit diff:      3.6347594261\n",
      "Post ablation logit diff: 3.7074680328\n",
      "Logit diff % change:      2.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 20.476190476190474%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 43.333333333333336%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 90.47619047619048%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 13.80952380952381%\n",
      "Checkpoint 57000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10), (9, 4)]\n",
      "Original logit diff:      3.4276928902\n",
      "Post ablation logit diff: 3.7153589725\n",
      "Logit diff % change:      8.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 81.42857142857143%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 75.71428571428571%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Checkpoint 58000:\n",
      "Heads ablated:            [(9, 4), (8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.5477247238\n",
      "Post ablation logit diff: 3.6419198513\n",
      "Logit diff % change:      2.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 77.14285714285715%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 20.952380952380953%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 12.857142857142856%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 42.857142857142854%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 80.47619047619048%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 15.714285714285714%\n",
      "Checkpoint 59000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10), (9, 4)]\n",
      "Original logit diff:      3.9921560287\n",
      "Post ablation logit diff: 4.0272884369\n",
      "Logit diff % change:      0.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 78.0952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 20.952380952380953%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 11.904761904761903%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 17.142857142857142%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 50.476190476190474%\n",
      "Checkpoint 60000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10), (9, 4)]\n",
      "Original logit diff:      4.0257959366\n",
      "Post ablation logit diff: 4.1613192558\n",
      "Logit diff % change:      3.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 49.047619047619044%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 13.333333333333334%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 76.19047619047619%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 15.714285714285714%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 67.61904761904762%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 61000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 10), (8, 2)]\n",
      "Original logit diff:      3.4262354374\n",
      "Post ablation logit diff: 3.7369272709\n",
      "Logit diff % change:      9.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 76.19047619047619%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 21.428571428571427%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 12.857142857142856%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 50.476190476190474%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 67.14285714285714%\n",
      "Checkpoint 62000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.4798839092\n",
      "Post ablation logit diff: 3.6043434143\n",
      "Logit diff % change:      3.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 62.38095238095238%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 50.476190476190474%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 12.857142857142856%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 76.19047619047619%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 21.428571428571427%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 4.285714285714286%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 63000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.7453613281\n",
      "Post ablation logit diff: 3.7493555546\n",
      "Logit diff % change:      0.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 58.57142857142858%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 20.476190476190474%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 47.14285714285714%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 9.523809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 12.380952380952381%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Checkpoint 64000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      4.1064085960\n",
      "Post ablation logit diff: 4.2660937309\n",
      "Logit diff % change:      3.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 20.952380952380953%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 14.761904761904763%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 52.85714285714286%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 46.19047619047619%\n",
      "Checkpoint 65000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.8020830154\n",
      "Post ablation logit diff: 4.0923237801\n",
      "Logit diff % change:      7.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 51.90476190476191%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 14.761904761904763%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 74.28571428571429%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Checkpoint 66000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.8306570053\n",
      "Post ablation logit diff: 4.1305289268\n",
      "Logit diff % change:      7.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 56.19047619047619%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 50.476190476190474%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 14.761904761904763%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 75.71428571428571%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Checkpoint 67000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.7540388107\n",
      "Post ablation logit diff: 4.1097245216\n",
      "Logit diff % change:      9.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 49.523809523809526%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 18.571428571428573%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 51.42857142857142%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 21.904761904761905%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 73.80952380952381%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 68000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      4.1240215302\n",
      "Post ablation logit diff: 4.0902123451\n",
      "Logit diff % change:      -0.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 21.428571428571427%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 48.57142857142857%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 54.285714285714285%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 13.80952380952381%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 75.71428571428571%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 69000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      4.0991587639\n",
      "Post ablation logit diff: 4.1630911827\n",
      "Logit diff % change:      1.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 12.857142857142856%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 43.80952380952381%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 21.904761904761905%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 57.14285714285714%\n",
      "Checkpoint 70000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      4.2723064423\n",
      "Post ablation logit diff: 4.0697107315\n",
      "Logit diff % change:      -4.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 15.238095238095239%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 9.523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 56.19047619047619%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 24.761904761904763%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 45.714285714285715%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 71000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      4.5890569687\n",
      "Post ablation logit diff: 4.5939931870\n",
      "Logit diff % change:      0.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 72.38095238095238%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 18.571428571428573%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 13.80952380952381%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 53.333333333333336%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 24.285714285714285%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 33.33333333333333%\n",
      "Checkpoint 72000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      3.8747401237\n",
      "Post ablation logit diff: 4.1215691566\n",
      "Logit diff % change:      6.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 75.23809523809524%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 11.904761904761903%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 27.142857142857142%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 29.04761904761905%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 53.333333333333336%\n",
      "Checkpoint 73000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      4.1822485924\n",
      "Post ablation logit diff: 4.1410803795\n",
      "Logit diff % change:      -0.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 37.142857142857146%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 28.095238095238095%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 53.80952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 15.714285714285714%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 12.380952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 18.571428571428573%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 72.38095238095238%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Checkpoint 74000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      4.1615295410\n",
      "Post ablation logit diff: 4.2686285973\n",
      "Logit diff % change:      2.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 73.33333333333333%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 18.571428571428573%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 15.714285714285714%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 50.476190476190474%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 26.190476190476193%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 37.61904761904762%\n",
      "Checkpoint 75000:\n",
      "Heads ablated:            [(8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      4.1215491295\n",
      "Post ablation logit diff: 4.1399106979\n",
      "Logit diff % change:      0.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 31.9047619047619%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 24.761904761904763%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 18.095238095238095%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 49.523809523809526%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 72.85714285714285%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 76000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      4.1687951088\n",
      "Post ablation logit diff: 4.0954875946\n",
      "Logit diff % change:      -1.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 53.333333333333336%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 18.095238095238095%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 24.761904761904763%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 29.04761904761905%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 77000:\n",
      "Heads ablated:            [(8, 1), (8, 10), (8, 2)]\n",
      "Original logit diff:      3.8891611099\n",
      "Post ablation logit diff: 4.2572760582\n",
      "Logit diff % change:      9.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 18.571428571428573%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 22.857142857142858%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 52.85714285714286%\n",
      "Checkpoint 78000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      3.9371225834\n",
      "Post ablation logit diff: 4.3170328140\n",
      "Logit diff % change:      9.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 24.761904761904763%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 22.380952380952383%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 52.38095238095239%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 18.095238095238095%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.476190476190476%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 79000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      4.3646335602\n",
      "Post ablation logit diff: 4.4347558022\n",
      "Logit diff % change:      1.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 20.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 22.380952380952383%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 45.714285714285715%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 13.80952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 70.47619047619048%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.476190476190476%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 80000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      4.4647302628\n",
      "Post ablation logit diff: 4.4390048981\n",
      "Logit diff % change:      -0.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 70.0%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 17.142857142857142%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.476190476190476%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 16.666666666666664%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 51.90476190476191%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 20.476190476190474%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 24.761904761904763%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 81000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      4.2701420784\n",
      "Post ablation logit diff: 4.4379029274\n",
      "Logit diff % change:      3.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 11.904761904761903%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 15.714285714285714%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 51.90476190476191%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 82000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      3.7159681320\n",
      "Post ablation logit diff: 4.0923280716\n",
      "Logit diff % change:      10.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 11.904761904761903%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 55.23809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 83000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      3.9499201775\n",
      "Post ablation logit diff: 4.1649451256\n",
      "Logit diff % change:      5.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 58.57142857142858%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 11.904761904761903%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Checkpoint 84000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.6950199604\n",
      "Post ablation logit diff: 3.9329512119\n",
      "Logit diff % change:      6.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 53.80952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 20.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 11.904761904761903%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Checkpoint 85000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      4.4282345772\n",
      "Post ablation logit diff: 4.7502684593\n",
      "Logit diff % change:      7.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 50.476190476190474%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 20.952380952380953%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 74.28571428571429%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 15.714285714285714%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 86000:\n",
      "Heads ablated:            [(8, 1), (8, 10), (8, 2)]\n",
      "Original logit diff:      4.7322416306\n",
      "Post ablation logit diff: 5.0137224197\n",
      "Logit diff % change:      5.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 54.761904761904766%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 87000:\n",
      "Heads ablated:            [(8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      4.2836489677\n",
      "Post ablation logit diff: 4.2701163292\n",
      "Logit diff % change:      -0.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 88000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      4.8267207146\n",
      "Post ablation logit diff: 4.4432830811\n",
      "Logit diff % change:      -7.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 89000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.8253905773\n",
      "Post ablation logit diff: 4.5747113228\n",
      "Logit diff % change:      19.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 29.523809523809526%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 49.047619047619044%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 73.80952380952381%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 90000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      4.0528397560\n",
      "Post ablation logit diff: 4.2227921486\n",
      "Logit diff % change:      4.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 73.33333333333333%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 91000:\n",
      "Heads ablated:            [(8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.5448956490\n",
      "Post ablation logit diff: 4.0310006142\n",
      "Logit diff % change:      13.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 73.80952380952381%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 92000:\n",
      "Heads ablated:            [(8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.1936087608\n",
      "Post ablation logit diff: 3.6164159775\n",
      "Logit diff % change:      13.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Checkpoint 93000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.5039720535\n",
      "Post ablation logit diff: 3.6234147549\n",
      "Logit diff % change:      3.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Checkpoint 94000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.1994807720\n",
      "Post ablation logit diff: 3.6220529079\n",
      "Logit diff % change:      13.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 95000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.9099199772\n",
      "Post ablation logit diff: 4.0544338226\n",
      "Logit diff % change:      3.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 44.285714285714285%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 44.285714285714285%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Checkpoint 96000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.5936560631\n",
      "Post ablation logit diff: 3.7887003422\n",
      "Logit diff % change:      5.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 97000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      3.8341448307\n",
      "Post ablation logit diff: 4.0037817955\n",
      "Logit diff % change:      4.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 53.80952380952381%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Checkpoint 98000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1)]\n",
      "Original logit diff:      3.4808588028\n",
      "Post ablation logit diff: 3.5355644226\n",
      "Logit diff % change:      1.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 74.28571428571429%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 12.857142857142856%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 47.61904761904761%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 12.857142857142856%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 42.38095238095238%\n",
      "Checkpoint 99000:\n",
      "Heads ablated:            [(8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.7597773075\n",
      "Post ablation logit diff: 3.8471848965\n",
      "Logit diff % change:      2.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 15.714285714285714%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 100000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.6237792969\n",
      "Post ablation logit diff: 3.5648171902\n",
      "Logit diff % change:      -1.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 70.0%\n",
      "Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 7.142857142857142%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 51.90476190476191%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 9.523809523809524%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 18.095238095238095%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 79.52380952380952%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Checkpoint 101000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.8082239628\n",
      "Post ablation logit diff: 3.5521578789\n",
      "Logit diff % change:      -6.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 71.9047619047619%\n",
      "Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 25.238095238095237%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 48.57142857142857%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 18.571428571428573%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 80.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 102000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.7126774788\n",
      "Post ablation logit diff: 3.1538050175\n",
      "Logit diff % change:      -15.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 43.80952380952381%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 103000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      4.0290465355\n",
      "Post ablation logit diff: 3.9410593510\n",
      "Logit diff % change:      -2.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 53.80952380952381%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Checkpoint 104000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.7538783550\n",
      "Post ablation logit diff: 3.5125946999\n",
      "Logit diff % change:      -6.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 34.76190476190476%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 18.095238095238095%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 105000:\n",
      "Heads ablated:            [(9, 4), (8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.3821690083\n",
      "Post ablation logit diff: 3.1830327511\n",
      "Logit diff % change:      -5.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 11.904761904761903%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.8 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 81.9047619047619%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 38.57142857142858%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 27.142857142857142%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 106000:\n",
      "Heads ablated:            [(8, 10), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.3200852871\n",
      "Post ablation logit diff: 3.1718740463\n",
      "Logit diff % change:      -4.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 107000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      3.5197105408\n",
      "Post ablation logit diff: 3.4115154743\n",
      "Logit diff % change:      -3.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 70.47619047619048%\n",
      "Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 26.666666666666668%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 11.904761904761903%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Checkpoint 108000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      4.0382037163\n",
      "Post ablation logit diff: 3.7155592442\n",
      "Logit diff % change:      -7.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 30.476190476190478%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Checkpoint 109000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.4698822498\n",
      "Post ablation logit diff: 3.2715702057\n",
      "Logit diff % change:      -5.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 78.0952380952381%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 11.428571428571429%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 110000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.4515860081\n",
      "Post ablation logit diff: 3.1630399227\n",
      "Logit diff % change:      -8.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 29.523809523809526%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Checkpoint 111000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.5362937450\n",
      "Post ablation logit diff: 3.4552791119\n",
      "Logit diff % change:      -2.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 12.857142857142856%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 29.04761904761905%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 29.523809523809526%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 84.76190476190476%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 5.0 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 112000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1), (10, 7), (5, 0)]\n",
      "Original logit diff:      3.8387377262\n",
      "Post ablation logit diff: 3.5162432194\n",
      "Logit diff % change:      -8.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 82.38095238095238%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 17.142857142857142%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 26.666666666666668%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 70.95238095238095%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 15.238095238095239%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 7.142857142857142%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 113000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 10)]\n",
      "Original logit diff:      3.3746984005\n",
      "Post ablation logit diff: 3.4104878902\n",
      "Logit diff % change:      1.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 64.28571428571429%\n",
      "Checkpoint 114000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      3.5887677670\n",
      "Post ablation logit diff: 3.3340165615\n",
      "Logit diff % change:      -7.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 115000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.6147425175\n",
      "Post ablation logit diff: 3.7619035244\n",
      "Logit diff % change:      4.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 80.47619047619048%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 23.809523809523807%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 116000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.9088082314\n",
      "Post ablation logit diff: 3.5745005608\n",
      "Logit diff % change:      -8.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 117000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.7798814774\n",
      "Post ablation logit diff: 3.9516112804\n",
      "Logit diff % change:      4.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 56.666666666666664%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 20.476190476190474%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 80.47619047619048%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Checkpoint 118000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.9797604084\n",
      "Post ablation logit diff: 3.5991773605\n",
      "Logit diff % change:      -9.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 15.714285714285714%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 16.19047619047619%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 9.2 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 52.38095238095239%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 21.428571428571427%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 16.19047619047619%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Checkpoint 119000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      4.2788739204\n",
      "Post ablation logit diff: 3.7523872852\n",
      "Logit diff % change:      -12.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 78.0952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Checkpoint 120000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.9677882195\n",
      "Post ablation logit diff: 3.7978868484\n",
      "Logit diff % change:      -4.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 79.04761904761905%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 15.238095238095239%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 17.142857142857142%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 49.047619047619044%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 16.666666666666664%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 14.761904761904763%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 121000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.8009722233\n",
      "Post ablation logit diff: 3.3016083241\n",
      "Logit diff % change:      -13.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 40.0%\n",
      "Copy circuit for head 9.2 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 20.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 13.333333333333334%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 10.476190476190476%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Copy circuit for head 10.8 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 78.0952380952381%\n",
      "Copy circuit for head 10.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 5.0 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 122000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (10, 7), (5, 0)]\n",
      "Original logit diff:      3.6271793842\n",
      "Post ablation logit diff: 3.3270494938\n",
      "Logit diff % change:      -8.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 21.428571428571427%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Checkpoint 123000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.5861654282\n",
      "Post ablation logit diff: 3.4430840015\n",
      "Logit diff % change:      -3.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 5.0 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 76.19047619047619%\n",
      "Copy circuit for head 10.8 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 13.80952380952381%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 12.380952380952381%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 22.380952380952383%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 37.61904761904762%\n",
      "Checkpoint 124000:\n",
      "Heads ablated:            [(5, 0), (10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      4.1183228493\n",
      "Post ablation logit diff: 3.8154463768\n",
      "Logit diff % change:      -7.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 77.14285714285715%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 11.904761904761903%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 14.761904761904763%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 22.857142857142858%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.2 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 35.714285714285715%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 125000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.8746874332\n",
      "Post ablation logit diff: 3.4469809532\n",
      "Logit diff % change:      -11.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 5.0 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 11.904761904761903%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 23.809523809523807%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 69.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 31.9047619047619%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Checkpoint 126000:\n",
      "Heads ablated:            [(5, 0), (8, 2), (8, 10)]\n",
      "Original logit diff:      4.0825066566\n",
      "Post ablation logit diff: 4.0230722427\n",
      "Logit diff % change:      -1.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 127000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.4978530407\n",
      "Post ablation logit diff: 3.6705369949\n",
      "Logit diff % change:      4.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 22.380952380952383%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Checkpoint 128000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.6692709923\n",
      "Post ablation logit diff: 3.6408827305\n",
      "Logit diff % change:      -0.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 10.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 72.38095238095238%\n",
      "Copy circuit for head 10.8 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.11 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 9.2 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 30.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 129000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      3.6639120579\n",
      "Post ablation logit diff: 3.4995851517\n",
      "Logit diff % change:      -4.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 73.33333333333333%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 130000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.8661262989\n",
      "Post ablation logit diff: 3.5244510174\n",
      "Logit diff % change:      -8.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 29.04761904761905%\n",
      "Copy circuit for head 9.2 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 25.71428571428571%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 10.476190476190476%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.8 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 69.04761904761905%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 131000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.6959834099\n",
      "Post ablation logit diff: 3.6061725616\n",
      "Logit diff % change:      -2.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 63.33333333333333%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 5.0 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 132000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (5, 0)]\n",
      "Original logit diff:      4.1065506935\n",
      "Post ablation logit diff: 4.0874266624\n",
      "Logit diff % change:      -0.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 133000:\n",
      "Heads ablated:            []\n",
      "Original logit diff:      3.4976053238\n",
      "Post ablation logit diff: 3.4976053238\n",
      "Logit diff % change:      0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 72.85714285714285%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 7.142857142857142%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 27.142857142857142%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Checkpoint 134000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      3.6688826084\n",
      "Post ablation logit diff: 3.5282623768\n",
      "Logit diff % change:      -3.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 72.38095238095238%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 28.095238095238095%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 4.285714285714286%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 135000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      3.7239780426\n",
      "Post ablation logit diff: 3.6856148243\n",
      "Logit diff % change:      -1.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 62.38095238095238%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 30.476190476190478%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 46.19047619047619%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Checkpoint 136000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      3.8265407085\n",
      "Post ablation logit diff: 3.9418013096\n",
      "Logit diff % change:      3.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 5.0 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 10.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 66.19047619047619%\n",
      "Copy circuit for head 10.8 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 25.238095238095237%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 29.523809523809526%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 45.714285714285715%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 137000:\n",
      "Heads ablated:            [(5, 0), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.5678858757\n",
      "Post ablation logit diff: 3.7611389160\n",
      "Logit diff % change:      5.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 5.0 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 66.19047619047619%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 32.38095238095238%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 21.428571428571427%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Checkpoint 138000:\n",
      "Heads ablated:            [(5, 0), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.8395097256\n",
      "Post ablation logit diff: 3.8581306934\n",
      "Logit diff % change:      0.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 61.904761904761905%\n",
      "Copy circuit for head 10.8 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 7.11 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 47.14285714285714%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.2 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 20.952380952380953%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 2.380952380952381%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 139000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      3.9237160683\n",
      "Post ablation logit diff: 3.8220305443\n",
      "Logit diff % change:      -2.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 25.71428571428571%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Checkpoint 140000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.5421364307\n",
      "Post ablation logit diff: 3.6935260296\n",
      "Logit diff % change:      4.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 62.857142857142854%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 30.476190476190478%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 46.19047619047619%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 24.285714285714285%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Checkpoint 141000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      3.6798231602\n",
      "Post ablation logit diff: 3.9012730122\n",
      "Logit diff % change:      6.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 16.666666666666664%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 36.666666666666664%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 29.523809523809526%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 10.8 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 60.476190476190474%\n",
      "Copy circuit for head 5.0 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 142000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (5, 0)]\n",
      "Original logit diff:      4.1640057564\n",
      "Post ablation logit diff: 4.0420594215\n",
      "Logit diff % change:      -2.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 66.19047619047619%\n",
      "Copy circuit for head 10.8 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 7.142857142857142%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 33.80952380952381%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 39.523809523809526%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 20.952380952380953%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Checkpoint 143000:\n",
      "Heads ablated:            [(8, 2), (8, 10)]\n",
      "Original logit diff:      4.1340785027\n",
      "Post ablation logit diff: 4.0280790329\n",
      "Logit diff % change:      -2.56%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "experiment_metrics = dict()\n",
    "# create folder\n",
    "os.makedirs(f'results/backup/{BASE_MODEL}', exist_ok=True)\n",
    "\n",
    "for checkpoint in range(4000, 144000, 1000):\n",
    "\n",
    "    experiment_metrics = run_iteration(df, checkpoint=checkpoint, experiment_metrics=experiment_metrics)\n",
    "    experiment_metrics = process_backup_results(df, checkpoint, experiment_metrics)\n",
    "\n",
    "    # save to file, using pytorch format\n",
    "    torch.save(experiment_metrics, f'results/backup/{BASE_MODEL}/nmh_backup_metrics.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_metrics = torch.load(f'results/backup/{BASE_MODEL}/nmh_backup_metrics.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10000: {'logit_diff': 2.421691417694092,\n",
       "  'per_head_logit_diffs': array([[-1.91408989e-03,  5.08053578e-04,  1.10229256e-03,\n",
       "          -5.32193296e-03,  4.81794879e-04, -2.42428086e-03,\n",
       "          -4.75557381e-03,  6.66759536e-03, -2.47690501e-03,\n",
       "           2.50994391e-03, -1.95404282e-03, -4.71131457e-03],\n",
       "         [-1.08297328e-02,  5.00292005e-03,  3.60604352e-03,\n",
       "          -2.03508302e-03,  2.70822609e-04, -7.41213048e-03,\n",
       "           4.72147251e-03, -1.63456390e-03, -2.15056818e-03,\n",
       "          -1.16616171e-02, -2.62599206e-03, -2.21730908e-03],\n",
       "         [-1.35358865e-03, -5.37946820e-03, -3.86888464e-03,\n",
       "          -1.04065752e-02, -3.19856056e-03,  2.13863491e-03,\n",
       "          -9.56851430e-03, -1.56810775e-03, -2.34713836e-04,\n",
       "           1.28491549e-03, -1.61476852e-03, -1.16304622e-03],\n",
       "         [-1.02184508e-02, -3.13080754e-03, -1.41960022e-03,\n",
       "           5.12934895e-03,  7.26311794e-03,  1.29262055e-03,\n",
       "           5.76168313e-05,  1.14478706e-03,  5.53678954e-03,\n",
       "           7.47668045e-03, -5.51896682e-03, -1.02602746e-04],\n",
       "         [ 8.63644015e-03, -5.37554221e-03,  7.06063118e-04,\n",
       "           1.39474426e-03,  2.82610510e-03, -7.31434394e-03,\n",
       "          -7.43994629e-03, -2.35257412e-05,  5.55116124e-03,\n",
       "          -7.32174935e-03,  2.20364984e-03,  5.52014157e-04],\n",
       "         [-1.36549436e-02,  3.82041931e-03,  4.47439306e-05,\n",
       "           4.31634625e-03, -9.98563948e-04,  3.72992683e-04,\n",
       "           1.45102944e-03,  5.39012661e-04, -6.64667133e-03,\n",
       "          -5.38645964e-03,  1.93046720e-03, -1.52507611e-02],\n",
       "         [-1.37457214e-02, -3.15597485e-04, -5.48711186e-03,\n",
       "           3.05924244e-04, -8.90463823e-04,  1.05253141e-02,\n",
       "          -2.27616001e-02,  5.88414085e-04, -7.93999876e-04,\n",
       "           2.18983318e-04,  1.23304257e-03, -9.32389311e-03],\n",
       "         [ 2.55439663e-03, -3.01271514e-03,  1.88881252e-02,\n",
       "           1.44219608e-03, -1.95581280e-03,  3.33082047e-03,\n",
       "           9.94218164e-04, -6.22713566e-03, -1.70650482e-02,\n",
       "          -6.22198265e-03,  2.53624702e-03, -1.66987851e-02],\n",
       "         [ 1.03065954e-03, -6.62101582e-02, -2.43193600e-02,\n",
       "           3.28426715e-04, -1.59712601e-02, -3.40737612e-03,\n",
       "           1.02466333e-03,  1.09886480e-02, -2.29284577e-02,\n",
       "           1.60611045e+00,  2.68226489e-02,  1.22185948e-03],\n",
       "         [-1.25862425e-03, -2.80418724e-01,  3.42229987e-03,\n",
       "          -5.46225756e-02, -1.01405168e-02,  5.43301599e-03,\n",
       "           9.58446786e-03,  3.88904102e-02,  2.01053601e-02,\n",
       "           2.59602116e-03,  1.74845476e-03, -5.03381132e-04],\n",
       "         [-1.40886492e-04,  5.49415946e-02, -7.99600035e-03,\n",
       "          -3.62123898e-03, -8.22510570e-04, -1.21556025e-03,\n",
       "           1.50631210e-02, -1.79757588e-02, -1.59531552e-02,\n",
       "          -2.64453026e-03,  5.43406885e-03, -4.42481749e-02],\n",
       "         [ 2.80136075e-02,  3.13092675e-03,  5.31819761e-02,\n",
       "           1.03081241e-02, -1.59137212e-02, -1.91015005e-03,\n",
       "           2.11009961e-02,  5.84649807e-03,  1.09021021e-02,\n",
       "          -1.16840461e-02, -2.10282207e-03,  3.28322244e-03]], dtype=float32),\n",
       "  'ablation_targets': [(8, 1), (8, 10)],\n",
       "  'ablated_logit_diff': 2.9410898685455322,\n",
       "  'per_head_ablated_logit_diffs': array([[-1.91408989e-03,  5.08053578e-04,  1.10229256e-03,\n",
       "          -5.32193296e-03,  4.81794879e-04, -2.42428086e-03,\n",
       "          -4.75557381e-03,  6.66759536e-03, -2.47690501e-03,\n",
       "           2.50994391e-03, -1.95404282e-03, -4.71131457e-03],\n",
       "         [-1.08297328e-02,  5.00292005e-03,  3.60604352e-03,\n",
       "          -2.03508302e-03,  2.70822609e-04, -7.41213048e-03,\n",
       "           4.72147251e-03, -1.63456390e-03, -2.15056818e-03,\n",
       "          -1.16616171e-02, -2.62599206e-03, -2.21730908e-03],\n",
       "         [-1.35358865e-03, -5.37946820e-03, -3.86888464e-03,\n",
       "          -1.04065752e-02, -3.19856056e-03,  2.13863491e-03,\n",
       "          -9.56851430e-03, -1.56810775e-03, -2.34713836e-04,\n",
       "           1.28491549e-03, -1.61476852e-03, -1.16304622e-03],\n",
       "         [-1.02184508e-02, -3.13080754e-03, -1.41960022e-03,\n",
       "           5.12934895e-03,  7.26311794e-03,  1.29262055e-03,\n",
       "           5.76168313e-05,  1.14478706e-03,  5.53678954e-03,\n",
       "           7.47668045e-03, -5.51896682e-03, -1.02602746e-04],\n",
       "         [ 8.63644015e-03, -5.37554221e-03,  7.06063118e-04,\n",
       "           1.39474426e-03,  2.82610510e-03, -7.31434394e-03,\n",
       "          -7.43994629e-03, -2.35257412e-05,  5.55116124e-03,\n",
       "          -7.32174935e-03,  2.20364984e-03,  5.52014157e-04],\n",
       "         [-1.36549436e-02,  3.82041931e-03,  4.47439306e-05,\n",
       "           4.31634625e-03, -9.98563948e-04,  3.72992683e-04,\n",
       "           1.45102944e-03,  5.39012661e-04, -6.64667133e-03,\n",
       "          -5.38645964e-03,  1.93046720e-03, -1.52507611e-02],\n",
       "         [-1.37457214e-02, -3.15597485e-04, -5.48711186e-03,\n",
       "           3.05924244e-04, -8.90463823e-04,  1.05253141e-02,\n",
       "          -2.27616001e-02,  5.88414085e-04, -7.93999876e-04,\n",
       "           2.18983318e-04,  1.23304257e-03, -9.32389311e-03],\n",
       "         [ 2.55439663e-03, -3.01271514e-03,  1.88881252e-02,\n",
       "           1.44219608e-03, -1.95581280e-03,  3.33082047e-03,\n",
       "           9.94218164e-04, -6.22713566e-03, -1.70650482e-02,\n",
       "          -6.22198265e-03,  2.53624702e-03, -1.66987851e-02],\n",
       "         [ 1.03065954e-03,  0.00000000e+00, -2.43193600e-02,\n",
       "           3.28426715e-04, -1.59712601e-02, -3.40737612e-03,\n",
       "           1.02466333e-03,  1.09886480e-02, -2.29284577e-02,\n",
       "           1.60611045e+00,  0.00000000e+00,  1.22185948e-03],\n",
       "         [-1.21348130e-03, -1.32939622e-01,  3.87115427e-03,\n",
       "          -5.89928478e-02, -2.01236960e-02,  2.41744914e-03,\n",
       "           1.20263631e-02,  3.22960578e-02,  2.70095021e-02,\n",
       "           2.25578109e-03,  1.78891269e-03, -5.26577351e-04],\n",
       "         [-2.01656349e-05,  9.67314020e-02, -5.17477049e-03,\n",
       "          -3.59925884e-03, -2.08084050e-04, -9.85656632e-04,\n",
       "           1.47554837e-02, -7.29296915e-03, -1.85164753e-02,\n",
       "          -1.30311633e-03,  5.44848619e-03, -5.05574383e-02],\n",
       "         [ 3.55648845e-02,  3.01066693e-03,  5.96716888e-02,\n",
       "           1.07947243e-02, -1.62741486e-02, -1.65560376e-03,\n",
       "           2.28700358e-02,  4.17552749e-03,  1.40063409e-02,\n",
       "          -1.05687398e-02, -3.15488013e-03,  3.84876085e-03]], dtype=float32),\n",
       "  'per_head_logit_diff_delta': array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 4.51429514e-05,  1.47479102e-01,  4.48854407e-04,\n",
       "          -4.37027216e-03, -9.98317916e-03, -3.01556685e-03,\n",
       "           2.44189519e-03, -6.59435242e-03,  6.90414198e-03,\n",
       "          -3.40240076e-04,  4.04579332e-05, -2.31962185e-05],\n",
       "         [ 1.20720855e-04,  4.17898074e-02,  2.82122986e-03,\n",
       "           2.19801441e-05,  6.14426506e-04,  2.29903613e-04,\n",
       "          -3.07637267e-04,  1.06827896e-02, -2.56332010e-03,\n",
       "           1.34141394e-03,  1.44173391e-05, -6.30926341e-03],\n",
       "         [ 7.55127706e-03, -1.20259821e-04,  6.48971274e-03,\n",
       "           4.86600213e-04, -3.60427424e-04,  2.54546292e-04,\n",
       "           1.76903978e-03, -1.67097058e-03,  3.10423877e-03,\n",
       "           1.11530628e-03, -1.05205807e-03,  5.65538416e-04]], dtype=float32),\n",
       "  'in_circuit_head_delta': array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.1474791 , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.04178981, 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.00648971, 0.        , 0.        ,\n",
       "          0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "          0.        , 0.        ]], dtype=float32),\n",
       "  'outside_circuit_head_delta': array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 4.51429514e-05,  0.00000000e+00,  4.48854407e-04,\n",
       "          -4.37027216e-03, -9.98317916e-03, -3.01556685e-03,\n",
       "           2.44189519e-03, -6.59435242e-03,  6.90414198e-03,\n",
       "          -3.40240076e-04,  4.04579332e-05, -2.31962185e-05],\n",
       "         [ 1.20720855e-04,  0.00000000e+00,  2.82122986e-03,\n",
       "           2.19801441e-05,  6.14426506e-04,  2.29903613e-04,\n",
       "          -3.07637267e-04,  1.06827896e-02, -2.56332010e-03,\n",
       "           1.34141394e-03,  1.44173391e-05, -6.30926341e-03],\n",
       "         [ 7.55127706e-03, -1.20259821e-04,  0.00000000e+00,\n",
       "           4.86600213e-04, -3.60427424e-04,  2.54546292e-04,\n",
       "           1.76903978e-03, -1.67097058e-03,  3.10423877e-03,\n",
       "           1.11530628e-03, -1.05205807e-03,  5.65538416e-04]], dtype=float32),\n",
       "  'summed_in_circuit_head_delta': 0.1957586258649826,\n",
       "  'summed_outside_circuit_head_delta': 0.0038631781935691833,\n",
       "  'summed_total_head_delta': 0.1996217966079712},\n",
       " 11000: {'logit_diff': 2.5263617038726807,\n",
       "  'per_head_logit_diffs': array([[-2.0823958e-03,  2.5867738e-03,  5.2267761e-04, -4.6802289e-03,\n",
       "           1.6007047e-03, -2.7582503e-03, -4.4457396e-03,  8.6377403e-03,\n",
       "          -1.4312349e-03,  1.2539384e-03, -1.4292729e-03, -2.8229048e-03],\n",
       "         [-1.0031196e-02,  3.9439620e-03,  2.3167252e-03, -1.6789801e-03,\n",
       "          -1.4219059e-03, -5.6671835e-03,  4.8098578e-03,  1.2600280e-03,\n",
       "          -7.2644936e-04, -9.3485471e-03, -1.8021541e-03, -2.6482942e-03],\n",
       "         [-3.7112478e-03, -3.0432353e-03, -2.5033350e-03, -1.0333395e-02,\n",
       "          -4.2337044e-03,  5.9466378e-04, -8.5988985e-03,  3.4921372e-04,\n",
       "           1.1995236e-03,  3.1793134e-03, -2.6298056e-03, -1.3328518e-03],\n",
       "         [-5.3762374e-03, -2.2268791e-03, -2.9574607e-03,  5.2970723e-04,\n",
       "           9.6518807e-03,  4.0439315e-04,  1.3975416e-03, -8.8888971e-04,\n",
       "           7.3730852e-03,  6.5146754e-03, -5.9225885e-03, -1.7568448e-03],\n",
       "         [ 8.9085503e-03, -5.4025822e-03,  1.3122287e-03,  9.7176537e-04,\n",
       "           3.0761140e-03, -6.7550000e-03, -6.5296916e-03, -7.4025511e-04,\n",
       "           6.0076406e-03, -5.7063592e-03,  1.9901008e-03, -1.6651758e-03],\n",
       "         [-9.2297345e-03, -3.2880137e-04, -1.5230951e-04,  3.4920995e-03,\n",
       "           2.5732553e-04, -5.7087867e-03,  2.2445649e-03,  7.0912676e-04,\n",
       "          -4.8072534e-03, -8.8230995e-03,  3.0337700e-03, -1.1867203e-02],\n",
       "         [-1.3108529e-02,  4.7278725e-05, -7.0289508e-03,  1.0129837e-03,\n",
       "          -7.5033524e-05,  9.3960902e-03, -1.6426500e-02, -4.9151556e-04,\n",
       "          -8.1951736e-04, -8.1173965e-04,  2.6278673e-03, -3.5986577e-03],\n",
       "         [ 6.2257721e-04, -8.6194574e-04,  1.3243833e-02,  1.4529747e-03,\n",
       "          -1.5440118e-03,  1.8917748e-03,  4.6863066e-04, -4.4693486e-03,\n",
       "          -2.5026713e-02, -5.0763707e-03,  4.3207859e-03, -1.2210831e-02],\n",
       "         [ 6.7084155e-04, -6.3671477e-02, -3.5748556e-02,  7.1068737e-04,\n",
       "          -1.6520357e-02, -3.9600432e-03,  1.6320454e-03,  1.2096318e-02,\n",
       "          -1.6208515e-02,  1.6589576e+00,  2.7684236e-02,  1.1686407e-03],\n",
       "         [-6.0156354e-04, -2.7392840e-01, -2.0487818e-03, -6.1719377e-02,\n",
       "          -3.7760239e-02,  7.6853661e-03,  8.0408510e-03,  3.8338110e-02,\n",
       "           1.4346809e-02,  3.5019312e-03,  1.5678790e-03, -7.2515797e-04],\n",
       "         [-3.3446055e-04,  8.4583037e-02, -6.8620965e-03, -3.0212728e-03,\n",
       "          -8.5793014e-05, -2.6111740e-03,  1.4252934e-02,  7.0772306e-03,\n",
       "          -1.7025821e-02,  4.0016202e-03,  2.5303031e-03, -4.8991244e-02],\n",
       "         [ 3.9406069e-02,  6.1500692e-03,  3.9909221e-02,  9.0570468e-03,\n",
       "          -1.0281484e-02, -1.8693054e-03,  2.2089181e-02,  3.5288946e-03,\n",
       "           5.4209498e-03, -1.1199784e-02,  3.8679659e-03, -5.1009320e-03]],\n",
       "        dtype=float32),\n",
       "  'ablation_targets': [(10, 7)],\n",
       "  'ablated_logit_diff': 2.5399458408355713,\n",
       "  'per_head_ablated_logit_diffs': array([[-2.0823958e-03,  2.5867738e-03,  5.2267761e-04, -4.6802289e-03,\n",
       "           1.6007047e-03, -2.7582503e-03, -4.4457396e-03,  8.6377403e-03,\n",
       "          -1.4312349e-03,  1.2539384e-03, -1.4292729e-03, -2.8229048e-03],\n",
       "         [-1.0031196e-02,  3.9439620e-03,  2.3167252e-03, -1.6789801e-03,\n",
       "          -1.4219059e-03, -5.6671835e-03,  4.8098578e-03,  1.2600280e-03,\n",
       "          -7.2644936e-04, -9.3485471e-03, -1.8021541e-03, -2.6482942e-03],\n",
       "         [-3.7112478e-03, -3.0432353e-03, -2.5033350e-03, -1.0333395e-02,\n",
       "          -4.2337044e-03,  5.9466378e-04, -8.5988985e-03,  3.4921372e-04,\n",
       "           1.1995236e-03,  3.1793134e-03, -2.6298056e-03, -1.3328518e-03],\n",
       "         [-5.3762374e-03, -2.2268791e-03, -2.9574607e-03,  5.2970723e-04,\n",
       "           9.6518807e-03,  4.0439315e-04,  1.3975416e-03, -8.8888971e-04,\n",
       "           7.3730852e-03,  6.5146754e-03, -5.9225885e-03, -1.7568448e-03],\n",
       "         [ 8.9085503e-03, -5.4025822e-03,  1.3122287e-03,  9.7176537e-04,\n",
       "           3.0761140e-03, -6.7550000e-03, -6.5296916e-03, -7.4025511e-04,\n",
       "           6.0076406e-03, -5.7063592e-03,  1.9901008e-03, -1.6651758e-03],\n",
       "         [-9.2297345e-03, -3.2880137e-04, -1.5230951e-04,  3.4920995e-03,\n",
       "           2.5732553e-04, -5.7087867e-03,  2.2445649e-03,  7.0912676e-04,\n",
       "          -4.8072534e-03, -8.8230995e-03,  3.0337700e-03, -1.1867203e-02],\n",
       "         [-1.3108529e-02,  4.7278725e-05, -7.0289508e-03,  1.0129837e-03,\n",
       "          -7.5033524e-05,  9.3960902e-03, -1.6426500e-02, -4.9151556e-04,\n",
       "          -8.1951736e-04, -8.1173965e-04,  2.6278673e-03, -3.5986577e-03],\n",
       "         [ 6.2257721e-04, -8.6194574e-04,  1.3243833e-02,  1.4529747e-03,\n",
       "          -1.5440118e-03,  1.8917748e-03,  4.6863066e-04, -4.4693486e-03,\n",
       "          -2.5026713e-02, -5.0763707e-03,  4.3207859e-03, -1.2210831e-02],\n",
       "         [ 6.7084155e-04, -6.3671477e-02, -3.5748556e-02,  7.1068737e-04,\n",
       "          -1.6520357e-02, -3.9600432e-03,  1.6320454e-03,  1.2096318e-02,\n",
       "          -1.6208515e-02,  1.6589576e+00,  2.7684236e-02,  1.1686407e-03],\n",
       "         [-6.0156354e-04, -2.7392840e-01, -2.0487818e-03, -6.1719377e-02,\n",
       "          -3.7760239e-02,  7.6853661e-03,  8.0408510e-03,  3.8338110e-02,\n",
       "           1.4346809e-02,  3.5019312e-03,  1.5678790e-03, -7.2515797e-04],\n",
       "         [-3.3446055e-04,  8.4583037e-02, -6.8620965e-03, -3.0212728e-03,\n",
       "          -8.5793014e-05, -2.6111740e-03,  1.4252934e-02,  0.0000000e+00,\n",
       "          -1.7025821e-02,  4.0016202e-03,  2.5303031e-03, -4.8991244e-02],\n",
       "         [ 4.1274801e-02,  6.2284810e-03,  3.8642231e-02,  9.1485279e-03,\n",
       "          -1.0139270e-02, -2.0407257e-03,  2.2131978e-02,  3.9716689e-03,\n",
       "           5.1935203e-03, -1.1155417e-02,  3.9531230e-03, -5.2075097e-03]],\n",
       "        dtype=float32),\n",
       "  'per_head_logit_diff_delta': array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 1.8687323e-03,  7.8411773e-05, -1.2669899e-03,  9.1481023e-05,\n",
       "           1.4221389e-04, -1.7142028e-04,  4.2796135e-05,  4.4277427e-04,\n",
       "          -2.2742944e-04,  4.4367276e-05,  8.5157109e-05, -1.0657776e-04]],\n",
       "        dtype=float32),\n",
       "  'in_circuit_head_delta': array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 1.8687323e-03,  0.0000000e+00, -1.2669899e-03,  9.1481023e-05,\n",
       "           0.0000000e+00,  0.0000000e+00,  4.2796135e-05,  4.4277427e-04,\n",
       "          -2.2742944e-04,  0.0000000e+00,  8.5157109e-05, -1.0657776e-04]],\n",
       "        dtype=float32),\n",
       "  'outside_circuit_head_delta': array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  7.8411773e-05,  0.0000000e+00,  0.0000000e+00,\n",
       "           1.4221389e-04, -1.7142028e-04,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  4.4367276e-05,  0.0000000e+00,  0.0000000e+00]],\n",
       "        dtype=float32),\n",
       "  'summed_in_circuit_head_delta': 0.0009299437515437603,\n",
       "  'summed_outside_circuit_head_delta': 9.357265662401915e-05,\n",
       "  'summed_total_head_delta': 0.0010235164081677794}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
