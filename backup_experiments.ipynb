{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as tl_utils\n",
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens.patching as patching\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.data_utils import generate_data_and_caches\n",
    "from utils.data_processing import (\n",
    "    load_edge_scores_into_dictionary,\n",
    "    read_json_file,\n",
    "    get_ckpts,\n",
    "    load_metrics,\n",
    "    compute_ged,\n",
    "    compute_weighted_ged,\n",
    "    compute_gtd,\n",
    "    compute_jaccard_similarity_to_reference,\n",
    "    compute_jaccard_similarity,\n",
    "    aggregate_metrics_to_tensors_step_number,\n",
    "    get_ckpts\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = 'ioi'\n",
    "PERFORMANCE_METRIC = 'logit_diff'\n",
    "BASE_MODEL = \"pythia-160m\"\n",
    "VARIANT = None\n",
    "CACHE = \"model_cache\"\n",
    "CHECKPOINT = 10000\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "IOI_DATASET_SIZE = 70\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_head_names_to_tuple(head_name):\n",
    "    head_name = head_name.replace('a', '')\n",
    "    head_name = head_name.replace('h', '')\n",
    "    layer, head = head_name.split('.')\n",
    "    return (int(layer), int(head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_copy_circuit(model, layer, head, ioi_dataset, verbose=False, neg=False):\n",
    "    \n",
    "    # get the activation cache for the first layer from IOI dataset\n",
    "    logits, cache = model.run_with_cache(ioi_dataset.toks.long())\n",
    "    \n",
    "    # sign adjustment, optional\n",
    "    if neg:\n",
    "        sign = -1\n",
    "    else:\n",
    "        sign = 1\n",
    "\n",
    "    # pass the activations through the first layernorm for block 1\n",
    "    #z_0 = model.blocks[1].attn.ln1(cache[\"blocks.0.hook_resid_post\"])\n",
    "    z_0 = cache[\"blocks.0.hook_resid_post\"]\n",
    "\n",
    "    # pass the activations through the attention weights (values) for the head\n",
    "    v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n",
    "    # add the bias\n",
    "    v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # pass the activations through the attention weights (output only) for the head\n",
    "    o = sign * torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n",
    "\n",
    "    # pass the activations through the final layernorm\n",
    "    logits = model.unembed(o)\n",
    "\n",
    "    k = 5\n",
    "    n_right = 0\n",
    "\n",
    "    for seq_idx, prompt in enumerate(ioi_dataset.ioi_prompts):\n",
    "        for word in [\"IO\", \"S1\", \"S2\"]:\n",
    "            pred_tokens = [\n",
    "                model.tokenizer.decode(token)\n",
    "                for token in torch.topk(\n",
    "                    logits[seq_idx, ioi_dataset.word_idx[word][seq_idx]], k\n",
    "                ).indices\n",
    "            ]\n",
    "            if \"S\" in word:\n",
    "                name = \"S\"\n",
    "            else:\n",
    "                name = word\n",
    "            if \" \" + prompt[name] in pred_tokens:\n",
    "                n_right += 1\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"-------\")\n",
    "                    print(\"Seq: \" + ioi_dataset.sentences[seq_idx])\n",
    "                    print(\"Target: \" + ioi_dataset.ioi_prompts[seq_idx][name])\n",
    "                    print(\n",
    "                        \" \".join(\n",
    "                            [\n",
    "                                f\"({i+1}):{model.tokenizer.decode(token)}\"\n",
    "                                for i, token in enumerate(\n",
    "                                    torch.topk(\n",
    "                                        logits[\n",
    "                                            seq_idx, ioi_dataset.word_idx[word][seq_idx]\n",
    "                                        ],\n",
    "                                        k,\n",
    "                                    ).indices\n",
    "                                )\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "    percent_right = (n_right / (ioi_dataset.N * 3)) * 100\n",
    "    print(\n",
    "        f\"Copy circuit for head {layer}.{head} (sign={sign}) : Top {k} accuracy: {percent_right}%\"\n",
    "    )\n",
    "    return percent_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(BASE_MODEL, VARIANT, CHECKPOINT, CACHE, device):\n",
    "    if not VARIANT:\n",
    "        model = HookedTransformer.from_pretrained(\n",
    "            BASE_MODEL,\n",
    "            checkpoint_value=CHECKPOINT,\n",
    "            center_unembed=True,\n",
    "            center_writing_weights=True,\n",
    "            fold_ln=True,\n",
    "            refactor_factored_attn_matrices=False,\n",
    "            dtype=torch.bfloat16,\n",
    "            **{\"cache_dir\": CACHE},\n",
    "        )\n",
    "    else:\n",
    "        revision = f\"step{CHECKPOINT}\"\n",
    "        source_model = AutoModelForCausalLM.from_pretrained(\n",
    "           VARIANT, revision=revision, cache_dir=CACHE\n",
    "        ).to(device).to(torch.bfloat16)\n",
    "\n",
    "        model = HookedTransformer.from_pretrained(\n",
    "            BASE_MODEL,\n",
    "            hf_model=source_model,\n",
    "            center_unembed=False,\n",
    "            center_writing_weights=False,\n",
    "            fold_ln=False,\n",
    "            dtype=torch.bfloat16,\n",
    "            **{\"cache_dir\": CACHE},\n",
    "        )\n",
    "\n",
    "    model.cfg.use_split_qkv_input = True\n",
    "    model.cfg.use_attn_result = True\n",
    "    model.cfg.use_hook_mlp_in = True\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve & Process Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/153: results/graphs/pythia-160m/ioi/57000.json\n",
      "Processing file 2/153: results/graphs/pythia-160m/ioi/141000.json\n",
      "Processing file 3/153: results/graphs/pythia-160m/ioi/95000.json\n",
      "Processing file 4/153: results/graphs/pythia-160m/ioi/107000.json\n",
      "Processing file 5/153: results/graphs/pythia-160m/ioi/34000.json\n",
      "Processing file 6/153: results/graphs/pythia-160m/ioi/6000.json\n",
      "Processing file 7/153: results/graphs/pythia-160m/ioi/37000.json\n",
      "Processing file 8/153: results/graphs/pythia-160m/ioi/39000.json\n",
      "Processing file 9/153: results/graphs/pythia-160m/ioi/104000.json\n",
      "Processing file 10/153: results/graphs/pythia-160m/ioi/59000.json\n",
      "Processing file 11/153: results/graphs/pythia-160m/ioi/67000.json\n",
      "Processing file 12/153: results/graphs/pythia-160m/ioi/111000.json\n",
      "Processing file 13/153: results/graphs/pythia-160m/ioi/16.json\n",
      "Processing file 14/153: results/graphs/pythia-160m/ioi/76000.json\n",
      "Processing file 15/153: results/graphs/pythia-160m/ioi/1.json\n",
      "Processing file 16/153: results/graphs/pythia-160m/ioi/5000.json\n",
      "Processing file 17/153: results/graphs/pythia-160m/ioi/42000.json\n",
      "Processing file 18/153: results/graphs/pythia-160m/ioi/77000.json\n",
      "Processing file 19/153: results/graphs/pythia-160m/ioi/86000.json\n",
      "Processing file 20/153: results/graphs/pythia-160m/ioi/80000.json\n",
      "Processing file 21/153: results/graphs/pythia-160m/ioi/81000.json\n",
      "Processing file 22/153: results/graphs/pythia-160m/ioi/63000.json\n",
      "Processing file 23/153: results/graphs/pythia-160m/ioi/142000.json\n",
      "Processing file 24/153: results/graphs/pythia-160m/ioi/56000.json\n",
      "Processing file 25/153: results/graphs/pythia-160m/ioi/8000.json\n",
      "Processing file 26/153: results/graphs/pythia-160m/ioi/93000.json\n",
      "Processing file 27/153: results/graphs/pythia-160m/ioi/120000.json\n",
      "Processing file 28/153: results/graphs/pythia-160m/ioi/62000.json\n",
      "Processing file 29/153: results/graphs/pythia-160m/ioi/70000.json\n",
      "Processing file 30/153: results/graphs/pythia-160m/ioi/19000.json\n",
      "Processing file 31/153: results/graphs/pythia-160m/ioi/121000.json\n",
      "Processing file 32/153: results/graphs/pythia-160m/ioi/105000.json\n",
      "Processing file 33/153: results/graphs/pythia-160m/ioi/129000.json\n",
      "Processing file 34/153: results/graphs/pythia-160m/ioi/2000.json\n",
      "Processing file 35/153: results/graphs/pythia-160m/ioi/96000.json\n",
      "Processing file 36/153: results/graphs/pythia-160m/ioi/124000.json\n",
      "Processing file 37/153: results/graphs/pythia-160m/ioi/143000.json\n",
      "Processing file 38/153: results/graphs/pythia-160m/ioi/79000.json\n",
      "Processing file 39/153: results/graphs/pythia-160m/ioi/29000.json\n",
      "Processing file 40/153: results/graphs/pythia-160m/ioi/137000.json\n",
      "Processing file 41/153: results/graphs/pythia-160m/ioi/10000.json\n",
      "Processing file 42/153: results/graphs/pythia-160m/ioi/135000.json\n",
      "Processing file 43/153: results/graphs/pythia-160m/ioi/65000.json\n",
      "Processing file 44/153: results/graphs/pythia-160m/ioi/60000.json\n",
      "Processing file 45/153: results/graphs/pythia-160m/ioi/90000.json\n",
      "Processing file 46/153: results/graphs/pythia-160m/ioi/106000.json\n",
      "Processing file 47/153: results/graphs/pythia-160m/ioi/1000.json\n",
      "Processing file 48/153: results/graphs/pythia-160m/ioi/33000.json\n",
      "Processing file 49/153: results/graphs/pythia-160m/ioi/103000.json\n",
      "Processing file 50/153: results/graphs/pythia-160m/ioi/113000.json\n",
      "Processing file 51/153: results/graphs/pythia-160m/ioi/35000.json\n",
      "Processing file 52/153: results/graphs/pythia-160m/ioi/133000.json\n",
      "Processing file 53/153: results/graphs/pythia-160m/ioi/18000.json\n",
      "Processing file 54/153: results/graphs/pythia-160m/ioi/4.json\n",
      "Processing file 55/153: results/graphs/pythia-160m/ioi/55000.json\n",
      "Processing file 56/153: results/graphs/pythia-160m/ioi/102000.json\n",
      "Processing file 57/153: results/graphs/pythia-160m/ioi/108000.json\n",
      "Processing file 58/153: results/graphs/pythia-160m/ioi/49000.json\n",
      "Processing file 59/153: results/graphs/pythia-160m/ioi/130000.json\n",
      "Processing file 60/153: results/graphs/pythia-160m/ioi/83000.json\n",
      "Processing file 61/153: results/graphs/pythia-160m/ioi/31000.json\n",
      "Processing file 62/153: results/graphs/pythia-160m/ioi/46000.json\n",
      "Processing file 63/153: results/graphs/pythia-160m/ioi/112000.json\n",
      "Processing file 64/153: results/graphs/pythia-160m/ioi/26000.json\n",
      "Processing file 65/153: results/graphs/pythia-160m/ioi/78000.json\n",
      "Processing file 66/153: results/graphs/pythia-160m/ioi/13000.json\n",
      "Processing file 67/153: results/graphs/pythia-160m/ioi/47000.json\n",
      "Processing file 68/153: results/graphs/pythia-160m/ioi/58000.json\n",
      "Processing file 69/153: results/graphs/pythia-160m/ioi/134000.json\n",
      "Processing file 70/153: results/graphs/pythia-160m/ioi/32.json\n",
      "Processing file 71/153: results/graphs/pythia-160m/ioi/128.json\n",
      "Processing file 72/153: results/graphs/pythia-160m/ioi/100000.json\n",
      "Processing file 73/153: results/graphs/pythia-160m/ioi/138000.json\n",
      "Processing file 74/153: results/graphs/pythia-160m/ioi/27000.json\n",
      "Processing file 75/153: results/graphs/pythia-160m/ioi/48000.json\n",
      "Processing file 76/153: results/graphs/pythia-160m/ioi/91000.json\n",
      "Processing file 77/153: results/graphs/pythia-160m/ioi/122000.json\n",
      "Processing file 78/153: results/graphs/pythia-160m/ioi/99000.json\n",
      "Processing file 79/153: results/graphs/pythia-160m/ioi/32000.json\n",
      "Processing file 80/153: results/graphs/pythia-160m/ioi/30000.json\n",
      "Processing file 81/153: results/graphs/pythia-160m/ioi/44000.json\n",
      "Processing file 82/153: results/graphs/pythia-160m/ioi/136000.json\n",
      "Processing file 83/153: results/graphs/pythia-160m/ioi/116000.json\n",
      "Processing file 84/153: results/graphs/pythia-160m/ioi/74000.json\n",
      "Processing file 85/153: results/graphs/pythia-160m/ioi/118000.json\n",
      "Processing file 86/153: results/graphs/pythia-160m/ioi/94000.json\n",
      "Processing file 87/153: results/graphs/pythia-160m/ioi/119000.json\n",
      "Processing file 88/153: results/graphs/pythia-160m/ioi/64000.json\n",
      "Processing file 89/153: results/graphs/pythia-160m/ioi/2.json\n",
      "Processing file 90/153: results/graphs/pythia-160m/ioi/69000.json\n",
      "Processing file 91/153: results/graphs/pythia-160m/ioi/140000.json\n",
      "Processing file 92/153: results/graphs/pythia-160m/ioi/72000.json\n",
      "Processing file 93/153: results/graphs/pythia-160m/ioi/117000.json\n",
      "Processing file 94/153: results/graphs/pythia-160m/ioi/9000.json\n",
      "Processing file 95/153: results/graphs/pythia-160m/ioi/98000.json\n",
      "Processing file 96/153: results/graphs/pythia-160m/ioi/110000.json\n",
      "Processing file 97/153: results/graphs/pythia-160m/ioi/88000.json\n",
      "Processing file 98/153: results/graphs/pythia-160m/ioi/16000.json\n",
      "Processing file 99/153: results/graphs/pythia-160m/ioi/23000.json\n",
      "Processing file 100/153: results/graphs/pythia-160m/ioi/109000.json\n",
      "Processing file 101/153: results/graphs/pythia-160m/ioi/22000.json\n",
      "Processing file 102/153: results/graphs/pythia-160m/ioi/17000.json\n",
      "Processing file 103/153: results/graphs/pythia-160m/ioi/73000.json\n",
      "Processing file 104/153: results/graphs/pythia-160m/ioi/3000.json\n",
      "Processing file 105/153: results/graphs/pythia-160m/ioi/71000.json\n",
      "Processing file 106/153: results/graphs/pythia-160m/ioi/125000.json\n",
      "Processing file 107/153: results/graphs/pythia-160m/ioi/84000.json\n",
      "Processing file 108/153: results/graphs/pythia-160m/ioi/87000.json\n",
      "Processing file 109/153: results/graphs/pythia-160m/ioi/131000.json\n",
      "Processing file 110/153: results/graphs/pythia-160m/ioi/24000.json\n",
      "Processing file 111/153: results/graphs/pythia-160m/ioi/128000.json\n",
      "Processing file 112/153: results/graphs/pythia-160m/ioi/51000.json\n",
      "Processing file 113/153: results/graphs/pythia-160m/ioi/52000.json\n",
      "Processing file 114/153: results/graphs/pythia-160m/ioi/132000.json\n",
      "Processing file 115/153: results/graphs/pythia-160m/ioi/101000.json\n",
      "Processing file 116/153: results/graphs/pythia-160m/ioi/89000.json\n",
      "Processing file 117/153: results/graphs/pythia-160m/ioi/40000.json\n",
      "Processing file 118/153: results/graphs/pythia-160m/ioi/8.json\n",
      "Processing file 119/153: results/graphs/pythia-160m/ioi/43000.json\n",
      "Processing file 120/153: results/graphs/pythia-160m/ioi/54000.json\n",
      "Processing file 121/153: results/graphs/pythia-160m/ioi/11000.json\n",
      "Processing file 122/153: results/graphs/pythia-160m/ioi/36000.json\n",
      "Processing file 123/153: results/graphs/pythia-160m/ioi/45000.json\n",
      "Processing file 124/153: results/graphs/pythia-160m/ioi/50000.json\n",
      "Processing file 125/153: results/graphs/pythia-160m/ioi/114000.json\n",
      "Processing file 126/153: results/graphs/pythia-160m/ioi/64.json\n",
      "Processing file 127/153: results/graphs/pythia-160m/ioi/66000.json\n",
      "Processing file 128/153: results/graphs/pythia-160m/ioi/28000.json\n",
      "Processing file 129/153: results/graphs/pythia-160m/ioi/82000.json\n",
      "Processing file 130/153: results/graphs/pythia-160m/ioi/127000.json\n",
      "Processing file 131/153: results/graphs/pythia-160m/ioi/14000.json\n",
      "Processing file 132/153: results/graphs/pythia-160m/ioi/75000.json\n",
      "Processing file 133/153: results/graphs/pythia-160m/ioi/115000.json\n",
      "Processing file 134/153: results/graphs/pythia-160m/ioi/12000.json\n",
      "Processing file 135/153: results/graphs/pythia-160m/ioi/256.json\n",
      "Processing file 136/153: results/graphs/pythia-160m/ioi/21000.json\n",
      "Processing file 137/153: results/graphs/pythia-160m/ioi/38000.json\n",
      "Processing file 138/153: results/graphs/pythia-160m/ioi/15000.json\n",
      "Processing file 139/153: results/graphs/pythia-160m/ioi/85000.json\n",
      "Processing file 140/153: results/graphs/pythia-160m/ioi/139000.json\n",
      "Processing file 141/153: results/graphs/pythia-160m/ioi/61000.json\n",
      "Processing file 142/153: results/graphs/pythia-160m/ioi/7000.json\n",
      "Processing file 143/153: results/graphs/pythia-160m/ioi/512.json\n",
      "Processing file 144/153: results/graphs/pythia-160m/ioi/68000.json\n",
      "Processing file 145/153: results/graphs/pythia-160m/ioi/53000.json\n",
      "Processing file 146/153: results/graphs/pythia-160m/ioi/25000.json\n",
      "Processing file 147/153: results/graphs/pythia-160m/ioi/97000.json\n",
      "Processing file 148/153: results/graphs/pythia-160m/ioi/4000.json\n",
      "Processing file 149/153: results/graphs/pythia-160m/ioi/123000.json\n",
      "Processing file 150/153: results/graphs/pythia-160m/ioi/20000.json\n",
      "Processing file 151/153: results/graphs/pythia-160m/ioi/41000.json\n",
      "Processing file 152/153: results/graphs/pythia-160m/ioi/126000.json\n",
      "Processing file 153/153: results/graphs/pythia-160m/ioi/92000.json\n"
     ]
    }
   ],
   "source": [
    "folder_path = f'results/graphs/pythia-160m/{TASK}'\n",
    "df = load_edge_scores_into_dictionary(folder_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory_path = 'results'\n",
    "perf_metrics = load_metrics(directory_path)\n",
    "\n",
    "ckpts = get_ckpts(schedule=\"exp_plus_detail\")\n",
    "#pythia_evals = aggregate_metrics_to_tensors_step_number(\"results/pythia-evals/pythia-v1\")\n",
    "\n",
    "# filter everything before 1000 steps\n",
    "df = df[df['checkpoint'] >= 1000]\n",
    "\n",
    "df[['source', 'target']] = df['edge'].str.split('->', expand=True)\n",
    "len(df['target'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_metric = perf_metrics['pythia-160m'][TASK][PERFORMANCE_METRIC]\n",
    "\n",
    "perf_metric = [x.item() for x in perf_metric]\n",
    "\n",
    "# zip into dictionary with ckpts as key\n",
    "perf_metric_dict = dict(zip(ckpts, perf_metric))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "initial_model = load_model(BASE_MODEL, VARIANT, 143000, CACHE, device)\n",
    "size=70\n",
    "ioi_dataset, abc_dataset = generate_data_and_caches(initial_model, size, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Experimental Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTAL_CHECKPOINT = 80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "experimental_model = load_model(BASE_MODEL, VARIANT, EXPERIMENTAL_CHECKPOINT, CACHE, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_nmh = df[df['target']=='logits']\n",
    "candidate_nmh = candidate_nmh[candidate_nmh['in_circuit'] == True]\n",
    "\n",
    "candidate_list = candidate_nmh[candidate_nmh['checkpoint']==EXPERIMENTAL_CHECKPOINT]['source'].unique().tolist()\n",
    "candidate_list = [convert_head_names_to_tuple(c) for c in candidate_list if (c[0] != 'm' and c != 'input')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 20.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.9 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 22.380952380952383%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 45.714285714285715%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 13.80952380952381%\n",
      "Copy circuit for head 11.10 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 70.47619047619048%\n",
      "Copy circuit for head 10.1 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.476190476190476%\n",
      "Copy circuit for head 7.2 (sign=1) : Top 5 accuracy: 0.0%\n",
      "Copy circuit for head 6.5 (sign=1) : Top 5 accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "for layer, head in candidate_list:\n",
    "    copy_score = check_copy_circuit(experimental_model, layer, head, ioi_dataset, verbose=False, neg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
