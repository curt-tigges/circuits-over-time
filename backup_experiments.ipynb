{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import re\n",
    "import einops\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from torch import Tensor\n",
    "from torchtyping import TensorType as TT\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from utils.data_utils import generate_data_and_caches\n",
    "from utils.data_processing import (\n",
    "    load_edge_scores_into_dictionary,\n",
    ")\n",
    "from utils.visualization import plot_attention_heads, imshow_p\n",
    "from utils.backup_analysis import (\n",
    "    load_model,\n",
    "    run_iteration,\n",
    "    process_backup_results,\n",
    "    get_past_nmhs_for_checkpoints,\n",
    "    plot_top_heads\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f854ef7ab30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = 'ioi'\n",
    "PERFORMANCE_METRIC = 'logit_diff'\n",
    "BASE_MODEL = \"pythia-160m\"\n",
    "VARIANT = \"EleutherAI/pythia-160m-weight-seed3\"\n",
    "MODEL_SHORTNAME = BASE_MODEL if not VARIANT else VARIANT[11:]\n",
    "CACHE = \"model_cache\"\n",
    "IOI_DATASET_SIZE = 70\n",
    "COPY_SCORE_THRESHOLD = 75.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/143: results/graphs/pythia-160m-weight-seed3/ioi/57000.json\n",
      "Processing file 2/143: results/graphs/pythia-160m-weight-seed3/ioi/141000.json\n",
      "Processing file 3/143: results/graphs/pythia-160m-weight-seed3/ioi/95000.json\n",
      "Processing file 4/143: results/graphs/pythia-160m-weight-seed3/ioi/107000.json\n",
      "Processing file 5/143: results/graphs/pythia-160m-weight-seed3/ioi/34000.json\n",
      "Processing file 6/143: results/graphs/pythia-160m-weight-seed3/ioi/6000.json\n",
      "Processing file 7/143: results/graphs/pythia-160m-weight-seed3/ioi/37000.json\n",
      "Processing file 8/143: results/graphs/pythia-160m-weight-seed3/ioi/39000.json\n",
      "Processing file 9/143: results/graphs/pythia-160m-weight-seed3/ioi/104000.json\n",
      "Processing file 10/143: results/graphs/pythia-160m-weight-seed3/ioi/59000.json\n",
      "Processing file 11/143: results/graphs/pythia-160m-weight-seed3/ioi/67000.json\n",
      "Processing file 12/143: results/graphs/pythia-160m-weight-seed3/ioi/111000.json\n",
      "Processing file 13/143: results/graphs/pythia-160m-weight-seed3/ioi/76000.json\n",
      "Processing file 14/143: results/graphs/pythia-160m-weight-seed3/ioi/5000.json\n",
      "Processing file 15/143: results/graphs/pythia-160m-weight-seed3/ioi/42000.json\n",
      "Processing file 16/143: results/graphs/pythia-160m-weight-seed3/ioi/77000.json\n",
      "Processing file 17/143: results/graphs/pythia-160m-weight-seed3/ioi/86000.json\n",
      "Processing file 18/143: results/graphs/pythia-160m-weight-seed3/ioi/80000.json\n",
      "Processing file 19/143: results/graphs/pythia-160m-weight-seed3/ioi/81000.json\n",
      "Processing file 20/143: results/graphs/pythia-160m-weight-seed3/ioi/63000.json\n",
      "Processing file 21/143: results/graphs/pythia-160m-weight-seed3/ioi/142000.json\n",
      "Processing file 22/143: results/graphs/pythia-160m-weight-seed3/ioi/56000.json\n",
      "Processing file 23/143: results/graphs/pythia-160m-weight-seed3/ioi/8000.json\n",
      "Processing file 24/143: results/graphs/pythia-160m-weight-seed3/ioi/93000.json\n",
      "Processing file 25/143: results/graphs/pythia-160m-weight-seed3/ioi/120000.json\n",
      "Processing file 26/143: results/graphs/pythia-160m-weight-seed3/ioi/62000.json\n",
      "Processing file 27/143: results/graphs/pythia-160m-weight-seed3/ioi/70000.json\n",
      "Processing file 28/143: results/graphs/pythia-160m-weight-seed3/ioi/19000.json\n",
      "Processing file 29/143: results/graphs/pythia-160m-weight-seed3/ioi/121000.json\n",
      "Processing file 30/143: results/graphs/pythia-160m-weight-seed3/ioi/105000.json\n",
      "Processing file 31/143: results/graphs/pythia-160m-weight-seed3/ioi/129000.json\n",
      "Processing file 32/143: results/graphs/pythia-160m-weight-seed3/ioi/2000.json\n",
      "Processing file 33/143: results/graphs/pythia-160m-weight-seed3/ioi/96000.json\n",
      "Processing file 34/143: results/graphs/pythia-160m-weight-seed3/ioi/124000.json\n",
      "Processing file 35/143: results/graphs/pythia-160m-weight-seed3/ioi/143000.json\n",
      "Processing file 36/143: results/graphs/pythia-160m-weight-seed3/ioi/79000.json\n",
      "Processing file 37/143: results/graphs/pythia-160m-weight-seed3/ioi/29000.json\n",
      "Processing file 38/143: results/graphs/pythia-160m-weight-seed3/ioi/137000.json\n",
      "Processing file 39/143: results/graphs/pythia-160m-weight-seed3/ioi/10000.json\n",
      "Processing file 40/143: results/graphs/pythia-160m-weight-seed3/ioi/135000.json\n",
      "Processing file 41/143: results/graphs/pythia-160m-weight-seed3/ioi/65000.json\n",
      "Processing file 42/143: results/graphs/pythia-160m-weight-seed3/ioi/60000.json\n",
      "Processing file 43/143: results/graphs/pythia-160m-weight-seed3/ioi/90000.json\n",
      "Processing file 44/143: results/graphs/pythia-160m-weight-seed3/ioi/106000.json\n",
      "Processing file 45/143: results/graphs/pythia-160m-weight-seed3/ioi/1000.json\n",
      "Processing file 46/143: results/graphs/pythia-160m-weight-seed3/ioi/33000.json\n",
      "Processing file 47/143: results/graphs/pythia-160m-weight-seed3/ioi/103000.json\n",
      "Processing file 48/143: results/graphs/pythia-160m-weight-seed3/ioi/113000.json\n",
      "Processing file 49/143: results/graphs/pythia-160m-weight-seed3/ioi/35000.json\n",
      "Processing file 50/143: results/graphs/pythia-160m-weight-seed3/ioi/133000.json\n",
      "Processing file 51/143: results/graphs/pythia-160m-weight-seed3/ioi/18000.json\n",
      "Processing file 52/143: results/graphs/pythia-160m-weight-seed3/ioi/55000.json\n",
      "Processing file 53/143: results/graphs/pythia-160m-weight-seed3/ioi/102000.json\n",
      "Processing file 54/143: results/graphs/pythia-160m-weight-seed3/ioi/108000.json\n",
      "Processing file 55/143: results/graphs/pythia-160m-weight-seed3/ioi/49000.json\n",
      "Processing file 56/143: results/graphs/pythia-160m-weight-seed3/ioi/130000.json\n",
      "Processing file 57/143: results/graphs/pythia-160m-weight-seed3/ioi/83000.json\n",
      "Processing file 58/143: results/graphs/pythia-160m-weight-seed3/ioi/31000.json\n",
      "Processing file 59/143: results/graphs/pythia-160m-weight-seed3/ioi/46000.json\n",
      "Processing file 60/143: results/graphs/pythia-160m-weight-seed3/ioi/112000.json\n",
      "Processing file 61/143: results/graphs/pythia-160m-weight-seed3/ioi/26000.json\n",
      "Processing file 62/143: results/graphs/pythia-160m-weight-seed3/ioi/78000.json\n",
      "Processing file 63/143: results/graphs/pythia-160m-weight-seed3/ioi/13000.json\n",
      "Processing file 64/143: results/graphs/pythia-160m-weight-seed3/ioi/47000.json\n",
      "Processing file 65/143: results/graphs/pythia-160m-weight-seed3/ioi/58000.json\n",
      "Processing file 66/143: results/graphs/pythia-160m-weight-seed3/ioi/134000.json\n",
      "Processing file 67/143: results/graphs/pythia-160m-weight-seed3/ioi/100000.json\n",
      "Processing file 68/143: results/graphs/pythia-160m-weight-seed3/ioi/138000.json\n",
      "Processing file 69/143: results/graphs/pythia-160m-weight-seed3/ioi/27000.json\n",
      "Processing file 70/143: results/graphs/pythia-160m-weight-seed3/ioi/48000.json\n",
      "Processing file 71/143: results/graphs/pythia-160m-weight-seed3/ioi/91000.json\n",
      "Processing file 72/143: results/graphs/pythia-160m-weight-seed3/ioi/122000.json\n",
      "Processing file 73/143: results/graphs/pythia-160m-weight-seed3/ioi/99000.json\n",
      "Processing file 74/143: results/graphs/pythia-160m-weight-seed3/ioi/32000.json\n",
      "Processing file 75/143: results/graphs/pythia-160m-weight-seed3/ioi/30000.json\n",
      "Processing file 76/143: results/graphs/pythia-160m-weight-seed3/ioi/44000.json\n",
      "Processing file 77/143: results/graphs/pythia-160m-weight-seed3/ioi/136000.json\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m folder_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mresults/graphs/\u001b[39m\u001b[39m{\u001b[39;00mMODEL_SHORTNAME\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mTASK\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m load_edge_scores_into_dictionary(folder_path)\n\u001b[1;32m      4\u001b[0m \u001b[39m# filter everything before 1000 steps\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39mcheckpoint\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m]\n",
      "File \u001b[0;32m/notebooks/utils/data_processing.py:32\u001b[0m, in \u001b[0;36mload_edge_scores_into_dictionary\u001b[0;34m(folder_path, checkpoint)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mProcessing file \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(file_paths)\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m data \u001b[39m=\u001b[39m read_json_file(file_path)\n\u001b[1;32m     33\u001b[0m edges \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39medges\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     34\u001b[0m scores \u001b[39m=\u001b[39m [edge[\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m edge \u001b[39min\u001b[39;00m edges\u001b[39m.\u001b[39mvalues()]\n",
      "File \u001b[0;32m/notebooks/utils/data_processing.py:16\u001b[0m, in \u001b[0;36mread_json_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_json_file\u001b[39m(file_path):\n\u001b[1;32m     15\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_path, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m---> 16\u001b[0m         \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39;49mload(file)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[1;32m    294\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m, object_hook\u001b[39m=\u001b[39;49mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39;49mparse_float, parse_int\u001b[39m=\u001b[39;49mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39;49mparse_constant, object_pairs_hook\u001b[39m=\u001b[39;49mobject_pairs_hook, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "folder_path = f'results/graphs/{MODEL_SHORTNAME}/{TASK}'\n",
    "df = load_edge_scores_into_dictionary(folder_path)\n",
    "\n",
    "# filter everything before 1000 steps\n",
    "df = df[df['checkpoint'] >= 1000]\n",
    "\n",
    "df[['source', 'target']] = df['edge'].str.split('->', expand=True)\n",
    "len(df['target'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a9e98872f0462b8f2ac40f18a70c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/598 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4879db09610940b595d3fc0bc4eb67b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6bdee3ba5b48ae99e03bbc7843a76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step143000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "initial_model = load_model(BASE_MODEL, VARIANT, 143000, CACHE, device)\n",
    "size=70\n",
    "ioi_dataset, abc_dataset = generate_data_and_caches(initial_model, size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imshow_p(\n",
    "#     per_head_ablated_logit_diffs,\n",
    "#     title=\"Headwise logit diff contribution, post NMH KO\",\n",
    "#     labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Logit diff attribution\"},\n",
    "#     #coloraxis=dict(colorbar_ticksuffix = \"%\"),\n",
    "#     border=True,\n",
    "#     width=600,\n",
    "#     margin={\"r\": 100, \"l\": 100}\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step4000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Checkpoint 4000:\n",
      "Heads ablated:            [(8, 1)]\n",
      "Original logit diff:      0.8774140477\n",
      "Post ablation logit diff: 1.0147744417\n",
      "Logit diff % change:      15.66%\n",
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step5000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 47.14285714285714%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 5000:\n",
      "Heads ablated:            [(8, 1), (8, 10)]\n",
      "Original logit diff:      1.5074634552\n",
      "Post ablation logit diff: 1.4938864708\n",
      "Logit diff % change:      -0.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step6000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 73.33333333333333%\n",
      "Checkpoint 6000:\n",
      "Heads ablated:            [(8, 10), (8, 1)]\n",
      "Original logit diff:      2.0983071327\n",
      "Post ablation logit diff: 1.9507633448\n",
      "Logit diff % change:      -7.03%\n",
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step7000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 80.95238095238095%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Checkpoint 7000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 10)]\n",
      "Original logit diff:      2.3801794052\n",
      "Post ablation logit diff: 2.4812474251\n",
      "Logit diff % change:      4.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step8000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 22.857142857142858%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Checkpoint 8000:\n",
      "Heads ablated:            [(8, 10), (8, 1), (10, 7)]\n",
      "Original logit diff:      2.5609090328\n",
      "Post ablation logit diff: 2.4754891396\n",
      "Logit diff % change:      -3.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step9000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Checkpoint 9000:\n",
      "Heads ablated:            [(8, 10), (8, 1)]\n",
      "Original logit diff:      2.4609885216\n",
      "Post ablation logit diff: 2.2680990696\n",
      "Logit diff % change:      -7.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step10000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Copy circuit for head 11.8 (sign=1) : Top 5 accuracy: 59.04761904761905%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 13.333333333333334%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 25.71428571428571%\n",
      "Checkpoint 10000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.1183340549\n",
      "Post ablation logit diff: 2.7587451935\n",
      "Logit diff % change:      -11.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step11000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 15.238095238095239%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 11000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.1467432976\n",
      "Post ablation logit diff: 2.9557740688\n",
      "Logit diff % change:      -6.07%\n",
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step12000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.9047619047619%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 13.80952380952381%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 12000:\n",
      "Heads ablated:            [(8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.0244793892\n",
      "Post ablation logit diff: 2.6767439842\n",
      "Logit diff % change:      -11.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step13000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.9047619047619%\n",
      "Copy circuit for head 11.8 (sign=1) : Top 5 accuracy: 44.761904761904766%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 46.666666666666664%\n",
      "Checkpoint 13000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.1469430923\n",
      "Post ablation logit diff: 2.8826324940\n",
      "Logit diff % change:      -8.40%\n",
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step14000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.857142857142854%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 90.47619047619048%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 14.761904761904763%\n",
      "Checkpoint 14000:\n",
      "Heads ablated:            [(8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      2.7993061543\n",
      "Post ablation logit diff: 3.1556169987\n",
      "Logit diff % change:      12.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step15000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 36.19047619047619%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 90.47619047619048%\n",
      "Checkpoint 15000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.9921891689\n",
      "Post ablation logit diff: 3.5350375175\n",
      "Logit diff % change:      -11.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step16000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 13.333333333333334%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 35.23809523809524%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.142857142857142%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 50.95238095238095%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 20.476190476190474%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 16000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.4396693707\n",
      "Post ablation logit diff: 3.2975933552\n",
      "Logit diff % change:      -4.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step17000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 36.666666666666664%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 50.95238095238095%\n",
      "Checkpoint 17000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.7224340439\n",
      "Post ablation logit diff: 3.6142177582\n",
      "Logit diff % change:      -2.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step18000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 62.38095238095238%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 12.857142857142856%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 37.142857142857146%\n",
      "Checkpoint 18000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.7128663063\n",
      "Post ablation logit diff: 3.6329803467\n",
      "Logit diff % change:      -2.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step19000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 16.666666666666664%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 60.952380952380956%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 34.285714285714285%\n",
      "Checkpoint 19000:\n",
      "Heads ablated:            [(8, 10), (8, 2)]\n",
      "Original logit diff:      3.2806375027\n",
      "Post ablation logit diff: 3.1564421654\n",
      "Logit diff % change:      -3.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step20000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 35.23809523809524%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 59.04761904761905%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 16.666666666666664%\n",
      "Checkpoint 20000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      4.5556230545\n",
      "Post ablation logit diff: 4.4376325607\n",
      "Logit diff % change:      -2.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step21000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 33.80952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 18.571428571428573%\n",
      "Checkpoint 21000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      2.7181048393\n",
      "Post ablation logit diff: 2.6167707443\n",
      "Logit diff % change:      -3.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step22000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.38095238095238%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 9.523809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 64.76190476190476%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 18.571428571428573%\n",
      "Checkpoint 22000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.5818908215\n",
      "Post ablation logit diff: 3.7463593483\n",
      "Logit diff % change:      4.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step23000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.476190476190478%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 53.80952380952381%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 18.095238095238095%\n",
      "Checkpoint 23000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      2.8724248409\n",
      "Post ablation logit diff: 2.8956525326\n",
      "Logit diff % change:      0.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step24000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 57.14285714285714%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 46.666666666666664%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.38095238095238%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Checkpoint 24000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.8160548210\n",
      "Post ablation logit diff: 4.0264568329\n",
      "Logit diff % change:      5.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step25000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 48.095238095238095%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 61.42857142857143%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Checkpoint 25000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.6006650925\n",
      "Post ablation logit diff: 3.9161517620\n",
      "Logit diff % change:      8.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step26000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 16.19047619047619%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.9047619047619%\n",
      "Checkpoint 26000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.5316271782\n",
      "Post ablation logit diff: 3.5185561180\n",
      "Logit diff % change:      -0.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step27000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 60.952380952380956%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 58.0952380952381%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 29.523809523809526%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Checkpoint 27000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.1346387863\n",
      "Post ablation logit diff: 3.4264886379\n",
      "Logit diff % change:      9.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step28000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 62.857142857142854%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 11.904761904761903%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.142857142857142%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 55.23809523809524%\n",
      "Checkpoint 28000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.2898459435\n",
      "Post ablation logit diff: 3.7952227592\n",
      "Logit diff % change:      15.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step29000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.9047619047619%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.476190476190476%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 60.952380952380956%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 54.761904761904766%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 14.761904761904763%\n",
      "Checkpoint 29000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      4.0918493271\n",
      "Post ablation logit diff: 3.7849409580\n",
      "Logit diff % change:      -7.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step30000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 15.238095238095239%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 58.57142857142858%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 61.904761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Checkpoint 30000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.6829319000\n",
      "Post ablation logit diff: 3.5467934608\n",
      "Logit diff % change:      -3.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step31000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.476190476190478%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.4 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 60.0%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.476190476190476%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 68.0952380952381%\n",
      "Checkpoint 31000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.5374736786\n",
      "Post ablation logit diff: 3.4194324017\n",
      "Logit diff % change:      -3.34%\n",
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step32000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.38095238095238%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 68.0952380952381%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 56.666666666666664%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 18.095238095238095%\n",
      "Checkpoint 32000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.5337259769\n",
      "Post ablation logit diff: 3.4055581093\n",
      "Logit diff % change:      -3.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step33000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 57.14285714285714%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 73.80952380952381%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 16.19047619047619%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.476190476190478%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Checkpoint 33000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.4183170795\n",
      "Post ablation logit diff: 3.2724177837\n",
      "Logit diff % change:      -4.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step34000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 59.523809523809526%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 71.9047619047619%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 16.19047619047619%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.476190476190478%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 90.47619047619048%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Checkpoint 34000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.5629084110\n",
      "Post ablation logit diff: 3.5525166988\n",
      "Logit diff % change:      -0.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step35000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.38095238095238%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 11.904761904761903%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.4 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 73.80952380952381%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 60.476190476190474%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Checkpoint 35000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.9811048508\n",
      "Post ablation logit diff: 3.7303841114\n",
      "Logit diff % change:      -6.30%\n",
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step36000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 61.42857142857143%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 73.80952380952381%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Checkpoint 36000:\n",
      "Heads ablated:            [(8, 10), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.7905578613\n",
      "Post ablation logit diff: 3.8760559559\n",
      "Logit diff % change:      2.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step37000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.9047619047619%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 71.9047619047619%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 60.952380952380956%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Checkpoint 37000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 10)]\n",
      "Original logit diff:      3.7472186089\n",
      "Post ablation logit diff: 3.4548361301\n",
      "Logit diff % change:      -7.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step38000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.857142857142854%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 60.476190476190474%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Checkpoint 38000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.5956947803\n",
      "Post ablation logit diff: 3.7887258530\n",
      "Logit diff % change:      5.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step39000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 16.19047619047619%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 63.33333333333333%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Checkpoint 39000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.0010337830\n",
      "Post ablation logit diff: 2.8249998093\n",
      "Logit diff % change:      -5.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step40000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 80.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 65.23809523809524%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.142857142857142%\n",
      "Checkpoint 40000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.5532555580\n",
      "Post ablation logit diff: 3.6005601883\n",
      "Logit diff % change:      1.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step41000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.38095238095238%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 77.14285714285715%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 59.04761904761905%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 18.571428571428573%\n",
      "Checkpoint 41000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.3553140163\n",
      "Post ablation logit diff: 3.4130187035\n",
      "Logit diff % change:      1.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step42000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.476190476190478%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 80.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 56.666666666666664%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 16.19047619047619%\n",
      "Checkpoint 42000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.3687191010\n",
      "Post ablation logit diff: 3.4951341152\n",
      "Logit diff % change:      3.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step43000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.9047619047619%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 65.71428571428571%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 16.666666666666664%\n",
      "Checkpoint 43000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.4419305325\n",
      "Post ablation logit diff: 3.3946907520\n",
      "Logit diff % change:      -1.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step44000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 18.095238095238095%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 64.76190476190476%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.9047619047619%\n",
      "Checkpoint 44000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.4195830822\n",
      "Post ablation logit diff: 3.4148421288\n",
      "Logit diff % change:      -0.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step45000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.8 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.9047619047619%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 61.904761904761905%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Checkpoint 45000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.4556510448\n",
      "Post ablation logit diff: 3.3162891865\n",
      "Logit diff % change:      -4.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step46000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 59.04761904761905%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Checkpoint 46000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      4.0069127083\n",
      "Post ablation logit diff: 3.7794783115\n",
      "Logit diff % change:      -5.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step47000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.857142857142854%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 18.095238095238095%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 58.0952380952381%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 47000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 8), (8, 10), (8, 2)]\n",
      "Original logit diff:      3.2646117210\n",
      "Post ablation logit diff: 3.1660871506\n",
      "Logit diff % change:      -3.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step48000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 57.14285714285714%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.857142857142854%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Checkpoint 48000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.1063001156\n",
      "Post ablation logit diff: 3.1150753498\n",
      "Logit diff % change:      0.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step49000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 33.80952380952381%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 51.90476190476191%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.142857142857142%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Checkpoint 49000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 10), (8, 8)]\n",
      "Original logit diff:      3.0458719730\n",
      "Post ablation logit diff: 2.8686249256\n",
      "Logit diff % change:      -5.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step50000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 57.61904761904761%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 18.095238095238095%\n",
      "Checkpoint 50000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      2.8237726688\n",
      "Post ablation logit diff: 2.7196214199\n",
      "Logit diff % change:      -3.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step51000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 48.57142857142857%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 90.47619047619048%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 20.476190476190474%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.9047619047619%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.8 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Checkpoint 51000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      2.7472066879\n",
      "Post ablation logit diff: 2.8495392799\n",
      "Logit diff % change:      3.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step52000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 50.0%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 21.904761904761905%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.9047619047619%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 6.11 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Checkpoint 52000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.3105905056\n",
      "Post ablation logit diff: 3.2157502174\n",
      "Logit diff % change:      -2.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step53000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 42.857142857142854%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 20.952380952380953%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Checkpoint 53000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.2673645020\n",
      "Post ablation logit diff: 3.2220690250\n",
      "Logit diff % change:      -1.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step54000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 33.33333333333333%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 92.38095238095238%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 50.476190476190474%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Checkpoint 54000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.2507045269\n",
      "Post ablation logit diff: 3.1833205223\n",
      "Logit diff % change:      -2.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step55000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 46.666666666666664%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Checkpoint 55000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      4.1535263062\n",
      "Post ablation logit diff: 4.0898976326\n",
      "Logit diff % change:      -1.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step56000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 10.8 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 32.857142857142854%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 9.523809523809524%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 90.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 47.14285714285714%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.142857142857142%\n",
      "Checkpoint 56000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.1213386059\n",
      "Post ablation logit diff: 3.0780556202\n",
      "Logit diff % change:      -1.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step57000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 33.33333333333333%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.095238095238095%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 46.666666666666664%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 20.476190476190474%\n",
      "Checkpoint 57000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      2.8012292385\n",
      "Post ablation logit diff: 2.6785862446\n",
      "Logit diff % change:      -4.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step58000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.142857142857142%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 45.714285714285715%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 10.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.952380952380953%\n",
      "Checkpoint 58000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      2.8963887691\n",
      "Post ablation logit diff: 2.9113495350\n",
      "Logit diff % change:      0.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step59000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 10.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.476190476190478%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 45.714285714285715%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 59000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.4959895611\n",
      "Post ablation logit diff: 3.4198267460\n",
      "Logit diff % change:      -2.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step60000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 16.666666666666664%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 46.19047619047619%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 93.80952380952381%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.476190476190478%\n",
      "Checkpoint 60000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.1936192513\n",
      "Post ablation logit diff: 3.2556383610\n",
      "Logit diff % change:      1.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step61000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 42.38095238095238%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.142857142857142%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Checkpoint 61000:\n",
      "Heads ablated:            [(10, 7), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.1618967056\n",
      "Post ablation logit diff: 3.1253750324\n",
      "Logit diff % change:      -1.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step62000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 40.476190476190474%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Checkpoint 62000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      2.8204376698\n",
      "Post ablation logit diff: 2.7275838852\n",
      "Logit diff % change:      -3.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step63000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 7.142857142857142%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 94.76190476190476%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 44.761904761904766%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 18.571428571428573%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Checkpoint 63000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.2546241283\n",
      "Post ablation logit diff: 2.9543261528\n",
      "Logit diff % change:      -9.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step64000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 40.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 12.380952380952381%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 10.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 28.57142857142857%\n",
      "Checkpoint 64000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.4584672451\n",
      "Post ablation logit diff: 3.4094617367\n",
      "Logit diff % change:      -1.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step65000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 29.04761904761905%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 39.04761904761905%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Checkpoint 65000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      2.6949911118\n",
      "Post ablation logit diff: 2.6880457401\n",
      "Logit diff % change:      -0.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step66000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 40.95238095238095%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 7.8 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 27.142857142857142%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 10.6 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Checkpoint 66000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      2.8696324825\n",
      "Post ablation logit diff: 2.8112149239\n",
      "Logit diff % change:      -2.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step67000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 27.61904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 46.19047619047619%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 22.857142857142858%\n",
      "Checkpoint 67000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.2707591057\n",
      "Post ablation logit diff: 3.1031455994\n",
      "Logit diff % change:      -5.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step68000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 24.761904761904763%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 55.23809523809524%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 89.52380952380953%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 27.61904761904762%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Checkpoint 68000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      2.9035291672\n",
      "Post ablation logit diff: 2.6403608322\n",
      "Logit diff % change:      -9.06%\n",
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step69000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 25.238095238095237%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 55.714285714285715%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 26.666666666666668%\n",
      "Checkpoint 69000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1), (10, 7)]\n",
      "Original logit diff:      3.7018563747\n",
      "Post ablation logit diff: 3.4940247536\n",
      "Logit diff % change:      -5.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step70000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 6.6 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 85.23809523809524%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 26.666666666666668%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 64.28571428571429%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 25.71428571428571%\n",
      "Checkpoint 70000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.3562383652\n",
      "Post ablation logit diff: 2.9660327435\n",
      "Logit diff % change:      -11.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step71000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 82.85714285714286%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 24.285714285714285%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 17.142857142857142%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 89.04761904761904%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 61.904761904761905%\n",
      "Checkpoint 71000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.2580428123\n",
      "Post ablation logit diff: 3.1738085747\n",
      "Logit diff % change:      -2.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step72000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 64.28571428571429%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 83.80952380952381%\n",
      "Checkpoint 72000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (10, 7)]\n",
      "Original logit diff:      3.2968132496\n",
      "Post ablation logit diff: 3.0506057739\n",
      "Logit diff % change:      -7.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step73000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 80.47619047619048%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 27.61904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 85.71428571428571%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 68.57142857142857%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Checkpoint 73000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.2593789101\n",
      "Post ablation logit diff: 3.2348318100\n",
      "Logit diff % change:      -0.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step74000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 74.76190476190476%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 23.809523809523807%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 20.952380952380953%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 94.28571428571428%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 65.71428571428571%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Checkpoint 74000:\n",
      "Heads ablated:            [(8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.4909276962\n",
      "Post ablation logit diff: 3.2871997356\n",
      "Logit diff % change:      -5.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step75000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 75.23809523809524%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 24.761904761904763%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 20.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 59.523809523809526%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 34.76190476190476%\n",
      "Checkpoint 75000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.2216823101\n",
      "Post ablation logit diff: 3.0922091007\n",
      "Logit diff % change:      -4.02%\n",
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step76000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 78.0952380952381%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 27.142857142857142%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 34.285714285714285%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 68.0952380952381%\n",
      "Checkpoint 76000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.2125523090\n",
      "Post ablation logit diff: 2.9223361015\n",
      "Logit diff % change:      -9.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step77000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 66.66666666666666%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 36.19047619047619%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 21.428571428571427%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 73.33333333333333%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Checkpoint 77000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1)]\n",
      "Original logit diff:      3.4437711239\n",
      "Post ablation logit diff: 3.1486220360\n",
      "Logit diff % change:      -8.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step78000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 64.76190476190476%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 40.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 76.19047619047619%\n",
      "Copy circuit for head 10.10 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 24.761904761904763%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 95.23809523809523%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 78000:\n",
      "Heads ablated:            [(10, 7), (8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.5426728725\n",
      "Post ablation logit diff: 3.4938495159\n",
      "Logit diff % change:      -1.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step79000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 64.76190476190476%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 20.0%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 70.95238095238095%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 24.761904761904763%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 93.33333333333333%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 79000:\n",
      "Heads ablated:            [(8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.2653307915\n",
      "Post ablation logit diff: 3.0547547340\n",
      "Logit diff % change:      -6.45%\n",
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step80000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 62.857142857142854%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 97.14285714285714%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 40.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 20.952380952380953%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 72.38095238095238%\n",
      "Checkpoint 80000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1)]\n",
      "Original logit diff:      3.6578080654\n",
      "Post ablation logit diff: 3.6707816124\n",
      "Logit diff % change:      0.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step81000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 64.28571428571429%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 1.9047619047619049%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 90.47619047619048%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 21.904761904761905%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 72.85714285714285%\n",
      "Checkpoint 81000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1)]\n",
      "Original logit diff:      3.0331616402\n",
      "Post ablation logit diff: 3.0056927204\n",
      "Logit diff % change:      -0.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step82000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 60.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 44.285714285714285%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 25.238095238095237%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 67.61904761904762%\n",
      "Checkpoint 82000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2)]\n",
      "Original logit diff:      3.6272935867\n",
      "Post ablation logit diff: 3.2767765522\n",
      "Logit diff % change:      -9.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step83000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 8.5 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 56.19047619047619%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 44.761904761904766%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 70.47619047619048%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 7.142857142857142%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 23.809523809523807%\n",
      "Checkpoint 83000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1)]\n",
      "Original logit diff:      3.3894269466\n",
      "Post ablation logit diff: 3.2037022114\n",
      "Logit diff % change:      -5.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step84000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 70.0%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 24.761904761904763%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 48.57142857142857%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 90.47619047619048%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 50.0%\n",
      "Checkpoint 84000:\n",
      "Heads ablated:            [(8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.2351317406\n",
      "Post ablation logit diff: 3.2633213997\n",
      "Logit diff % change:      0.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step85000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 69.04761904761905%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 42.38095238095238%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 10.476190476190476%\n",
      "Checkpoint 85000:\n",
      "Heads ablated:            [(8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.6393032074\n",
      "Post ablation logit diff: 3.6112432480\n",
      "Logit diff % change:      -0.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step86000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 68.0952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 8.571428571428571%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 24.285714285714285%\n",
      "Copy circuit for head 9.6 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 29.04761904761905%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 51.42857142857142%\n",
      "Checkpoint 86000:\n",
      "Heads ablated:            [(8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.4989900589\n",
      "Post ablation logit diff: 3.5737957954\n",
      "Logit diff % change:      2.14%\n",
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step87000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 25.238095238095237%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 67.14285714285714%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 20.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 20.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 87000:\n",
      "Heads ablated:            [(8, 1), (8, 8), (8, 10), (8, 2)]\n",
      "Original logit diff:      3.7018136978\n",
      "Post ablation logit diff: 3.6515314579\n",
      "Logit diff % change:      -1.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step88000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 66.19047619047619%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 25.71428571428571%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 55.714285714285715%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 90.47619047619048%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 24.761904761904763%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 9.523809523809524%\n",
      "Checkpoint 88000:\n",
      "Heads ablated:            [(8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.8386592865\n",
      "Post ablation logit diff: 3.7866289616\n",
      "Logit diff % change:      -1.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step89000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 57.14285714285714%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 26.666666666666668%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 90.95238095238095%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 26.190476190476193%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 66.19047619047619%\n",
      "Checkpoint 89000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1)]\n",
      "Original logit diff:      3.9634082317\n",
      "Post ablation logit diff: 3.9040296078\n",
      "Logit diff % change:      -1.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step90000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 65.71428571428571%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 26.666666666666668%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 90.47619047619048%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 98.57142857142858%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 57.61904761904761%\n",
      "Checkpoint 90000:\n",
      "Heads ablated:            [(8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.8981649876\n",
      "Post ablation logit diff: 3.8081648350\n",
      "Logit diff % change:      -2.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step91000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 35.23809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 30.0%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 11.428571428571429%\n",
      "Checkpoint 91000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2)]\n",
      "Original logit diff:      3.8161077499\n",
      "Post ablation logit diff: 3.7621653080\n",
      "Logit diff % change:      -1.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step92000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 35.23809523809524%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.52380952380952%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 60.0%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 30.952380952380953%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 62.857142857142854%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 11.904761904761903%\n",
      "Checkpoint 92000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1)]\n",
      "Original logit diff:      3.7368273735\n",
      "Post ablation logit diff: 3.8693537712\n",
      "Logit diff % change:      3.55%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5f9ed865a54a7fb37076a50353c05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step93000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 62.857142857142854%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 13.80952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 30.476190476190478%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 56.666666666666664%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 37.142857142857146%\n",
      "Checkpoint 93000:\n",
      "Heads ablated:            [(8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.4614629745\n",
      "Post ablation logit diff: 3.3163011074\n",
      "Logit diff % change:      -4.19%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2024504d1aad427fad8ce5a5feb674ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step94000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 61.904761904761905%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 16.666666666666664%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 13.80952380952381%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 30.952380952380953%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 88.57142857142857%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 36.666666666666664%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 59.523809523809526%\n",
      "Checkpoint 94000:\n",
      "Heads ablated:            [(8, 1), (8, 2), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.5660009384\n",
      "Post ablation logit diff: 3.9692008495\n",
      "Logit diff % change:      11.31%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb18f9b568ea4648ad7d912ccd9a5249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step95000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 61.42857142857143%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 98.09523809523809%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 42.38095238095238%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 66.66666666666666%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 29.04761904761905%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 96.19047619047619%\n",
      "Checkpoint 95000:\n",
      "Heads ablated:            [(8, 2), (8, 10), (8, 8)]\n",
      "Original logit diff:      3.9980154037\n",
      "Post ablation logit diff: 4.0376839638\n",
      "Logit diff % change:      0.99%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249184aa7a8244afa2c0f2df2ba4cc53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step96000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 59.04761904761905%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 97.61904761904762%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 96000:\n",
      "Heads ablated:            [(8, 8), (8, 10)]\n",
      "Original logit diff:      3.5605666637\n",
      "Post ablation logit diff: 3.7965645790\n",
      "Logit diff % change:      6.63%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e2ad81f6904d978685caaf86b65194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step97000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 99.04761904761905%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 43.333333333333336%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 96.66666666666667%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 60.476190476190474%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 29.523809523809526%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 17.142857142857142%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 61.42857142857143%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 13.333333333333334%\n",
      "Checkpoint 97000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1)]\n",
      "Original logit diff:      4.1608524323\n",
      "Post ablation logit diff: 4.3045535088\n",
      "Logit diff % change:      3.45%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7da308126b482096355613166b417c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step98000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 43.80952380952381%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 98000:\n",
      "Heads ablated:            [(8, 8), (8, 10)]\n",
      "Original logit diff:      3.4448816776\n",
      "Post ablation logit diff: 3.5361001492\n",
      "Logit diff % change:      2.65%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9050e8cc3704f3e854fb84a17881b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step99000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 95.71428571428572%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 99000:\n",
      "Heads ablated:            [(8, 8), (8, 10)]\n",
      "Original logit diff:      3.1775939465\n",
      "Post ablation logit diff: 3.3944940567\n",
      "Logit diff % change:      6.83%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed971f2f27642949d48944da05c70ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step100000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 45.714285714285715%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 16.19047619047619%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Checkpoint 100000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 1)]\n",
      "Original logit diff:      3.2676994801\n",
      "Post ablation logit diff: 3.5644042492\n",
      "Logit diff % change:      9.08%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c977f30c054559b3093f27bc4cd3cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step101000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 69.52380952380952%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 92.85714285714286%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 46.19047619047619%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 91.9047619047619%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 88.09523809523809%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 29.523809523809526%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 17.142857142857142%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 58.0952380952381%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 14.285714285714285%\n",
      "Checkpoint 101000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 2), (8, 1)]\n",
      "Original logit diff:      3.5127615929\n",
      "Post ablation logit diff: 3.7769854069\n",
      "Logit diff % change:      7.52%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304c3d593e884d7a81ad211b3e96cd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step102000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 87.61904761904762%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 91.42857142857143%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 102000:\n",
      "Heads ablated:            [(8, 1), (8, 8), (8, 10)]\n",
      "Original logit diff:      3.6279888153\n",
      "Post ablation logit diff: 3.9062352180\n",
      "Logit diff % change:      7.67%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b596484874874a7fbe3bdc2cf6742857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step103000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 46.666666666666664%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 87.14285714285714%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 57.14285714285714%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 31.428571428571427%\n",
      "Checkpoint 103000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 1)]\n",
      "Original logit diff:      3.8974649906\n",
      "Post ablation logit diff: 4.1543936729\n",
      "Logit diff % change:      6.59%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc253ca083b49c89fd8f7b5a2b3a59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step104000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 65.71428571428571%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 76.66666666666667%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 47.61904761904761%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Checkpoint 104000:\n",
      "Heads ablated:            [(8, 10), (8, 8), (8, 1)]\n",
      "Original logit diff:      3.5403478146\n",
      "Post ablation logit diff: 3.9770648479\n",
      "Logit diff % change:      12.34%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01669b2d42df4b0c8945edc1e58c3f67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step105000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 73.33333333333333%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 49.523809523809526%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 86.19047619047619%\n",
      "Checkpoint 105000:\n",
      "Heads ablated:            [(8, 10), (8, 1)]\n",
      "Original logit diff:      3.6404767036\n",
      "Post ablation logit diff: 4.0004153252\n",
      "Logit diff % change:      9.89%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0262c1fedf7e4ac1880b6da4d5bc8eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step106000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 70.95238095238095%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Checkpoint 106000:\n",
      "Heads ablated:            [(8, 10), (8, 1)]\n",
      "Original logit diff:      3.4120151997\n",
      "Post ablation logit diff: 3.9302241802\n",
      "Logit diff % change:      15.19%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bdf32a09054326b1bde565d1dc53fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step107000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 86.66666666666667%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 69.04761904761905%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 107000:\n",
      "Heads ablated:            [(8, 1), (8, 10)]\n",
      "Original logit diff:      3.8128850460\n",
      "Post ablation logit diff: 4.2036046982\n",
      "Logit diff % change:      10.25%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e522623095f144aeaf354dcf23607a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step108000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 65.71428571428571%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 68.0952380952381%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 108000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.6820647717\n",
      "Post ablation logit diff: 3.9877083302\n",
      "Logit diff % change:      8.30%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc9c375b4c74e01a7c37ed7bc206e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step109000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 59.04761904761905%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 109000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.2483024597\n",
      "Post ablation logit diff: 3.7379381657\n",
      "Logit diff % change:      15.07%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd1756158774bff895e0f59715c0803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step110000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 38.095238095238095%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 83.33333333333334%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 69.04761904761905%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 49.047619047619044%\n",
      "Checkpoint 110000:\n",
      "Heads ablated:            [(8, 10), (8, 1)]\n",
      "Original logit diff:      3.5436263084\n",
      "Post ablation logit diff: 4.0088539124\n",
      "Logit diff % change:      13.13%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4dd784fb934965a8203482880bfe7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step111000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 48.57142857142857%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 31.9047619047619%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 111000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.7435317039\n",
      "Post ablation logit diff: 4.2192454338\n",
      "Logit diff % change:      12.71%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34471aa48f724bac9acbc35942c75526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step112000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 44.761904761904766%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 55.23809523809524%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 27.61904761904762%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 112000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.6175262928\n",
      "Post ablation logit diff: 3.9885284901\n",
      "Logit diff % change:      10.26%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9293db796654ab08e827d021dcad172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step113000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 45.23809523809524%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 19.047619047619047%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 113000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.4972681999\n",
      "Post ablation logit diff: 4.1292743683\n",
      "Logit diff % change:      18.07%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f83396eedd14d88b24ebbcd3c90bed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step114000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 55.714285714285715%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 114000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.6397414207\n",
      "Post ablation logit diff: 3.9691214561\n",
      "Logit diff % change:      9.05%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61898d463794f739afaf1809546eea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step115000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 44.761904761904766%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 66.19047619047619%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 50.0%\n",
      "Copy circuit for head 8.8 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 115000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.6432216167\n",
      "Post ablation logit diff: 4.0651578903\n",
      "Logit diff % change:      11.58%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fe421ac0354eb6a8bf9c5f40415597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step116000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 66.19047619047619%\n",
      "Checkpoint 116000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.5812804699\n",
      "Post ablation logit diff: 3.8684601784\n",
      "Logit diff % change:      8.02%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b2ae3f09394bbe9ca2997af97396df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step117000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 67.14285714285714%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 43.80952380952381%\n",
      "Checkpoint 117000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.2543992996\n",
      "Post ablation logit diff: 3.5804519653\n",
      "Logit diff % change:      10.02%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67afa1d7525447119032ea1b2fa05905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step118000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 43.80952380952381%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 61.42857142857143%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 65.23809523809524%\n",
      "Checkpoint 118000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.4010450840\n",
      "Post ablation logit diff: 3.6946938038\n",
      "Logit diff % change:      8.63%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362124637f424bc49578a3dbe521e9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step119000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 65.23809523809524%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 61.42857142857143%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 4.285714285714286%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 30.952380952380953%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 6.190476190476191%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 24.761904761904763%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 42.857142857142854%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Checkpoint 119000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.9135935307\n",
      "Post ablation logit diff: 4.0135240555\n",
      "Logit diff % change:      2.55%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d12fbf8ae248febf97009828c3658a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step120000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 65.23809523809524%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 64.76190476190476%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 26.190476190476193%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 23.333333333333332%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 42.38095238095238%\n",
      "Checkpoint 120000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      4.4822506905\n",
      "Post ablation logit diff: 4.6183238029\n",
      "Logit diff % change:      3.04%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06157edcfd7d4ce799eb0111007062fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step121000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 21.904761904761905%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 66.19047619047619%\n",
      "Checkpoint 121000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.5944514275\n",
      "Post ablation logit diff: 3.4419775009\n",
      "Logit diff % change:      -4.24%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34969803b674f3291e8f29560b19fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step122000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 28.57142857142857%\n",
      "Copy circuit for head 9.8 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 62.857142857142854%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 65.71428571428571%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 19.523809523809526%\n",
      "Copy circuit for head 7.9 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 7.142857142857142%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 45.23809523809524%\n",
      "Copy circuit for head 5.5 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Checkpoint 122000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      4.5480418205\n",
      "Post ablation logit diff: 4.2158889771\n",
      "Logit diff % change:      -7.30%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17c707bab4b41c994a133d143bcabcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step123000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 60.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 123000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.6875662804\n",
      "Post ablation logit diff: 3.4382033348\n",
      "Logit diff % change:      -6.76%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539386e3d2174e17944f4794e655f83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step124000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 46.666666666666664%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 0.9523809523809524%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 17.61904761904762%\n",
      "Copy circuit for head 8.2 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.4 (sign=1) : Top 5 accuracy: 0.4761904761904762%\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 46.19047619047619%\n",
      "Checkpoint 124000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.9744837284\n",
      "Post ablation logit diff: 3.7679042816\n",
      "Logit diff % change:      -5.20%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7683468a7fe849d0a846bcaf779cf59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step125000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 34.76190476190476%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 61.42857142857143%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 17.142857142857142%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 24.761904761904763%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 47.61904761904761%\n",
      "Checkpoint 125000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.9687879086\n",
      "Post ablation logit diff: 3.7922658920\n",
      "Logit diff % change:      -4.45%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a78b21ce594258957809eb86216ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step126000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 126000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.7914445400\n",
      "Post ablation logit diff: 3.7300384045\n",
      "Logit diff % change:      -1.62%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16817253591460faf94cc7633da8c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step127000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 20.952380952380953%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 127000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.3446874619\n",
      "Post ablation logit diff: 3.4085471630\n",
      "Logit diff % change:      1.91%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3713846d21054bb89863fda305da7c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step128000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.5 (sign=1) : Top 5 accuracy: 9.047619047619047%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 48.57142857142857%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 11.904761904761903%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 47.14285714285714%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Checkpoint 128000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.7995648384\n",
      "Post ablation logit diff: 3.9917380810\n",
      "Logit diff % change:      5.06%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2bdf393630497ea43637926c657673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step129000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 129000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.5301592350\n",
      "Post ablation logit diff: 3.7459716797\n",
      "Logit diff % change:      6.11%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e4eec0fc3d491a986ef7864f49fbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step130000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 41.42857142857143%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 42.38095238095238%\n",
      "Checkpoint 130000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.7103049755\n",
      "Post ablation logit diff: 3.8948459625\n",
      "Logit diff % change:      4.97%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362feb5604494a9da515cf42488acb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step131000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 43.333333333333336%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 3.8095238095238098%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 18.095238095238095%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 40.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 131000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.8659989834\n",
      "Post ablation logit diff: 3.9359099865\n",
      "Logit diff % change:      1.81%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022db4258ed44cf8b92c06638c7638f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step132000; now loading into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 132000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.5665013790\n",
      "Post ablation logit diff: 3.7362439632\n",
      "Logit diff % change:      4.76%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68df328f79694654b619204b9ca94e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step133000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 39.523809523809526%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 13.333333333333334%\n",
      "Copy circuit for head 8.1 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 5.238095238095238%\n",
      "Checkpoint 133000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.7026576996\n",
      "Post ablation logit diff: 3.8817112446\n",
      "Logit diff % change:      4.84%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915ecb4fef5344e592c78447d9436ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step134000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 39.523809523809526%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 3.3333333333333335%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 10.952380952380953%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 4.761904761904762%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 23.809523809523807%\n",
      "Checkpoint 134000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.5571510792\n",
      "Post ablation logit diff: 3.6855390072\n",
      "Logit diff % change:      3.61%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb240a9399c48159ac4073cfe01702f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step135000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 9.523809523809524%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 16.666666666666664%\n",
      "Checkpoint 135000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.7393887043\n",
      "Post ablation logit diff: 3.7103908062\n",
      "Logit diff % change:      -0.78%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9b6204d3724c26ac7a7fd18758995b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step136000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 136000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.6624240875\n",
      "Post ablation logit diff: 3.9168386459\n",
      "Logit diff % change:      6.95%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69819e0c142e43d9a9928578fa62b998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step137000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 2.857142857142857%\n",
      "Copy circuit for head 9.9 (sign=1) : Top 5 accuracy: 7.6190476190476195%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 5.714285714285714%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 10.0%\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 137000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.8636553288\n",
      "Post ablation logit diff: 3.9344565868\n",
      "Logit diff % change:      1.83%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a23581049b4c7ca90a0b8442d2ad61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step138000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 138000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.9709925652\n",
      "Post ablation logit diff: 4.0534920692\n",
      "Logit diff % change:      2.08%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed9df6d106b45228511ba28ad2a32c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step139000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 139000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.8794176579\n",
      "Post ablation logit diff: 4.1224994659\n",
      "Logit diff % change:      6.27%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafb7e5e1b2041c385af4388068377eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step140000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 140000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.7727386951\n",
      "Post ablation logit diff: 3.8461897373\n",
      "Logit diff % change:      1.95%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2adbc3239cc7426a8a3664eb3c875ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step141000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Copy circuit for head 8.7 (sign=1) : Top 5 accuracy: 2.380952380952381%\n",
      "Copy circuit for head 9.7 (sign=1) : Top 5 accuracy: 6.666666666666667%\n",
      "Copy circuit for head 10.11 (sign=1) : Top 5 accuracy: 1.4285714285714286%\n",
      "Copy circuit for head 10.7 (sign=1) : Top 5 accuracy: 30.476190476190478%\n",
      "Checkpoint 141000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.8212573528\n",
      "Post ablation logit diff: 3.8989706039\n",
      "Logit diff % change:      2.03%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d61e5fc6f440198893bfdfe2b6ab42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step142000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 142000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.6675205231\n",
      "Post ablation logit diff: 3.8591077328\n",
      "Logit diff % change:      5.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model EleutherAI/pythia-160m-weight-seed2 at step143000; now loading into HookedTransformer\n",
      "Loaded pretrained model pythia-160m into HookedTransformer\n",
      "Copy circuit for head 8.10 (sign=1) : Top 5 accuracy: 100.0%\n",
      "Checkpoint 143000:\n",
      "Heads ablated:            [(8, 10)]\n",
      "Original logit diff:      3.8680002689\n",
      "Post ablation logit diff: 3.8249075413\n",
      "Logit diff % change:      -1.11%\n"
     ]
    }
   ],
   "source": [
    "experiment_metrics = dict()\n",
    "# create folder\n",
    "os.makedirs(f'results/backup/{MODEL_SHORTNAME}', exist_ok=True)\n",
    "\n",
    "for checkpoint in range(4000, 144000, 1000):\n",
    "\n",
    "    experiment_metrics = run_iteration(\n",
    "        BASE_MODEL, VARIANT, df, checkpoint=checkpoint, dataset=ioi_dataset, experiment_metrics=experiment_metrics, \n",
    "        threshold=COPY_SCORE_THRESHOLD\n",
    "    )\n",
    "    experiment_metrics = process_backup_results(df, checkpoint, experiment_metrics)\n",
    "\n",
    "    # save to file, using pytorch format\n",
    "    torch.save(experiment_metrics, f'results/backup/{MODEL_SHORTNAME}/nmh_backup_metrics.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([4000, 5000, 6000, 7000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_metrics.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pythia 160m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TO_VIEW = \"pythia-160m-alldropout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_metrics = torch.load(f'results/backup/{MODEL_TO_VIEW}/nmh_backup_metrics.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['logit_diff', 'per_head_logit_diffs', 'ablation_targets', 'ablated_logit_diff', 'per_head_ablated_logit_diffs', 'per_head_logit_diff_delta', 'in_circuit_head_delta', 'outside_circuit_head_delta', 'summed_in_circuit_head_delta', 'summed_outside_circuit_head_delta', 'summed_total_head_delta'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_metrics[4000].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_in_circuit_head_deltas = {checkpoint: experiment_metrics[checkpoint][\"summed_in_circuit_head_delta\"] for checkpoint in experiment_metrics.keys()}\n",
    "summed_outside_circuit_head_deltas = {checkpoint: experiment_metrics[checkpoint][\"summed_outside_circuit_head_delta\"] for checkpoint in experiment_metrics.keys()}\n",
    "summed_total_head_deltas = {checkpoint: experiment_metrics[checkpoint][\"summed_total_head_delta\"] for checkpoint in experiment_metrics.keys()}\n",
    "per_head_logit_diff_deltas = {checkpoint: experiment_metrics[checkpoint][\"per_head_logit_diff_delta\"] for checkpoint in experiment_metrics.keys()}\n",
    "total_logit_diff_deltas = {checkpoint: experiment_metrics[checkpoint]['ablated_logit_diff'] - experiment_metrics[checkpoint]['logit_diff'] for checkpoint in experiment_metrics.keys()}\n",
    "\n",
    "for checkpoint in experiment_metrics.keys():\n",
    "    # divide by total original logit diff\n",
    "    summed_in_circuit_head_deltas[checkpoint] = summed_in_circuit_head_deltas[checkpoint] / experiment_metrics[checkpoint][\"logit_diff\"]\n",
    "    summed_outside_circuit_head_deltas[checkpoint] = summed_outside_circuit_head_deltas[checkpoint] / experiment_metrics[checkpoint][\"logit_diff\"]\n",
    "    summed_total_head_deltas[checkpoint] = summed_total_head_deltas[checkpoint] / experiment_metrics[checkpoint][\"logit_diff\"]\n",
    "    per_head_logit_diff_deltas[checkpoint] = per_head_logit_diff_deltas[checkpoint] / experiment_metrics[checkpoint][\"logit_diff\"]\n",
    "    total_logit_diff_deltas[checkpoint] = total_logit_diff_deltas[checkpoint] / experiment_metrics[checkpoint][\"logit_diff\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Checkpoint=%{x}<br>Change as % of original logit diff=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          4000,
          5000,
          6000,
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          126000,
          127000,
          128000,
          129000,
          130000,
          131000,
          132000,
          133000,
          134000,
          135000,
          136000,
          137000,
          138000,
          139000,
          140000,
          141000,
          142000,
          143000
         ],
         "xaxis": "x",
         "y": [
          -0.04874759206355586,
          0,
          -0.004927556561437061,
          0.0006743107202406805,
          0.04036578656102951,
          0.09597346521368576,
          0.10268239756901225,
          0.05690063353421415,
          0.09948137809491371,
          0.07299122642324421,
          0.10252651704533937,
          0.11269554053433817,
          0.10051376825947178,
          0.1088063776437573,
          0.12957015031302271,
          0.10681447081913822,
          0.14895983960725495,
          0.12758877854470932,
          0.13645988726566838,
          0.14865951587102463,
          0.14164626970710809,
          0.12957601809369412,
          0.12878618492862373,
          0.08272382214374036,
          0.10825265798896781,
          0.11976527111009047,
          0.11758727988664926,
          0.12307609275652977,
          0.10645015916966778,
          0.09973174072990366,
          0.1004438753795971,
          0.11348029558509394,
          0.11181337549663709,
          0.08856981395822675,
          0.07580316626216874,
          0.10308891037353882,
          0.08925231558624137,
          0.06620143341399017,
          0.06140121536160252,
          0.09611860008277916,
          0.07933803025040254,
          0.10934576306318014,
          0.07065813294263679,
          0.07085727218614304,
          0.05536279021059895,
          0.09221853705243352,
          0.09588969922890397,
          0.08822287125266501,
          0.06862912712177309,
          0.0877592843441588,
          0.07751692498788725,
          0.0737459113971808,
          0.08568443159713797,
          0.0813632727738016,
          0.08691193583856249,
          0.056643390227164594,
          0.06657101217973874,
          0.10561306791571012,
          0.07475683633136156,
          0.0750266473402659,
          0.07487797617407495,
          0.08496792846714966,
          0.062175367993380956,
          0.06392574909504538,
          0.09451198044168062,
          0.05890794534716129,
          0.09054137132725114,
          0.07413826899615883,
          0.0844143143946076,
          0.06570059019004362,
          0.07920544498006071,
          0.11261990881072628,
          0.09277152714132121,
          0.07376765518034124,
          0.04737410089469147,
          0.07426975146102822,
          0.03485145286770831,
          0.0651146774228661,
          0.07628403816979303,
          0.05445336968129576,
          0.029765411406656533,
          0.05696537247676474,
          0.044782428656720163,
          0.045010959778588974,
          0.05228933268163094,
          0.07097674095813095,
          0.04246955357100003,
          0.06467394143593337,
          0.04563181243663678,
          0.04340673084905829,
          0.04306732583159576,
          0.05211456211993966,
          0.05871719707301907,
          0.05810109109598053,
          0.06079037078225046,
          0.03944882268243631,
          0.04728356991911338,
          0.05733666305026681,
          0.0388132774520031,
          0.04730385263207168,
          0.04114124955393634,
          0.03949183592119944,
          0.03457835370841648,
          0.03459737426985162,
          0.0319678010528512,
          0.03308853582748141,
          0.027661118782368894,
          0.03456853108911376,
          0.02840640385111496,
          0.01930603554786443,
          0.03601782777642817,
          0.023082051785884863,
          0.03125438246715211,
          0.01682396002549297,
          0.020912885085288317,
          0.015418706460666776,
          0.017511601479999958,
          0.009844104245258343,
          0.017460074066661557,
          0.026944187919356082,
          0.014296447739947957,
          0.012739445342321756,
          0.006476104462168121,
          0.009815081547151416,
          0.009769135654954898,
          0.010748837822350003,
          0.010674380710562491,
          0.020791681357431868,
          0.015429756254052416,
          0.007981631458420691,
          0.01217451486085811,
          0.018553015444649157,
          0.015657068481213406,
          0.020563142097295095,
          0.011444063409415926,
          0.011145184411754565,
          0.019839719319202333,
          0.015308745634116293,
          0.009208870476862177,
          0.011535058932260723
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Summed Post-NMH-Ablation In-Circuit Head Logit Diff Change Over Time (pythia-160m-alldropout)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Checkpoint"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Change as % of original logit diff"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"0515f89e-4987-4eec-9bed-743ec134d2f6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0515f89e-4987-4eec-9bed-743ec134d2f6\")) {                    Plotly.newPlot(                        \"0515f89e-4987-4eec-9bed-743ec134d2f6\",                        [{\"hovertemplate\":\"Checkpoint=%{x}\\u003cbr\\u003eChange as % of original logit diff=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4000,5000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000],\"xaxis\":\"x\",\"y\":[-0.04874759206355586,0.0,-0.004927556561437061,0.0006743107202406805,0.04036578656102951,0.09597346521368576,0.10268239756901225,0.05690063353421415,0.09948137809491371,0.07299122642324421,0.10252651704533937,0.11269554053433817,0.10051376825947178,0.1088063776437573,0.12957015031302271,0.10681447081913822,0.14895983960725495,0.12758877854470932,0.13645988726566838,0.14865951587102463,0.14164626970710809,0.12957601809369412,0.12878618492862373,0.08272382214374036,0.10825265798896781,0.11976527111009047,0.11758727988664926,0.12307609275652977,0.10645015916966778,0.09973174072990366,0.1004438753795971,0.11348029558509394,0.11181337549663709,0.08856981395822675,0.07580316626216874,0.10308891037353882,0.08925231558624137,0.06620143341399017,0.06140121536160252,0.09611860008277916,0.07933803025040254,0.10934576306318014,0.07065813294263679,0.07085727218614304,0.05536279021059895,0.09221853705243352,0.09588969922890397,0.08822287125266501,0.06862912712177309,0.0877592843441588,0.07751692498788725,0.0737459113971808,0.08568443159713797,0.0813632727738016,0.08691193583856249,0.056643390227164594,0.06657101217973874,0.10561306791571012,0.07475683633136156,0.0750266473402659,0.07487797617407495,0.08496792846714966,0.062175367993380956,0.06392574909504538,0.09451198044168062,0.05890794534716129,0.09054137132725114,0.07413826899615883,0.0844143143946076,0.06570059019004362,0.07920544498006071,0.11261990881072628,0.09277152714132121,0.07376765518034124,0.04737410089469147,0.07426975146102822,0.03485145286770831,0.0651146774228661,0.07628403816979303,0.05445336968129576,0.029765411406656533,0.05696537247676474,0.044782428656720163,0.045010959778588974,0.05228933268163094,0.07097674095813095,0.04246955357100003,0.06467394143593337,0.04563181243663678,0.04340673084905829,0.04306732583159576,0.05211456211993966,0.05871719707301907,0.05810109109598053,0.06079037078225046,0.03944882268243631,0.04728356991911338,0.05733666305026681,0.0388132774520031,0.04730385263207168,0.04114124955393634,0.03949183592119944,0.03457835370841648,0.03459737426985162,0.0319678010528512,0.03308853582748141,0.027661118782368894,0.03456853108911376,0.02840640385111496,0.01930603554786443,0.03601782777642817,0.023082051785884863,0.03125438246715211,0.01682396002549297,0.020912885085288317,0.015418706460666776,0.017511601479999958,0.009844104245258343,0.017460074066661557,0.026944187919356082,0.014296447739947957,0.012739445342321756,0.006476104462168121,0.009815081547151416,0.009769135654954898,0.010748837822350003,0.010674380710562491,0.020791681357431868,0.015429756254052416,0.007981631458420691,0.01217451486085811,0.018553015444649157,0.015657068481213406,0.020563142097295095,0.011444063409415926,0.011145184411754565,0.019839719319202333,0.015308745634116293,0.009208870476862177,0.011535058932260723],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Checkpoint\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Change as % of original logit diff\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Summed Post-NMH-Ablation In-Circuit Head Logit Diff Change Over Time (pythia-160m-alldropout)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('0515f89e-4987-4eec-9bed-743ec134d2f6');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot summed_in_circuit_head_deltas with plotly express\n",
    "fig = px.line(\n",
    "    x=list(summed_in_circuit_head_deltas.keys()), \n",
    "    y=list(summed_in_circuit_head_deltas.values()), \n",
    "    title=f\"Summed Post-NMH-Ablation In-Circuit Head Logit Diff Change Over Time ({MODEL_TO_VIEW})\",\n",
    "    labels={'x': 'Checkpoint', 'y': 'Change as % of original logit diff'} \n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Checkpoint=%{x}<br>Change as % of original logit diff=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          4000,
          5000,
          6000,
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          126000,
          127000,
          128000,
          129000,
          130000,
          131000,
          132000,
          133000,
          134000,
          135000,
          136000,
          137000,
          138000,
          139000,
          140000,
          141000,
          142000,
          143000
         ],
         "xaxis": "x",
         "y": [
          -0.043071704995338224,
          0,
          -0.034838356390819954,
          0.010491199797567822,
          0.010968819453935222,
          0.0015313982563985365,
          0.00532543547393766,
          0.010752839912506215,
          0.02531287764098698,
          0.018958980378701782,
          0.012663146585631049,
          0.015745443908849608,
          0.001253890462658407,
          0.014050342054257883,
          0.01441419142351048,
          0.013618688241534208,
          0.029345154340804314,
          0.00206194990681872,
          0.02779381463404436,
          0.02809484386991504,
          0.030302537238172285,
          0.002692509833182894,
          0.02245335973625196,
          0.0010800536696668995,
          -0.00033378300289789686,
          0.02278989011528307,
          -0.0007894061245561732,
          0.0005445490384878283,
          -0.0007571442634131655,
          -0.000715703684103466,
          -0.000020625481121832807,
          0.0004697040728810489,
          0.0007784223780814578,
          0.012825845535615372,
          0.03468700412608689,
          0.015627993597741183,
          0.0419716025512508,
          0.02877968397416872,
          0.019712446398668718,
          0.026902490602005048,
          0.016781069312192788,
          0.002071077983715289,
          0.018275545416346094,
          0.015047084829562117,
          0.030879653712632793,
          0.016442919292460903,
          0.0044167596571767906,
          0.028159189672801635,
          0.07909375240706666,
          0.05178431847726203,
          0.0018901401182523845,
          0.0034961710715242637,
          0.00292895533018044,
          0.002126415556090234,
          0.0030492390829039333,
          0.0030223033136976794,
          0.02008166412505668,
          0.0035456041883057607,
          0.020610581350076086,
          0.024104055071373132,
          0.0029305579335846266,
          0.004411356993980214,
          0.0036760890413248598,
          0.0552390828331396,
          0.010253089622964395,
          0.003070315727987247,
          0.004598480793756628,
          0.025428259363388748,
          0.004157358861025079,
          0.0033033339611961395,
          0.02958754344612417,
          0.019078342996456648,
          0.005432832619587728,
          0.025313198970504536,
          0.01829353996991709,
          0.0031303388257330022,
          0.04855533403575165,
          0.00331778741172562,
          0.0019858912256388987,
          0.00414795719644388,
          0.04796310680849898,
          0.006522627675811524,
          0.0433516493848596,
          0.026138310138358635,
          0.009362040020852181,
          0.006914942879938271,
          0.042187168378208253,
          0.0037349999310240796,
          0.049817972231316204,
          0.002789436935838183,
          0.002797758411895943,
          0.05204835040152067,
          0.059878940640000565,
          0.0163152878400242,
          0.0022534859389283317,
          0.0319401522048137,
          0.007048238089235162,
          0.0121535512832368,
          0.0038414043481558528,
          0.0037456667069525173,
          0.008391687502348944,
          0.0027589807898952116,
          0.005605328460150576,
          0.0013763728010548737,
          0.0011645933953647026,
          0.008593100898644101,
          0.004654583232310729,
          0.00584879137740316,
          0.0100021931536962,
          0.03065953835606348,
          0.008855759003169365,
          0.019745341236709697,
          0.002796119614619538,
          0.011352454245092403,
          0.00666919775743624,
          0.004675234223743095,
          0.0018308533567863894,
          0.009164939117404373,
          0.004221618018106341,
          0.004134954167825886,
          0.01302100501016565,
          0.012646882327714179,
          0.0011572067096873562,
          0.0007284961634218276,
          0.0018684002774627673,
          0.012999804107363734,
          0.004371371921108856,
          0.01478777802326914,
          0.004974097550796613,
          0.0007935620404013462,
          0.011419992415595876,
          0.00855909908955385,
          0.0055227099616256955,
          0.005477746329584981,
          0.004117184404643398,
          0.005805052026921262,
          0.0030363544844265026,
          0.0010752626343117785,
          0.00492113538277586,
          0.0021855264517527504
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Summed Post-NMH-Ablation Outside-Circuit Head Attribution Change (pythia-160m-alldropout)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Checkpoint"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Change as % of original logit diff"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"bd1257e2-2024-4d66-b393-aa731fdc075b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bd1257e2-2024-4d66-b393-aa731fdc075b\")) {                    Plotly.newPlot(                        \"bd1257e2-2024-4d66-b393-aa731fdc075b\",                        [{\"hovertemplate\":\"Checkpoint=%{x}\\u003cbr\\u003eChange as % of original logit diff=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4000,5000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000],\"xaxis\":\"x\",\"y\":[-0.043071704995338224,0.0,-0.034838356390819954,0.010491199797567822,0.010968819453935222,0.0015313982563985365,0.00532543547393766,0.010752839912506215,0.02531287764098698,0.018958980378701782,0.012663146585631049,0.015745443908849608,0.001253890462658407,0.014050342054257883,0.01441419142351048,0.013618688241534208,0.029345154340804314,0.00206194990681872,0.02779381463404436,0.02809484386991504,0.030302537238172285,0.002692509833182894,0.02245335973625196,0.0010800536696668995,-0.00033378300289789686,0.02278989011528307,-0.0007894061245561732,0.0005445490384878283,-0.0007571442634131655,-0.000715703684103466,-2.0625481121832807e-05,0.0004697040728810489,0.0007784223780814578,0.012825845535615372,0.03468700412608689,0.015627993597741183,0.0419716025512508,0.02877968397416872,0.019712446398668718,0.026902490602005048,0.016781069312192788,0.002071077983715289,0.018275545416346094,0.015047084829562117,0.030879653712632793,0.016442919292460903,0.0044167596571767906,0.028159189672801635,0.07909375240706666,0.05178431847726203,0.0018901401182523845,0.0034961710715242637,0.00292895533018044,0.002126415556090234,0.0030492390829039333,0.0030223033136976794,0.02008166412505668,0.0035456041883057607,0.020610581350076086,0.024104055071373132,0.0029305579335846266,0.004411356993980214,0.0036760890413248598,0.0552390828331396,0.010253089622964395,0.003070315727987247,0.004598480793756628,0.025428259363388748,0.004157358861025079,0.0033033339611961395,0.02958754344612417,0.019078342996456648,0.005432832619587728,0.025313198970504536,0.01829353996991709,0.0031303388257330022,0.04855533403575165,0.00331778741172562,0.0019858912256388987,0.00414795719644388,0.04796310680849898,0.006522627675811524,0.0433516493848596,0.026138310138358635,0.009362040020852181,0.006914942879938271,0.042187168378208253,0.0037349999310240796,0.049817972231316204,0.002789436935838183,0.002797758411895943,0.05204835040152067,0.059878940640000565,0.0163152878400242,0.0022534859389283317,0.0319401522048137,0.007048238089235162,0.0121535512832368,0.0038414043481558528,0.0037456667069525173,0.008391687502348944,0.0027589807898952116,0.005605328460150576,0.0013763728010548737,0.0011645933953647026,0.008593100898644101,0.004654583232310729,0.00584879137740316,0.0100021931536962,0.03065953835606348,0.008855759003169365,0.019745341236709697,0.002796119614619538,0.011352454245092403,0.00666919775743624,0.004675234223743095,0.0018308533567863894,0.009164939117404373,0.004221618018106341,0.004134954167825886,0.01302100501016565,0.012646882327714179,0.0011572067096873562,0.0007284961634218276,0.0018684002774627673,0.012999804107363734,0.004371371921108856,0.01478777802326914,0.004974097550796613,0.0007935620404013462,0.011419992415595876,0.00855909908955385,0.0055227099616256955,0.005477746329584981,0.004117184404643398,0.005805052026921262,0.0030363544844265026,0.0010752626343117785,0.00492113538277586,0.0021855264517527504],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Checkpoint\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Change as % of original logit diff\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Summed Post-NMH-Ablation Outside-Circuit Head Attribution Change (pythia-160m-alldropout)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('bd1257e2-2024-4d66-b393-aa731fdc075b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot summed_outside_circuit_head_deltas\n",
    "fig = px.line(\n",
    "    x=list(summed_outside_circuit_head_deltas.keys()), \n",
    "    y=list(summed_outside_circuit_head_deltas.values()), \n",
    "    title=f\"Summed Post-NMH-Ablation Outside-Circuit Head Attribution Change ({MODEL_TO_VIEW})\",\n",
    "    labels={'x': 'Checkpoint', 'y': 'Change as % of original logit diff'} \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Checkpoint=%{x}<br>Change as % of original logit diff=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          4000,
          5000,
          6000,
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          126000,
          127000,
          128000,
          129000,
          130000,
          131000,
          132000,
          133000,
          134000,
          135000,
          136000,
          137000,
          138000,
          139000,
          140000,
          141000,
          142000,
          143000
         ],
         "xaxis": "x",
         "y": [
          -0.09181928713007813,
          0,
          -0.039765914020408424,
          0.01116551022715545,
          0.051334606014964736,
          0.09750486636720443,
          0.10800784066806234,
          0.06765347279882251,
          0.12479425826108283,
          0.09195020963770843,
          0.11518965522190165,
          0.128440988434745,
          0.10176766107991159,
          0.1228567347769904,
          0.1439843356084074,
          0.12043316510958856,
          0.17830498310010995,
          0.12965072871278943,
          0.1642537085664069,
          0.17675435150077187,
          0.17194881988970384,
          0.13226852553203503,
          0.15123954058477676,
          0.08380387909712117,
          0.10791887682335967,
          0.1425551631691294,
          0.11679787032397541,
          0.12362063481083123,
          0.10569302566158754,
          0.09901603454923519,
          0.10042324979623075,
          0.11394998603594424,
          0.11259181107306139,
          0.10139566819219548,
          0.11049018338626121,
          0.11871690580446906,
          0.13122389728812567,
          0.09498112064017077,
          0.0811136667586547,
          0.1230210873512162,
          0.09611909437568336,
          0.11141684850653204,
          0.08893368347231226,
          0.08590435365016641,
          0.08624244750859282,
          0.10866145001564591,
          0.10030645493135638,
          0.11638205412696341,
          0.14772288633808067,
          0.13954360282142084,
          0.0794070589818,
          0.07724207831375601,
          0.08861338397714501,
          0.08348970034680132,
          0.08996116278146113,
          0.059665694624924985,
          0.08665267466094624,
          0.10915866688377719,
          0.09536740728816968,
          0.09913069721492958,
          0.07780853291173524,
          0.08937928730952241,
          0.06585145890019257,
          0.11916483192818499,
          0.10476508250137885,
          0.06197825677345977,
          0.095139840021588,
          0.099566513295163,
          0.08857167325563267,
          0.06900393207472022,
          0.10879298468315261,
          0.1316982499112236,
          0.09820434704559171,
          0.0990808577939736,
          0.06566764086460855,
          0.0774000920619909,
          0.08340680601639823,
          0.06843246187673153,
          0.0782699267371303,
          0.05860133021861987,
          0.07772852410521922,
          0.06348800371155022,
          0.0881340741239871,
          0.07114926564199614,
          0.061651373663250186,
          0.07789168640282297,
          0.08465671829094253,
          0.06840894264239977,
          0.09544978466795298,
          0.0461961634318856,
          0.04586508070564317,
          0.10416291642660444,
          0.11859615118839517,
          0.07441637472111778,
          0.06304386017745693,
          0.07138897488725,
          0.05433181309565516,
          0.06949022205997323,
          0.042654679505241315,
          0.05104951438495987,
          0.04953293803828524,
          0.042250814394357246,
          0.040183682168567054,
          0.03597374950986658,
          0.033132390085256556,
          0.041681634966804854,
          0.03231570583529021,
          0.04041732603867094,
          0.03840859613269863,
          0.0499655723974868,
          0.04487359440843663,
          0.04282739825967072,
          0.03405050291304387,
          0.028176418197317206,
          0.02758208242884376,
          0.02009394152225901,
          0.019342454420329797,
          0.019009043362662716,
          0.0216816916780099,
          0.03107914129881196,
          0.02731745354737444,
          0.025386325255579545,
          0.0076333108536929305,
          0.010543577524289915,
          0.011637534614462893,
          0.023748641929713737,
          0.015045752631671348,
          0.035579460993888985,
          0.020403854963299786,
          0.008775193305801598,
          0.023594506477032005,
          0.027112112985555506,
          0.02117977670743777,
          0.026040886831036357,
          0.015561247034568867,
          0.016950236041370415,
          0.02287607401389129,
          0.016384007975106016,
          0.014130005106486683,
          0.013720585958523226
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Summed Total Post-NMH-Ablation Head Attribution Change (pythia-160m-alldropout)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Checkpoint"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Change as % of original logit diff"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"37dee492-fe36-4bfd-8803-c69cb5a2b6ff\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"37dee492-fe36-4bfd-8803-c69cb5a2b6ff\")) {                    Plotly.newPlot(                        \"37dee492-fe36-4bfd-8803-c69cb5a2b6ff\",                        [{\"hovertemplate\":\"Checkpoint=%{x}\\u003cbr\\u003eChange as % of original logit diff=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4000,5000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000],\"xaxis\":\"x\",\"y\":[-0.09181928713007813,0.0,-0.039765914020408424,0.01116551022715545,0.051334606014964736,0.09750486636720443,0.10800784066806234,0.06765347279882251,0.12479425826108283,0.09195020963770843,0.11518965522190165,0.128440988434745,0.10176766107991159,0.1228567347769904,0.1439843356084074,0.12043316510958856,0.17830498310010995,0.12965072871278943,0.1642537085664069,0.17675435150077187,0.17194881988970384,0.13226852553203503,0.15123954058477676,0.08380387909712117,0.10791887682335967,0.1425551631691294,0.11679787032397541,0.12362063481083123,0.10569302566158754,0.09901603454923519,0.10042324979623075,0.11394998603594424,0.11259181107306139,0.10139566819219548,0.11049018338626121,0.11871690580446906,0.13122389728812567,0.09498112064017077,0.0811136667586547,0.1230210873512162,0.09611909437568336,0.11141684850653204,0.08893368347231226,0.08590435365016641,0.08624244750859282,0.10866145001564591,0.10030645493135638,0.11638205412696341,0.14772288633808067,0.13954360282142084,0.0794070589818,0.07724207831375601,0.08861338397714501,0.08348970034680132,0.08996116278146113,0.059665694624924985,0.08665267466094624,0.10915866688377719,0.09536740728816968,0.09913069721492958,0.07780853291173524,0.08937928730952241,0.06585145890019257,0.11916483192818499,0.10476508250137885,0.06197825677345977,0.095139840021588,0.099566513295163,0.08857167325563267,0.06900393207472022,0.10879298468315261,0.1316982499112236,0.09820434704559171,0.0990808577939736,0.06566764086460855,0.0774000920619909,0.08340680601639823,0.06843246187673153,0.0782699267371303,0.05860133021861987,0.07772852410521922,0.06348800371155022,0.0881340741239871,0.07114926564199614,0.061651373663250186,0.07789168640282297,0.08465671829094253,0.06840894264239977,0.09544978466795298,0.0461961634318856,0.04586508070564317,0.10416291642660444,0.11859615118839517,0.07441637472111778,0.06304386017745693,0.07138897488725,0.05433181309565516,0.06949022205997323,0.042654679505241315,0.05104951438495987,0.04953293803828524,0.042250814394357246,0.040183682168567054,0.03597374950986658,0.033132390085256556,0.041681634966804854,0.03231570583529021,0.04041732603867094,0.03840859613269863,0.0499655723974868,0.04487359440843663,0.04282739825967072,0.03405050291304387,0.028176418197317206,0.02758208242884376,0.02009394152225901,0.019342454420329797,0.019009043362662716,0.0216816916780099,0.03107914129881196,0.02731745354737444,0.025386325255579545,0.0076333108536929305,0.010543577524289915,0.011637534614462893,0.023748641929713737,0.015045752631671348,0.035579460993888985,0.020403854963299786,0.008775193305801598,0.023594506477032005,0.027112112985555506,0.02117977670743777,0.026040886831036357,0.015561247034568867,0.016950236041370415,0.02287607401389129,0.016384007975106016,0.014130005106486683,0.013720585958523226],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Checkpoint\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Change as % of original logit diff\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Summed Total Post-NMH-Ablation Head Attribution Change (pythia-160m-alldropout)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('37dee492-fe36-4bfd-8803-c69cb5a2b6ff');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot total_head_deltas\n",
    "fig = px.line(\n",
    "    x=list(summed_total_head_deltas.keys()), \n",
    "    y=list(summed_total_head_deltas.values()), \n",
    "    title=f\"Summed Total Post-NMH-Ablation Head Attribution Change ({MODEL_TO_VIEW})\",\n",
    "    labels={'x': 'Checkpoint', 'y': 'Change as % of original logit diff'}\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_nmhs, checkpoint_nmhs = get_past_nmhs_for_checkpoints(experiment_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Layer-Head=Layer 10-Head 7<br>Checkpoint=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "Layer 10-Head 7",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Layer 10-Head 7",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          4000,
          6000,
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          20000,
          21000,
          24000,
          25000,
          26000,
          28000,
          29000,
          30000,
          31000,
          33000,
          35000,
          55000,
          66000,
          72000,
          78000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          129000,
          130000,
          132000
         ],
         "xaxis": "x",
         "y": [
          0.0026725521311163902,
          0.000629258924163878,
          0.0007657704991288483,
          0.0003119302273262292,
          0.0005408104043453932,
          0.0005257618031464517,
          0.00027662841603159904,
          0.00040443631587550044,
          0.00045996386324986815,
          0.0005266859079711139,
          0.0006991972331888974,
          0.0004529976285994053,
          0.0007804500637575984,
          0.0006502866744995117,
          0.0009810348274186254,
          0.0008349480340257287,
          0.0009809155017137527,
          0.000916038581635803,
          0.0005797024932689965,
          0.0003255648189224303,
          0.0006155213923193514,
          0.00020697912259493023,
          0.00013133237371221185,
          0.00025235459906980395,
          0.0004624385910574347,
          0.0008549588965252042,
          0.0006586548988707364,
          0.0004686087486334145,
          0.0015414286172017455,
          0.003221277380362153,
          0.00985526293516159,
          0.0173241775482893,
          0.029232485219836235,
          0.022043347358703613,
          0.028252970427274704,
          0.022193506360054016,
          0.022391492500901222,
          0.03811975196003914,
          0.03271893411874771,
          0.022330261766910553,
          0.033639296889305115,
          0.022757917642593384,
          0.02427608333528042,
          0.029711777344346046,
          0.025612127035856247,
          0.016769522801041603,
          0.022821063175797462,
          0.03760848566889763,
          0.02798982709646225,
          0.027296870946884155,
          0.037040431052446365,
          0.024523871019482613,
          0.020268121734261513,
          0.02189512364566326,
          0.021337129175662994,
          0.022509777918457985,
          0.014212765730917454,
          0.01839056983590126,
          0.01428866945207119,
          0.013621959835290909,
          0.013122176751494408,
          0.01007688045501709,
          0.01160617545247078,
          0.01563279703259468,
          0.0035480703227221966,
          0.011431626044213772,
          0.007735598832368851,
          0.012061845511198044,
          0.005630972795188427,
          0.004875671584159136,
          0.004165762569755316,
          0.002620133338496089,
          0.005015193950384855,
          0.00404421566054225,
          0.0008534201770089567,
          0.0004718709387816489,
          0.0004573932965286076,
          0.00028085170197300613
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Layer-Head=Layer 9-Head 9<br>Checkpoint=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "Layer 9-Head 9",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Layer 9-Head 9",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          7000,
          8000,
          11000,
          13000,
          14000,
          15000,
          17000,
          18000,
          19000,
          20000,
          22000,
          23000,
          24000,
          26000,
          29000,
          30000,
          31000,
          35000,
          36000,
          39000,
          40000,
          41000,
          43000,
          45000,
          49000,
          51000,
          52000,
          53000,
          55000,
          58000,
          61000,
          62000,
          63000,
          65000,
          66000,
          67000,
          68000,
          70000,
          71000,
          74000,
          75000,
          76000,
          77000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          106000,
          109000,
          110000,
          112000,
          114000,
          117000,
          118000,
          119000,
          124000,
          125000,
          128000,
          129000,
          131000,
          132000,
          134000
         ],
         "xaxis": "x",
         "y": [
          0.0017917121294885874,
          0.005667125340551138,
          0.007299109827727079,
          0.008191687986254692,
          0.012010126374661922,
          0.012112344615161419,
          0.01314469613134861,
          0.012295766733586788,
          0.012480290606617928,
          0.02863914519548416,
          0.024729236960411072,
          0.021760735660791397,
          0.02855166792869568,
          0.02219049260020256,
          0.02279970981180668,
          0.019900595769286156,
          0.02176225557923317,
          0.020162083208560944,
          0.01848527416586876,
          0.019513001665472984,
          0.018014416098594666,
          0.012220167554914951,
          0.017404286190867424,
          0.017807412892580032,
          0.016632266342639923,
          0.02064628154039383,
          0.01906583644449711,
          0.019717315211892128,
          0.011843998916447163,
          0.015505749732255936,
          0.020441871136426926,
          0.017366889864206314,
          0.01892448030412197,
          0.01852373033761978,
          0.013165752403438091,
          0.019930146634578705,
          0.021192502230405807,
          0.022864865139126778,
          0.019892964512109756,
          0.023508219048380852,
          0.025774521753191948,
          0.022104782983660698,
          0.021632587537169456,
          0.02096971869468689,
          0.020188646391034126,
          0.018835877999663353,
          0.016392558813095093,
          0.011681637726724148,
          0.008836541324853897,
          0.004337572958320379,
          0.003690236946567893,
          0.003030212363228202,
          0.0037146529648452997,
          0.0023884151596575975,
          0.004061004612594843,
          0.0027726173866540194,
          0.0021455190144479275,
          0.00363927218131721,
          0.0034062149934470654,
          0.004100973252207041,
          0.004484336357563734,
          0.0040551722049713135,
          0.0038121941033750772,
          0.0022667511366307735,
          0.0052386196330189705,
          0.0036718465853482485,
          0.0033050274942070246,
          0.0027817883528769016,
          0.0023229457437992096,
          0.0021952930837869644,
          0.00239189644344151,
          0.002751966007053852,
          0.0016180173261091113,
          0.0024881938006728888,
          0.0016974697355180979,
          0.00177038146648556,
          0.001340522663667798,
          0.0009367555612698197,
          0.0009449610952287912,
          0.0009817652171477675,
          0.001115450169891119,
          0.0014319070614874363
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Layer-Head=Layer 9-Head 6<br>Checkpoint=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "Layer 9-Head 6",
         "line": {
          "color": "#00cc96",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Layer 9-Head 6",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          84000,
          86000,
          90000,
          92000,
          95000,
          96000,
          113000,
          115000,
          121000,
          124000,
          125000,
          129000,
          131000,
          134000
         ],
         "xaxis": "x",
         "y": [
          0.002411766443401575,
          0.0033179090823978186,
          0.0038747189100831747,
          0.005100138485431671,
          0.0032757434528321028,
          0.005085034761577845,
          0.00363577320240438,
          0.004538726061582565,
          0.004653990268707275,
          0.0029530839528888464,
          0.005190160591155291,
          0.004965000320225954,
          0.005145061761140823,
          0.010231755673885345,
          0.0066286372020840645,
          0.008754000999033451,
          0.009256397373974323,
          0.011626716703176498,
          0.009798008017241955,
          0.008918024599552155,
          0.006602451205253601,
          0.009992163628339767,
          0.012730184011161327,
          0.009922172874212265,
          0.01246168464422226,
          0.012267830781638622,
          0.012358670122921467,
          0.011819890700280666,
          0.009157104417681694,
          0.011974005028605461,
          0.012825957499444485,
          0.010397862643003464,
          0.014318552799522877,
          0.012905200012028217,
          0.009982398711144924,
          0.01014366652816534,
          0.011478029191493988,
          0.014981011860072613,
          0.012985504232347012,
          0.015007461421191692,
          0.012812497094273567,
          0.010753141716122627,
          0.014166788198053837,
          0.016267696395516396,
          0.015970410779118538,
          0.015791580080986023,
          0.016894401982426643,
          0.013483737595379353,
          0.012541297823190689,
          0.015036255121231079,
          0.01540802139788866,
          0.013801447115838528,
          0.010804715566337109,
          0.016004567965865135,
          0.01627776399254799,
          0.015161830931901932,
          0.017550276592373848,
          0.015830403193831444,
          0.015380950644612312,
          0.012211491353809834,
          0.01687275804579258,
          0.017069285735487938,
          0.013757806271314621,
          0.018924430012702942,
          0.019756918773055077,
          0.02118491567671299,
          0.01618352346122265,
          0.020782627165317535,
          0.02340947650372982,
          0.019920840859413147,
          0.02092892676591873,
          0.016276195645332336,
          0.019673574715852737,
          0.01910588890314102,
          0.024919889867305756,
          0.025067415088415146,
          0.03254231810569763,
          0.03709037974476814,
          0.0400158166885376,
          0.039756856858730316,
          0.046799853444099426,
          0.05384105071425438,
          0.01724526286125183,
          0.01298323180526495,
          0.005388436373323202,
          0.003529398702085018,
          0.0063859689980745316,
          0.005692761391401291,
          0.009092289954423904,
          0.005608438979834318
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Layer-Head=Layer 9-Head 10<br>Checkpoint=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "Layer 9-Head 10",
         "line": {
          "color": "#ab63fa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Layer 9-Head 10",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          126000,
          127000,
          128000,
          129000,
          130000,
          131000,
          132000,
          133000,
          134000,
          135000,
          136000,
          137000,
          138000,
          139000,
          140000,
          141000,
          142000,
          143000
         ],
         "xaxis": "x",
         "y": [
          0.03222522884607315,
          0.07681969553232193,
          0.09267992526292801,
          0.050537109375,
          0.09160967916250229,
          0.06424630433320999,
          0.08639313280582428,
          0.09022432565689087,
          0.08270614594221115,
          0.09181993454694748,
          0.09710001945495605,
          0.08648547530174255,
          0.10375629365444183,
          0.09595594555139542,
          0.1080184206366539,
          0.10138272494077682,
          0.10140974074602127,
          0.09014643728733063,
          0.09361708164215088,
          0.062364812940359116,
          0.0844980850815773,
          0.09353552013635635,
          0.07908909767866135,
          0.07943552732467651,
          0.08190986514091492,
          0.07658962160348892,
          0.07329165190458298,
          0.07407847046852112,
          0.07207773625850677,
          0.07527463883161545,
          0.06588879227638245,
          0.07392334192991257,
          0.06360535323619843,
          0.04968547821044922,
          0.04612740874290466,
          0.06416758894920349,
          0.06921551376581192,
          0.07046577334403992,
          0.060766879469156265,
          0.06194257363677025,
          0.04834931716322899,
          0.06553802639245987,
          0.07097754627466202,
          0.06695930659770966,
          0.06924576312303543,
          0.067400261759758,
          0.05826321989297867,
          0.04571755602955818,
          0.06163875758647919,
          0.058743007481098175,
          0.05438004061579704,
          0.04628763347864151,
          0.05936237797141075,
          0.06313302367925644,
          0.054775454103946686,
          0.05461150407791138,
          0.0506063774228096,
          0.050201185047626495,
          0.03528834134340286,
          0.054817210882902145,
          0.05598009377717972,
          0.042144741863012314,
          0.052121851593256,
          0.053592946380376816,
          0.05531100556254387,
          0.04852185770869255,
          0.06011071056127548,
          0.05953570082783699,
          0.05484941974282265,
          0.052112944424152374,
          0.04271329194307327,
          0.05540832132101059,
          0.010691503994166851,
          0.01124979741871357,
          0.013822130858898163,
          0.007997299544513226,
          0.008184021338820457,
          0.017130574211478233,
          0.017278531566262245,
          0.02039288729429245,
          0.013443195261061192,
          0.02441134676337242,
          0.009756485000252724,
          0.024407168850302696,
          0.017793167382478714,
          0.013443860225379467,
          0.006988817825913429,
          0.01977366767823696,
          0.02635638415813446,
          0.0285998173058033,
          0.017801448702812195,
          0.02694586105644703,
          0.012630818411707878,
          0.012424803338944912,
          0.011190235614776611,
          0.023921163752675056,
          0.019014524295926094,
          0.016663946211338043,
          0.011803888715803623,
          0.01717207580804825,
          0.01147424802184105,
          0.01782493107020855,
          0.013697193004190922,
          0.021145636215806007,
          0.016838466748595238,
          0.006195883732289076,
          0.02008146233856678,
          0.016879579052329063,
          0.01805323176085949,
          0.008822017349302769,
          0.008493449538946152,
          0.009293714538216591,
          0.012265020981431007,
          0.005372397601604462,
          0.014539504423737526,
          0.0185600183904171,
          0.010052982717752457,
          0.011502386070787907,
          0.008226056583225727,
          0.009178150445222855,
          0.009944180026650429,
          0.010204619728028774,
          0.009734218940138817,
          0.02032715082168579,
          0.014304188080132008,
          0.006560435984283686,
          0.011993381194770336,
          0.019200043752789497,
          0.01598062738776207,
          0.02094927988946438,
          0.011559292674064636,
          0.010404814966022968,
          0.019887054339051247,
          0.012579450383782387,
          0.008997111581265926,
          0.009807485155761242
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "Layer-Head=Layer 9-Head 4<br>Checkpoint=%{x}<br>Value=%{y}<extra></extra>",
         "legendgroup": "Layer 9-Head 4",
         "line": {
          "color": "#FFA15A",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "Layer 9-Head 4",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          12000,
          13000,
          18000,
          23000,
          67000,
          80000,
          87000,
          89000,
          90000,
          96000,
          99000,
          100000,
          101000,
          115000,
          117000,
          123000,
          124000,
          125000,
          129000,
          130000,
          136000,
          139000
         ],
         "xaxis": "x",
         "y": [
          0.020280687138438225,
          0.0105930520221591,
          0.013085578568279743,
          0.02583824098110199,
          0.03103538230061531,
          0.02496826834976673,
          0.01818022131919861,
          0.01043331902474165,
          0.006169225089251995,
          0.006062897853553295,
          0.005714803002774715,
          0.005004238337278366,
          0.003871041350066662,
          0.003994766157120466,
          0.002384752035140991,
          0.0021040483843535185,
          0.002621949417516589,
          0.002662095008417964,
          0.0017760018818080425,
          0.0018928518984466791,
          0.0027008475735783577,
          0.0018888781778514385
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "height": 500,
        "legend": {
         "title": {
          "text": "Layer-Head"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Top Backup Heads Attribution Across Checkpoints (pythia-160m-alldropout)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Checkpoint"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Value"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"ad7c08d3-c340-433a-b5fb-8c8e1a8ade02\" class=\"plotly-graph-div\" style=\"height:500px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ad7c08d3-c340-433a-b5fb-8c8e1a8ade02\")) {                    Plotly.newPlot(                        \"ad7c08d3-c340-433a-b5fb-8c8e1a8ade02\",                        [{\"hovertemplate\":\"Layer-Head=Layer 10-Head 7\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Layer 10-Head 7\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Layer 10-Head 7\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[4000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,20000,21000,24000,25000,26000,28000,29000,30000,31000,33000,35000,55000,66000,72000,78000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,129000,130000,132000],\"xaxis\":\"x\",\"y\":[0.0026725521311163902,0.000629258924163878,0.0007657704991288483,0.0003119302273262292,0.0005408104043453932,0.0005257618031464517,0.00027662841603159904,0.00040443631587550044,0.00045996386324986815,0.0005266859079711139,0.0006991972331888974,0.0004529976285994053,0.0007804500637575984,0.0006502866744995117,0.0009810348274186254,0.0008349480340257287,0.0009809155017137527,0.000916038581635803,0.0005797024932689965,0.0003255648189224303,0.0006155213923193514,0.00020697912259493023,0.00013133237371221185,0.00025235459906980395,0.0004624385910574347,0.0008549588965252042,0.0006586548988707364,0.0004686087486334145,0.0015414286172017455,0.003221277380362153,0.00985526293516159,0.0173241775482893,0.029232485219836235,0.022043347358703613,0.028252970427274704,0.022193506360054016,0.022391492500901222,0.03811975196003914,0.03271893411874771,0.022330261766910553,0.033639296889305115,0.022757917642593384,0.02427608333528042,0.029711777344346046,0.025612127035856247,0.016769522801041603,0.022821063175797462,0.03760848566889763,0.02798982709646225,0.027296870946884155,0.037040431052446365,0.024523871019482613,0.020268121734261513,0.02189512364566326,0.021337129175662994,0.022509777918457985,0.014212765730917454,0.01839056983590126,0.01428866945207119,0.013621959835290909,0.013122176751494408,0.01007688045501709,0.01160617545247078,0.01563279703259468,0.0035480703227221966,0.011431626044213772,0.007735598832368851,0.012061845511198044,0.005630972795188427,0.004875671584159136,0.004165762569755316,0.002620133338496089,0.005015193950384855,0.00404421566054225,0.0008534201770089567,0.0004718709387816489,0.0004573932965286076,0.00028085170197300613],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Layer-Head=Layer 9-Head 9\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Layer 9-Head 9\",\"line\":{\"color\":\"#EF553B\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Layer 9-Head 9\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[7000,8000,11000,13000,14000,15000,17000,18000,19000,20000,22000,23000,24000,26000,29000,30000,31000,35000,36000,39000,40000,41000,43000,45000,49000,51000,52000,53000,55000,58000,61000,62000,63000,65000,66000,67000,68000,70000,71000,74000,75000,76000,77000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,106000,109000,110000,112000,114000,117000,118000,119000,124000,125000,128000,129000,131000,132000,134000],\"xaxis\":\"x\",\"y\":[0.0017917121294885874,0.005667125340551138,0.007299109827727079,0.008191687986254692,0.012010126374661922,0.012112344615161419,0.01314469613134861,0.012295766733586788,0.012480290606617928,0.02863914519548416,0.024729236960411072,0.021760735660791397,0.02855166792869568,0.02219049260020256,0.02279970981180668,0.019900595769286156,0.02176225557923317,0.020162083208560944,0.01848527416586876,0.019513001665472984,0.018014416098594666,0.012220167554914951,0.017404286190867424,0.017807412892580032,0.016632266342639923,0.02064628154039383,0.01906583644449711,0.019717315211892128,0.011843998916447163,0.015505749732255936,0.020441871136426926,0.017366889864206314,0.01892448030412197,0.01852373033761978,0.013165752403438091,0.019930146634578705,0.021192502230405807,0.022864865139126778,0.019892964512109756,0.023508219048380852,0.025774521753191948,0.022104782983660698,0.021632587537169456,0.02096971869468689,0.020188646391034126,0.018835877999663353,0.016392558813095093,0.011681637726724148,0.008836541324853897,0.004337572958320379,0.003690236946567893,0.003030212363228202,0.0037146529648452997,0.0023884151596575975,0.004061004612594843,0.0027726173866540194,0.0021455190144479275,0.00363927218131721,0.0034062149934470654,0.004100973252207041,0.004484336357563734,0.0040551722049713135,0.0038121941033750772,0.0022667511366307735,0.0052386196330189705,0.0036718465853482485,0.0033050274942070246,0.0027817883528769016,0.0023229457437992096,0.0021952930837869644,0.00239189644344151,0.002751966007053852,0.0016180173261091113,0.0024881938006728888,0.0016974697355180979,0.00177038146648556,0.001340522663667798,0.0009367555612698197,0.0009449610952287912,0.0009817652171477675,0.001115450169891119,0.0014319070614874363],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Layer-Head=Layer 9-Head 6\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Layer 9-Head 6\",\"line\":{\"color\":\"#00cc96\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Layer 9-Head 6\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,84000,86000,90000,92000,95000,96000,113000,115000,121000,124000,125000,129000,131000,134000],\"xaxis\":\"x\",\"y\":[0.002411766443401575,0.0033179090823978186,0.0038747189100831747,0.005100138485431671,0.0032757434528321028,0.005085034761577845,0.00363577320240438,0.004538726061582565,0.004653990268707275,0.0029530839528888464,0.005190160591155291,0.004965000320225954,0.005145061761140823,0.010231755673885345,0.0066286372020840645,0.008754000999033451,0.009256397373974323,0.011626716703176498,0.009798008017241955,0.008918024599552155,0.006602451205253601,0.009992163628339767,0.012730184011161327,0.009922172874212265,0.01246168464422226,0.012267830781638622,0.012358670122921467,0.011819890700280666,0.009157104417681694,0.011974005028605461,0.012825957499444485,0.010397862643003464,0.014318552799522877,0.012905200012028217,0.009982398711144924,0.01014366652816534,0.011478029191493988,0.014981011860072613,0.012985504232347012,0.015007461421191692,0.012812497094273567,0.010753141716122627,0.014166788198053837,0.016267696395516396,0.015970410779118538,0.015791580080986023,0.016894401982426643,0.013483737595379353,0.012541297823190689,0.015036255121231079,0.01540802139788866,0.013801447115838528,0.010804715566337109,0.016004567965865135,0.01627776399254799,0.015161830931901932,0.017550276592373848,0.015830403193831444,0.015380950644612312,0.012211491353809834,0.01687275804579258,0.017069285735487938,0.013757806271314621,0.018924430012702942,0.019756918773055077,0.02118491567671299,0.01618352346122265,0.020782627165317535,0.02340947650372982,0.019920840859413147,0.02092892676591873,0.016276195645332336,0.019673574715852737,0.01910588890314102,0.024919889867305756,0.025067415088415146,0.03254231810569763,0.03709037974476814,0.0400158166885376,0.039756856858730316,0.046799853444099426,0.05384105071425438,0.01724526286125183,0.01298323180526495,0.005388436373323202,0.003529398702085018,0.0063859689980745316,0.005692761391401291,0.009092289954423904,0.005608438979834318],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Layer-Head=Layer 9-Head 10\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Layer 9-Head 10\",\"line\":{\"color\":\"#ab63fa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Layer 9-Head 10\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000],\"xaxis\":\"x\",\"y\":[0.03222522884607315,0.07681969553232193,0.09267992526292801,0.050537109375,0.09160967916250229,0.06424630433320999,0.08639313280582428,0.09022432565689087,0.08270614594221115,0.09181993454694748,0.09710001945495605,0.08648547530174255,0.10375629365444183,0.09595594555139542,0.1080184206366539,0.10138272494077682,0.10140974074602127,0.09014643728733063,0.09361708164215088,0.062364812940359116,0.0844980850815773,0.09353552013635635,0.07908909767866135,0.07943552732467651,0.08190986514091492,0.07658962160348892,0.07329165190458298,0.07407847046852112,0.07207773625850677,0.07527463883161545,0.06588879227638245,0.07392334192991257,0.06360535323619843,0.04968547821044922,0.04612740874290466,0.06416758894920349,0.06921551376581192,0.07046577334403992,0.060766879469156265,0.06194257363677025,0.04834931716322899,0.06553802639245987,0.07097754627466202,0.06695930659770966,0.06924576312303543,0.067400261759758,0.05826321989297867,0.04571755602955818,0.06163875758647919,0.058743007481098175,0.05438004061579704,0.04628763347864151,0.05936237797141075,0.06313302367925644,0.054775454103946686,0.05461150407791138,0.0506063774228096,0.050201185047626495,0.03528834134340286,0.054817210882902145,0.05598009377717972,0.042144741863012314,0.052121851593256,0.053592946380376816,0.05531100556254387,0.04852185770869255,0.06011071056127548,0.05953570082783699,0.05484941974282265,0.052112944424152374,0.04271329194307327,0.05540832132101059,0.010691503994166851,0.01124979741871357,0.013822130858898163,0.007997299544513226,0.008184021338820457,0.017130574211478233,0.017278531566262245,0.02039288729429245,0.013443195261061192,0.02441134676337242,0.009756485000252724,0.024407168850302696,0.017793167382478714,0.013443860225379467,0.006988817825913429,0.01977366767823696,0.02635638415813446,0.0285998173058033,0.017801448702812195,0.02694586105644703,0.012630818411707878,0.012424803338944912,0.011190235614776611,0.023921163752675056,0.019014524295926094,0.016663946211338043,0.011803888715803623,0.01717207580804825,0.01147424802184105,0.01782493107020855,0.013697193004190922,0.021145636215806007,0.016838466748595238,0.006195883732289076,0.02008146233856678,0.016879579052329063,0.01805323176085949,0.008822017349302769,0.008493449538946152,0.009293714538216591,0.012265020981431007,0.005372397601604462,0.014539504423737526,0.0185600183904171,0.010052982717752457,0.011502386070787907,0.008226056583225727,0.009178150445222855,0.009944180026650429,0.010204619728028774,0.009734218940138817,0.02032715082168579,0.014304188080132008,0.006560435984283686,0.011993381194770336,0.019200043752789497,0.01598062738776207,0.02094927988946438,0.011559292674064636,0.010404814966022968,0.019887054339051247,0.012579450383782387,0.008997111581265926,0.009807485155761242],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"Layer-Head=Layer 9-Head 4\\u003cbr\\u003eCheckpoint=%{x}\\u003cbr\\u003eValue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Layer 9-Head 4\",\"line\":{\"color\":\"#FFA15A\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"Layer 9-Head 4\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[12000,13000,18000,23000,67000,80000,87000,89000,90000,96000,99000,100000,101000,115000,117000,123000,124000,125000,129000,130000,136000,139000],\"xaxis\":\"x\",\"y\":[0.020280687138438225,0.0105930520221591,0.013085578568279743,0.02583824098110199,0.03103538230061531,0.02496826834976673,0.01818022131919861,0.01043331902474165,0.006169225089251995,0.006062897853553295,0.005714803002774715,0.005004238337278366,0.003871041350066662,0.003994766157120466,0.002384752035140991,0.0021040483843535185,0.002621949417516589,0.002662095008417964,0.0017760018818080425,0.0018928518984466791,0.0027008475735783577,0.0018888781778514385],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Checkpoint\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Value\"}},\"legend\":{\"title\":{\"text\":\"Layer-Head\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Top Backup Heads Attribution Across Checkpoints (pythia-160m-alldropout)\"},\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ad7c08d3-c340-433a-b5fb-8c8e1a8ade02');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_backup_heads = plot_top_heads(model_name=MODEL_TO_VIEW, checkpoint_dict=per_head_logit_diff_deltas, cumulative_nmhs=cumulative_nmhs, top_k_per_checkpoint=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Head: %{x}<br>Layer: %{y}<br>Logit diff attribution: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           -0.00031765131279826164,
           0.3156936466693878,
           0.0032243020832538605,
           0.0043573444709181786,
           0.02283836156129837,
           0.1909862607717514,
           0.015091687440872192,
           0.03277648985385895,
           0.0030522309243679047,
           0.0035090791061520576,
           0.000053514144383370876,
           -0.00011852476745843887
          ],
          [
           -0.00014347699470818043,
           0.027818620204925537,
           -0.0041803522035479546,
           0.0003439248539507389,
           0.000005809706635773182,
           -0.001769278198480606,
           -0.0033143162727355957,
           0.039005570113658905,
           0.003304571844637394,
           -0.0014683082699775696,
           0.007542278617620468,
           -0.002905469387769699
          ],
          [
           -0.0010985229164361954,
           0.0011500654509291053,
           -0.0004440473858267069,
           0.00000995676964521408,
           -0.003775139572098851,
           0.02032054215669632,
           -0.0019474942237138748,
           -0.0006243046373128891,
           0.0016780085861682892,
           -0.00171709805727005,
           0.01802472025156021,
           -0.0001716803526505828
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorbar": {
          "title": {
           "text": "Logit diff attribution"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "margin": {
         "l": 100,
         "r": 100
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Headwise logit diff contribution, post NMH KO"
        },
        "width": 600,
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "scaleanchor": "y",
         "showline": true,
         "title": {
          "text": "Head"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "linecolor": "black",
         "linewidth": 1,
         "mirror": true,
         "showline": true,
         "title": {
          "text": "Layer"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"ff1a0e17-38be-4c43-99f6-37b628aadbba\" class=\"plotly-graph-div\" style=\"height:525px; width:600px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ff1a0e17-38be-4c43-99f6-37b628aadbba\")) {                    Plotly.newPlot(                        \"ff1a0e17-38be-4c43-99f6-37b628aadbba\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[-0.00031765131279826164,0.3156936466693878,0.0032243020832538605,0.0043573444709181786,0.02283836156129837,0.1909862607717514,0.015091687440872192,0.03277648985385895,0.0030522309243679047,0.0035090791061520576,5.3514144383370876e-05,-0.00011852476745843887],[-0.00014347699470818043,0.027818620204925537,-0.0041803522035479546,0.0003439248539507389,5.809706635773182e-06,-0.001769278198480606,-0.0033143162727355957,0.039005570113658905,0.003304571844637394,-0.0014683082699775696,0.007542278617620468,-0.002905469387769699],[-0.0010985229164361954,0.0011500654509291053,-0.0004440473858267069,9.95676964521408e-06,-0.003775139572098851,0.02032054215669632,-0.0019474942237138748,-0.0006243046373128891,0.0016780085861682892,-0.00171709805727005,0.01802472025156021,-0.0001716803526505828]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}\\u003cbr\\u003eLayer: %{y}\\u003cbr\\u003eLogit diff attribution: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"},\"showline\":true,\"linewidth\":1,\"linecolor\":\"black\",\"mirror\":true},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Logit diff attribution\"}},\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Headwise logit diff contribution, post NMH KO\"},\"width\":600,\"margin\":{\"r\":100,\"l\":100}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ff1a0e17-38be-4c43-99f6-37b628aadbba');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#per_head_logit_diff_deltas\n",
    "\n",
    "imshow_p(\n",
    "    experiment_metrics[143000]['per_head_logit_diff_delta'], #[143000],\n",
    "    title=\"Headwise logit diff contribution, post NMH KO\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Logit diff attribution\"},\n",
    "    #coloraxis=dict(colorbar_ticksuffix = \"%\"),\n",
    "    border=True,\n",
    "    width=600,\n",
    "    margin={\"r\": 100, \"l\": 100}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['logit_diff', 'per_head_logit_diffs', 'ablation_targets', 'ablated_logit_diff', 'per_head_ablated_logit_diffs', 'per_head_logit_diff_delta', 'in_circuit_head_delta', 'outside_circuit_head_delta', 'summed_in_circuit_head_delta', 'summed_outside_circuit_head_delta', 'summed_total_head_delta'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_metrics[143000].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Checkpoint</th>\n",
       "      <th>Layer-Head</th>\n",
       "      <th>Layer</th>\n",
       "      <th>Head</th>\n",
       "      <th>Value</th>\n",
       "      <th>Previous NMH</th>\n",
       "      <th>Checkpoint_sum</th>\n",
       "      <th>Value_sum</th>\n",
       "      <th>Previous NMH_sum</th>\n",
       "      <th>Top K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>20000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>25000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>30000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006404</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>35000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.014621</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>37000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013440</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>40000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010845</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>45000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035425</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>46000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033058</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>47000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.014968</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>48000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.008163</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>52000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.020150</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>61000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.015234</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>62000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>63000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007319</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>64000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005687</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>65000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>66000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004789</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>66000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>68000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003615</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>69000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005792</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>70000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>71000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005183</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>72000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003973</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>73000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>74000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003834</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>75000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005873</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>76000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>77000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>78000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>79000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>83000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>84000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>86000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>89000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>90000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>94000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>106000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>107000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.007569</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>109000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>110000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005541</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>111000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.008061</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>113000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>114000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>116000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>121000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.008774</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>122000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.006190</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>123000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.008291</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>125000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>130000</td>\n",
       "      <td>Layer 9-Head 4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.004050</td>\n",
       "      <td>True</td>\n",
       "      <td>3135000</td>\n",
       "      <td>0.334368</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>131000</td>\n",
       "      <td>Layer 10-Head 7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.007888</td>\n",
       "      <td>True</td>\n",
       "      <td>2216000</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Checkpoint       Layer-Head  Layer  Head     Value  Previous NMH  \\\n",
       "72        20000   Layer 9-Head 4      9     4  0.002460          True   \n",
       "97        25000   Layer 9-Head 4      9     4  0.006947          True   \n",
       "121       30000   Layer 9-Head 4      9     4  0.006404          True   \n",
       "149       35000   Layer 9-Head 4      9     4  0.014621          True   \n",
       "156       37000   Layer 9-Head 4      9     4  0.013440          True   \n",
       "171       40000   Layer 9-Head 4      9     4  0.010845          True   \n",
       "197       45000   Layer 9-Head 4      9     4  0.035425          True   \n",
       "202       46000   Layer 9-Head 4      9     4  0.033058          True   \n",
       "207       47000   Layer 9-Head 4      9     4  0.014968          True   \n",
       "212       48000   Layer 9-Head 4      9     4  0.008163          True   \n",
       "234       52000   Layer 9-Head 4      9     4  0.020150          True   \n",
       "276       61000   Layer 9-Head 4      9     4  0.015234          True   \n",
       "283       62000   Layer 9-Head 4      9     4  0.007663          True   \n",
       "287       63000   Layer 9-Head 4      9     4  0.007319          True   \n",
       "291       64000   Layer 9-Head 4      9     4  0.005687          True   \n",
       "295       65000   Layer 9-Head 4      9     4  0.007628          True   \n",
       "300       66000   Layer 9-Head 4      9     4  0.004789          True   \n",
       "301       66000  Layer 10-Head 7     10     7  0.005050          True   \n",
       "310       68000  Layer 10-Head 7     10     7  0.003615          True   \n",
       "316       69000   Layer 9-Head 4      9     4  0.005792          True   \n",
       "320       70000   Layer 9-Head 4      9     4  0.002369          True   \n",
       "326       71000   Layer 9-Head 4      9     4  0.005183          True   \n",
       "330       72000  Layer 10-Head 7     10     7  0.003973          True   \n",
       "335       73000   Layer 9-Head 4      9     4  0.002946          True   \n",
       "340       74000   Layer 9-Head 4      9     4  0.003834          True   \n",
       "345       75000   Layer 9-Head 4      9     4  0.005873          True   \n",
       "351       76000  Layer 10-Head 7     10     7  0.003486          True   \n",
       "355       77000  Layer 10-Head 7     10     7  0.003142          True   \n",
       "360       78000  Layer 10-Head 7     10     7  0.003221          True   \n",
       "368       79000  Layer 10-Head 7     10     7  0.001927          True   \n",
       "385       83000  Layer 10-Head 7     10     7  0.002465          True   \n",
       "390       84000  Layer 10-Head 7     10     7  0.002987          True   \n",
       "400       86000  Layer 10-Head 7     10     7  0.003382          True   \n",
       "415       89000  Layer 10-Head 7     10     7  0.003476          True   \n",
       "420       90000  Layer 10-Head 7     10     7  0.003540          True   \n",
       "440       94000  Layer 10-Head 7     10     7  0.004098          True   \n",
       "500      106000   Layer 9-Head 4      9     4  0.003578          True   \n",
       "506      107000  Layer 10-Head 7     10     7  0.007569          True   \n",
       "515      109000   Layer 9-Head 4      9     4  0.005699          True   \n",
       "520      110000   Layer 9-Head 4      9     4  0.005541          True   \n",
       "526      111000  Layer 10-Head 7     10     7  0.008061          True   \n",
       "536      113000   Layer 9-Head 4      9     4  0.005804          True   \n",
       "544      114000   Layer 9-Head 4      9     4  0.006457          True   \n",
       "550      116000   Layer 9-Head 4      9     4  0.004402          True   \n",
       "576      121000   Layer 9-Head 4      9     4  0.008774          True   \n",
       "580      122000   Layer 9-Head 4      9     4  0.006190          True   \n",
       "588      123000  Layer 10-Head 7     10     7  0.008291          True   \n",
       "596      125000   Layer 9-Head 4      9     4  0.003753          True   \n",
       "621      130000   Layer 9-Head 4      9     4  0.004050          True   \n",
       "629      131000  Layer 10-Head 7     10     7  0.007888          True   \n",
       "\n",
       "     Checkpoint_sum  Value_sum  Previous NMH_sum  Top K  \n",
       "72          3135000   0.334368                38   True  \n",
       "97          3135000   0.334368                38   True  \n",
       "121         3135000   0.334368                38   True  \n",
       "149         3135000   0.334368                38   True  \n",
       "156         3135000   0.334368                38   True  \n",
       "171         3135000   0.334368                38   True  \n",
       "197         3135000   0.334368                38   True  \n",
       "202         3135000   0.334368                38   True  \n",
       "207         3135000   0.334368                38   True  \n",
       "212         3135000   0.334368                38   True  \n",
       "234         3135000   0.334368                38   True  \n",
       "276         3135000   0.334368                38   True  \n",
       "283         3135000   0.334368                38   True  \n",
       "287         3135000   0.334368                38   True  \n",
       "291         3135000   0.334368                38   True  \n",
       "295         3135000   0.334368                38   True  \n",
       "300         3135000   0.334368                38   True  \n",
       "301         2216000   0.121681                22  False  \n",
       "310         2216000   0.121681                22  False  \n",
       "316         3135000   0.334368                38   True  \n",
       "320         3135000   0.334368                38   True  \n",
       "326         3135000   0.334368                38   True  \n",
       "330         2216000   0.121681                22  False  \n",
       "335         3135000   0.334368                38   True  \n",
       "340         3135000   0.334368                38   True  \n",
       "345         3135000   0.334368                38   True  \n",
       "351         2216000   0.121681                22  False  \n",
       "355         2216000   0.121681                22  False  \n",
       "360         2216000   0.121681                22  False  \n",
       "368         2216000   0.121681                22  False  \n",
       "385         2216000   0.121681                22  False  \n",
       "390         2216000   0.121681                22  False  \n",
       "400         2216000   0.121681                22  False  \n",
       "415         2216000   0.121681                22  False  \n",
       "420         2216000   0.121681                22  False  \n",
       "440         2216000   0.121681                22  False  \n",
       "500         3135000   0.334368                38   True  \n",
       "506         2216000   0.121681                22  False  \n",
       "515         3135000   0.334368                38   True  \n",
       "520         3135000   0.334368                38   True  \n",
       "526         2216000   0.121681                22  False  \n",
       "536         3135000   0.334368                38   True  \n",
       "544         3135000   0.334368                38   True  \n",
       "550         3135000   0.334368                38   True  \n",
       "576         3135000   0.334368                38   True  \n",
       "580         3135000   0.334368                38   True  \n",
       "588         2216000   0.121681                22  False  \n",
       "596         3135000   0.334368                38   True  \n",
       "621         3135000   0.334368                38   True  \n",
       "629         2216000   0.121681                22  False  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_backup_heads[top_backup_heads['Previous NMH']==True].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4000: set(),\n",
       " 5000: set(),\n",
       " 6000: set(),\n",
       " 7000: set(),\n",
       " 8000: {(8, 2)},\n",
       " 9000: set(),\n",
       " 10000: {(8, 1), (8, 10)},\n",
       " 11000: {(10, 7)},\n",
       " 12000: {(8, 2), (10, 7)},\n",
       " 13000: {(10, 7)},\n",
       " 14000: {(8, 2), (10, 7)},\n",
       " 15000: {(8, 2), (10, 7)},\n",
       " 16000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 17000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 18000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 19000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 20000: {(8, 2), (10, 7)},\n",
       " 21000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 22000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 23000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 24000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 25000: {(8, 2), (10, 7)},\n",
       " 26000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 27000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 28000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 29000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 30000: {(8, 2), (10, 7)},\n",
       " 31000: {(8, 1), (8, 2), (9, 4), (10, 7)},\n",
       " 32000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 33000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 34000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 35000: {(8, 2), (8, 10), (10, 7)},\n",
       " 36000: {(8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 37000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 38000: {(8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 39000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 40000: {(8, 2), (8, 10), (10, 7)},\n",
       " 41000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 42000: {(8, 2), (9, 4), (10, 7)},\n",
       " 43000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 44000: {(8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 45000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 46000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 47000: {(8, 2), (8, 10), (10, 7)},\n",
       " 48000: {(8, 2), (8, 10), (10, 7)},\n",
       " 49000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 50000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 51000: {(8, 2), (9, 4), (10, 7)},\n",
       " 52000: {(8, 2), (8, 10)},\n",
       " 53000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 54000: {(8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 55000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 56000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 57000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 58000: {(8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 59000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 60000: {(8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 61000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 62000: {(8, 2), (8, 10), (10, 7)},\n",
       " 63000: {(8, 2), (8, 10), (10, 7)},\n",
       " 64000: {(8, 2), (8, 10), (10, 7)},\n",
       " 65000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 66000: {(8, 2), (8, 10)},\n",
       " 67000: {(8, 2), (8, 10), (10, 7)},\n",
       " 68000: {(8, 2), (8, 10)},\n",
       " 69000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 70000: {(8, 2), (8, 10), (10, 7)},\n",
       " 71000: {(8, 2), (8, 10), (10, 7)},\n",
       " 72000: {(8, 2), (8, 10)},\n",
       " 73000: {(8, 2), (8, 10), (10, 7)},\n",
       " 74000: {(8, 2), (8, 10)},\n",
       " 75000: {(8, 1), (8, 2), (8, 10)},\n",
       " 76000: {(8, 2), (8, 10)},\n",
       " 77000: {(8, 1), (8, 2), (8, 10)},\n",
       " 78000: {(8, 2), (8, 10)},\n",
       " 79000: {(8, 2), (8, 10)},\n",
       " 80000: {(8, 2), (8, 10)},\n",
       " 81000: {(8, 2), (8, 10)},\n",
       " 82000: {(8, 2), (8, 10)},\n",
       " 83000: {(8, 2), (8, 10)},\n",
       " 84000: {(8, 2), (8, 10)},\n",
       " 85000: {(8, 2), (8, 10)},\n",
       " 86000: {(8, 1), (8, 2), (8, 10)},\n",
       " 87000: {(8, 1), (8, 2), (8, 10)},\n",
       " 88000: {(8, 2), (8, 10)},\n",
       " 89000: {(8, 10)},\n",
       " 90000: {(8, 2), (8, 10)},\n",
       " 91000: {(8, 1), (8, 2), (8, 10)},\n",
       " 92000: {(8, 1), (8, 2), (8, 10)},\n",
       " 93000: {(8, 2), (8, 10)},\n",
       " 94000: {(8, 2), (8, 10)},\n",
       " 95000: {(8, 2), (8, 10)},\n",
       " 96000: {(8, 2), (8, 10)},\n",
       " 97000: {(8, 2), (8, 10)},\n",
       " 98000: {(8, 1), (8, 2), (8, 10)},\n",
       " 99000: {(8, 1), (8, 2), (8, 10)},\n",
       " 100000: {(8, 2), (8, 10)},\n",
       " 101000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 102000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 103000: {(8, 2), (8, 10)},\n",
       " 104000: {(8, 2), (8, 10)},\n",
       " 105000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 106000: {(8, 1), (8, 10), (10, 7)},\n",
       " 107000: {(8, 2), (8, 10)},\n",
       " 108000: {(8, 2), (8, 10)},\n",
       " 109000: {(8, 2), (8, 10)},\n",
       " 110000: {(8, 2), (8, 10), (10, 7)},\n",
       " 111000: {(8, 2), (8, 10)},\n",
       " 112000: {(5, 0), (8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 113000: {(8, 1), (8, 10), (10, 7)},\n",
       " 114000: {(8, 2), (8, 10)},\n",
       " 115000: {(8, 10)},\n",
       " 116000: {(8, 2), (8, 10), (10, 7)},\n",
       " 117000: {(8, 10)},\n",
       " 118000: {(8, 2), (8, 10), (10, 7)},\n",
       " 119000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 120000: {(8, 2), (8, 10), (10, 7)},\n",
       " 121000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 122000: {(5, 0), (8, 2), (8, 10), (10, 7)},\n",
       " 123000: {(8, 2), (8, 10)},\n",
       " 124000: {(5, 0), (8, 2), (8, 10), (10, 7)},\n",
       " 125000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 126000: {(5, 0), (8, 2), (8, 10)},\n",
       " 127000: {(8, 2), (8, 10)},\n",
       " 128000: {(8, 2), (8, 10)},\n",
       " 129000: {(8, 2), (8, 10)},\n",
       " 130000: {(8, 2), (8, 10)},\n",
       " 131000: {(8, 2), (8, 10)},\n",
       " 132000: {(5, 0), (8, 2), (8, 10)},\n",
       " 133000: set(),\n",
       " 134000: {(8, 2), (8, 10)},\n",
       " 135000: {(8, 2), (8, 10)},\n",
       " 136000: {(8, 2), (8, 10)},\n",
       " 137000: {(5, 0), (8, 2), (8, 10)},\n",
       " 138000: {(5, 0), (8, 2), (8, 10)},\n",
       " 139000: {(8, 2), (8, 10)},\n",
       " 140000: {(8, 2), (8, 10)},\n",
       " 141000: {(8, 2), (8, 10)},\n",
       " 142000: {(5, 0), (8, 2), (8, 10)},\n",
       " 143000: {(8, 2), (8, 10)}}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_nmhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4000: set(),\n",
       " 5000: set(),\n",
       " 6000: set(),\n",
       " 7000: set(),\n",
       " 8000: {(8, 2)},\n",
       " 9000: {(8, 2)},\n",
       " 10000: {(8, 1), (8, 2), (8, 10)},\n",
       " 11000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 12000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 13000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 14000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 15000: {(8, 1), (8, 2), (8, 10), (10, 7)},\n",
       " 16000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 17000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 18000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 19000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 20000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 21000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 22000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 23000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 24000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 25000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 26000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 27000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 28000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 29000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 30000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 31000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 32000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 33000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 34000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 35000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 36000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 37000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 38000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 39000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 40000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 41000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 42000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 43000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 44000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 45000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 46000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 47000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 48000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 49000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 50000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 51000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 52000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 53000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 54000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 55000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 56000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 57000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 58000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 59000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 60000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 61000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 62000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 63000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 64000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 65000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 66000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 67000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 68000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 69000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 70000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 71000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 72000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 73000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 74000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 75000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 76000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 77000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 78000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 79000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 80000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 81000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 82000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 83000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 84000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 85000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 86000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 87000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 88000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 89000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 90000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 91000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 92000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 93000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 94000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 95000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 96000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 97000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 98000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 99000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 100000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 101000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 102000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 103000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 104000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 105000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 106000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 107000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 108000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 109000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 110000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 111000: {(8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 112000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 113000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 114000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 115000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 116000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 117000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 118000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 119000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 120000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 121000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 122000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 123000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 124000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 125000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 126000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 127000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 128000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 129000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 130000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 131000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 132000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 133000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 134000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 135000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 136000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 137000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 138000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 139000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 140000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 141000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 142000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)},\n",
       " 143000: {(5, 0), (8, 1), (8, 2), (8, 10), (9, 4), (10, 7)}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative_nmhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Checkpoint=%{x}<br>Number of NMHs=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          4000,
          5000,
          6000,
          7000,
          8000,
          9000,
          10000,
          11000,
          12000,
          13000,
          14000,
          15000,
          16000,
          17000,
          18000,
          19000,
          20000,
          21000,
          22000,
          23000,
          24000,
          25000,
          26000,
          27000,
          28000,
          29000,
          30000,
          31000,
          32000,
          33000,
          34000,
          35000,
          36000,
          37000,
          38000,
          39000,
          40000,
          41000,
          42000,
          43000,
          44000,
          45000,
          46000,
          47000,
          48000,
          49000,
          50000,
          51000,
          52000,
          53000,
          54000,
          55000,
          56000,
          57000,
          58000,
          59000,
          60000,
          61000,
          62000,
          63000,
          64000,
          65000,
          66000,
          67000,
          68000,
          69000,
          70000,
          71000,
          72000,
          73000,
          74000,
          75000,
          76000,
          77000,
          78000,
          79000,
          80000,
          81000,
          82000,
          83000,
          84000,
          85000,
          86000,
          87000,
          88000,
          89000,
          90000,
          91000,
          92000,
          93000,
          94000,
          95000,
          96000,
          97000,
          98000,
          99000,
          100000,
          101000,
          102000,
          103000,
          104000,
          105000,
          106000,
          107000,
          108000,
          109000,
          110000,
          111000,
          112000,
          113000,
          114000,
          115000,
          116000,
          117000,
          118000,
          119000,
          120000,
          121000,
          122000,
          123000,
          124000,
          125000,
          126000,
          127000,
          128000,
          129000,
          130000,
          131000,
          132000,
          133000,
          134000,
          135000,
          136000,
          137000,
          138000,
          139000,
          140000,
          141000,
          142000,
          143000
         ],
         "xaxis": "x",
         "y": [
          0,
          0,
          0,
          0,
          1,
          0,
          2,
          1,
          2,
          1,
          2,
          2,
          5,
          5,
          5,
          5,
          2,
          5,
          5,
          5,
          5,
          2,
          5,
          5,
          5,
          5,
          2,
          4,
          5,
          5,
          5,
          3,
          4,
          4,
          4,
          5,
          3,
          5,
          3,
          5,
          4,
          4,
          4,
          3,
          3,
          5,
          5,
          3,
          2,
          5,
          4,
          5,
          5,
          5,
          4,
          5,
          4,
          4,
          3,
          3,
          3,
          4,
          2,
          3,
          2,
          4,
          3,
          3,
          2,
          3,
          2,
          3,
          2,
          3,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          2,
          1,
          2,
          3,
          3,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          2,
          4,
          4,
          2,
          2,
          5,
          3,
          2,
          2,
          2,
          3,
          2,
          5,
          3,
          2,
          1,
          3,
          1,
          3,
          4,
          3,
          4,
          4,
          2,
          4,
          4,
          3,
          2,
          2,
          2,
          2,
          2,
          3,
          0,
          2,
          2,
          2,
          3,
          3,
          2,
          2,
          2,
          3,
          2
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Number of NMHs Over Time (pythia-160m)"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Checkpoint"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Number of NMHs"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"ac1fd460-59e7-451f-812d-e6e46a4d6776\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ac1fd460-59e7-451f-812d-e6e46a4d6776\")) {                    Plotly.newPlot(                        \"ac1fd460-59e7-451f-812d-e6e46a4d6776\",                        [{\"hovertemplate\":\"Checkpoint=%{x}\\u003cbr\\u003eNumber of NMHs=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[4000,5000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000],\"xaxis\":\"x\",\"y\":[0,0,0,0,1,0,2,1,2,1,2,2,5,5,5,5,2,5,5,5,5,2,5,5,5,5,2,4,5,5,5,3,4,4,4,5,3,5,3,5,4,4,4,3,3,5,5,3,2,5,4,5,5,5,4,5,4,4,3,3,3,4,2,3,2,4,3,3,2,3,2,3,2,3,2,2,2,2,2,2,2,2,3,3,2,1,2,3,3,2,2,2,2,2,3,3,2,4,4,2,2,5,3,2,2,2,3,2,5,3,2,1,3,1,3,4,3,4,4,2,4,4,3,2,2,2,2,2,3,0,2,2,2,3,3,2,2,2,3,2],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Checkpoint\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Number of NMHs\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Number of NMHs Over Time (pythia-160m)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ac1fd460-59e7-451f-812d-e6e46a4d6776');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot number of nmhs over time\n",
    "fig = px.line(\n",
    "    x=list(checkpoint_nmhs.keys()), \n",
    "    y=list([len(heads) for heads in checkpoint_nmhs.values()]), \n",
    "    title=f\"Number of NMHs Over Time ({MODEL_TO_VIEW})\",\n",
    "    labels={'x': 'Checkpoint', 'y': 'Number of NMHs'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
