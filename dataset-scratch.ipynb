{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39/1711627342.py:5: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_39/1711627342.py:6: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "ipython = get_ipython()\n",
    "ipython.magic(\"load_ext autoreload\")\n",
    "ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional, Union, Dict, Tuple\n",
    "from pathlib import Path \n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import circuitsvis as cv\n",
    "\n",
    "import transformer_lens.utils as tl_utils\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens.patching as patching\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "import plotly.express as px\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "\n",
    "from path_patching_cm.path_patching import Node, IterNode, path_patch, act_patch\n",
    "from path_patching_cm.ioi_dataset import IOIDataset, NAMES\n",
    "from neel_plotly import imshow as imshow_n\n",
    "\n",
    "from utils.visualization import imshow_p, plot_attention_heads, plot_attention\n",
    "from utils.data_utils import generate_data_and_caches, UniversalPatchingDataset\n",
    "from utils.metrics import compute_logit_diff, compute_probability_diff, compute_probability_mass, compute_rank_0_rate\n",
    "from utils.visualization_utils import (\n",
    "    plot_attention_heads,\n",
    "    scatter_attention_and_contribution,\n",
    "    get_attn_head_patterns\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_cache/pythia-160m-data-seed3/step9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-160m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from utils.model_utils import load_model\n",
    "model = load_model(\"EleutherAI/pythia-160m-data-seed3\",\"EleutherAI/pythia-160m\", \"step9000\", \"model_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('After Patrick and Peter went to the station, Patrick gave a snack to',\n",
       " ' Peter')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = model.to_string(ioi_dataset.toks.to(device)[32][:-7])\n",
    "test_answer = \" Peter\"\n",
    "test_input, test_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'After', ' Patrick', ' and', ' Peter', ' went', ' to', ' the', ' station', ',', ' Patrick', ' gave', ' a', ' snack', ' to']\n",
      "Tokenized answer: [' Peter']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.53</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">63.70</span><span style=\"font-weight: bold\">% Token: | Peter|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m17.53\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m63.70\u001b[0m\u001b[1m% Token: | Peter|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 17.53 Prob: 63.70% Token: | Peter|\n",
      "Top 1th token. Logit: 15.44 Prob:  7.91% Token: | the|\n",
      "Top 2th token. Logit: 14.67 Prob:  3.65% Token: | his|\n",
      "Top 3th token. Logit: 14.67 Prob:  3.64% Token: | a|\n",
      "Top 4th token. Logit: 13.64 Prob:  1.31% Token: | one|\n",
      "Top 5th token. Logit: 13.38 Prob:  1.00% Token: | them|\n",
      "Top 6th token. Logit: 13.13 Prob:  0.79% Token: | him|\n",
      "Top 7th token. Logit: 13.06 Prob:  0.73% Token: | their|\n",
      "Top 8th token. Logit: 12.92 Prob:  0.64% Token: | two|\n",
      "Top 9th token. Logit: 12.78 Prob:  0.55% Token: | Pete|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Peter'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Peter'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "test_prompt(test_input, test_answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mtest_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0manswer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprepend_space_to_answer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[bool]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprint_details\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[bool]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprepend_bos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[bool]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[int]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'None'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Test if the Model Can Give the Correct Answer to a Prompt.\n",
      "\n",
      "Intended for exploratory analysis. Prints out the performance on the answer (rank, logit, prob),\n",
      "as well as the top k tokens. Works for multi-token prompts and multi-token answers.\n",
      "\n",
      "Warning:\n",
      "\n",
      "This will print the results (it does not return them).\n",
      "\n",
      "Examples:\n",
      "\n",
      ">>> from transformer_lens import HookedTransformer, utils\n",
      ">>> model = HookedTransformer.from_pretrained(\"tiny-stories-1M\")\n",
      "Loaded pretrained model tiny-stories-1M into HookedTransformer\n",
      "\n",
      ">>> prompt = \"Why did the elephant cross the\"\n",
      ">>> answer = \"road\"\n",
      ">>> utils.test_prompt(prompt, answer, model)\n",
      "Tokenized prompt: ['<|endoftext|>', 'Why', ' did', ' the', ' elephant', ' cross', ' the']\n",
      "Tokenized answer: [' road']\n",
      "Performance on answer token:\n",
      "Rank: 2        Logit: 14.24 Prob:  3.51% Token: | road|\n",
      "Top 0th token. Logit: 14.51 Prob:  4.59% Token: | ground|\n",
      "Top 1th token. Logit: 14.41 Prob:  4.18% Token: | tree|\n",
      "Top 2th token. Logit: 14.24 Prob:  3.51% Token: | road|\n",
      "Top 3th token. Logit: 14.22 Prob:  3.45% Token: | car|\n",
      "Top 4th token. Logit: 13.92 Prob:  2.55% Token: | river|\n",
      "Top 5th token. Logit: 13.79 Prob:  2.25% Token: | street|\n",
      "Top 6th token. Logit: 13.77 Prob:  2.21% Token: | k|\n",
      "Top 7th token. Logit: 13.75 Prob:  2.16% Token: | hill|\n",
      "Top 8th token. Logit: 13.64 Prob:  1.92% Token: | swing|\n",
      "Top 9th token. Logit: 13.46 Prob:  1.61% Token: | park|\n",
      "Ranks of the answer tokens: [(' road', 2)]\n",
      "\n",
      "Args:\n",
      "    prompt:\n",
      "        The prompt string, e.g. \"Why did the elephant cross the\".\n",
      "    answer:\n",
      "        The answer, e.g. \"road\". Note that if you set prepend_space_to_answer to False, you need\n",
      "        to think about if you have a space before the answer here (as e.g. in this example the\n",
      "        answer may really be \" road\" if the prompt ends without a trailing space).\n",
      "    model:\n",
      "        The model.\n",
      "    prepend_space_to_answer:\n",
      "        Whether or not to prepend a space to the answer. Note this will only ever prepend a\n",
      "        space if the answer doesn't already start with one.\n",
      "    print_details:\n",
      "        Print the prompt (as a string but broken up by token), answer and top k tokens (all\n",
      "        with logit, rank and probability).\n",
      "    prepend_bos:\n",
      "        Overrides self.cfg.default_prepend_bos if set. Whether to prepend\n",
      "        the BOS token to the input (applicable when input is a string). Models generally learn\n",
      "        to use the BOS token as a resting place for attention heads (i.e. a way for them to be\n",
      "        \"turned off\"). This therefore often improves performance slightly.\n",
      "    top_k:\n",
      "        Top k tokens to print details of (when print_details is set to True).\n",
      "\n",
      "Returns:\n",
      "    None (just prints the results directly).\n",
      "\u001b[0;31mFile:\u001b[0m      /workspace/transformer_lens/transformer_lens/utils.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "test_prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f549ea8610499587e46fd889e7c6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafc37fc2c7049c68b9a98383e71220e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951da39492a9459d8ee474c9648b3de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d34e95322b4177b25d47515da72341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d774dccf10450bb6678158cd15b0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-160m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"EleutherAI/pythia-160m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=False,\n",
    ")\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_logits(\n",
    "        logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "        positions: Float[Tensor, \"batch\"] = None\n",
    ")-> Float[Tensor, \"batch d_vocab\"]:\n",
    "    \"\"\"Gets the logits at the provided positions. If no positions are provided, the final logits are returned.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits to use.\n",
    "        positions (torch.Tensor): Positions to get logits at. This should be a tensor of shape (batch_size,).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Logits at the provided positions.\n",
    "    \"\"\"\n",
    "    if positions is None:\n",
    "        return logits[:, -1, :]\n",
    "    \n",
    "    return logits[range(logits.size(0)), positions, :]\n",
    "\n",
    "\n",
    "def compute_logit_diff(\n",
    "        logits: Float[Tensor, \"batch seq d_vocab\"], \n",
    "        answer_token_indices: Float[Tensor, \"batch num_answers\"],\n",
    "        positions: Float[Tensor, \"batch\"] = None,\n",
    "        flags_tensor: torch.Tensor = None,\n",
    "        per_prompt=False,\n",
    "        mode=\"simple\"\n",
    ")-> Float[Tensor, \"batch num_answers\"]:\n",
    "    \"\"\"Computes the difference between a correct and incorrect logit (or mean of a group of logits) for each item in the batch.\n",
    "\n",
    "    Takes the full logits, and the indices of the tokens to compare. These indices can be of multiple types as follows:\n",
    "\n",
    "    - Simple: The tensor should be of shape (batch_size, 2), where the first index in the third dimension is the correct token index,\n",
    "        and the second index is the incorrect token index.\n",
    "\n",
    "    - Pairs: In this mode, answer_token_indices is a 3D tensor of shape (batch, num_pairs, 2). For each pair, you'll need to compute \n",
    "             the difference between the logits at the two indices, then average these differences across each pair for every batch item.\n",
    "\n",
    "    - Groups: Here, answer_token_indices is also a 3D tensor of shape (batch, num_tokens, 2). The third dimension indicates group membership \n",
    "              (correct or incorrect). The mean logits for each group are calculated and then subtracted from each other.\n",
    "              \n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits to use.\n",
    "        answer_token_indices (torch.Tensor): Indices of the tokens to compare.\n",
    "        positions (torch.Tensor): Positions to get logits at. Should be one position per batch item.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Difference between the logits of the provided tokens.\n",
    "    \"\"\"\n",
    "    logits = get_positional_logits(logits, positions)\n",
    "    \n",
    "    # Mode 1: Simple\n",
    "    if mode == \"simple\":\n",
    "        correct_logits = logits[torch.arange(logits.size(0)), answer_token_indices[:, 0]]\n",
    "        incorrect_logits = logits[torch.arange(logits.size(0)), answer_token_indices[:, 1]]\n",
    "        logit_diff = correct_logits - incorrect_logits\n",
    "\n",
    "    # Mode 2: Pairs\n",
    "    elif mode == \"pairs\":\n",
    "        pair_diffs = logits[torch.arange(logits.size(0))[:, None], answer_token_indices[..., 0]] - \\\n",
    "                     logits[torch.arange(logits.size(0))[:, None], answer_token_indices[..., 1]]\n",
    "        logit_diff = pair_diffs.mean(dim=1)\n",
    "\n",
    "    # Mode 3: Groups\n",
    "    elif mode == \"groups\":\n",
    "        assert flags_tensor is not None\n",
    "        logit_diff = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_logits = logits[i, answer_token_indices[i]]\n",
    "\n",
    "            # Calculate the logit difference using the correct/incorrect flags\n",
    "            correct_logits = selected_logits[flags_tensor[i] == 1]\n",
    "            incorrect_logits = selected_logits[flags_tensor[i] == -1]\n",
    "\n",
    "            # Handle cases where there are no correct or incorrect logits\n",
    "            if len(correct_logits) > 0:\n",
    "                correct_mean = correct_logits.mean()\n",
    "            else:\n",
    "                correct_mean = 0\n",
    "\n",
    "            if len(incorrect_logits) > 0:\n",
    "                incorrect_mean = incorrect_logits.mean()\n",
    "            else:\n",
    "                incorrect_mean = 0\n",
    "\n",
    "            logit_diff[i] = correct_mean - incorrect_mean\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified\")\n",
    "\n",
    "    return logit_diff.mean() if not per_prompt else logit_diff\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_probability_diff(\n",
    "        logits: torch.Tensor, \n",
    "        answer_token_indices: torch.Tensor,\n",
    "        positions: torch.Tensor = None,\n",
    "        flags_tensor: torch.Tensor = None,\n",
    "        per_prompt=False,\n",
    "        mode=\"simple\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Computes the difference between probability of a correct and incorrect logit (or mean of a group of logits) for each item in the batch.\n",
    "\n",
    "    Takes the full logits, and the indices of the tokens to compare. These indices can be of multiple types as follows:\n",
    "\n",
    "    - Simple: The tensor should be of shape (batch_size, 2), where the first index in the third dimension is the correct token index,\n",
    "        and the second index is the incorrect token index.\n",
    "\n",
    "    - Pairs: In this mode, answer_token_indices is a 3D tensor of shape (batch, num_pairs, 2). For each pair, you'll need to compute \n",
    "             the difference between the probabilities at the two indices, then average these differences across each pair for every batch item.\n",
    "\n",
    "    - Groups: Here, answer_token_indices is also a 3D tensor of shape (batch, num_tokens, 2). The third dimension indicates group membership \n",
    "              (correct or incorrect). The mean probabilities for each group are calculated and then subtracted from each other.\n",
    "              \n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits to use.\n",
    "        answer_token_indices (torch.Tensor): Indices of the tokens to compare.\n",
    "        positions (torch.Tensor): Positions to get logits at. Should be one position per batch item.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Difference between the logits of the provided tokens.\n",
    "    \"\"\"\n",
    "    logits = get_positional_logits(logits, positions)\n",
    "    probabilities = torch.softmax(logits, dim=-1)  # Applying softmax to logits\n",
    "    print(f\"probabilities={probabilities.shape}\")\n",
    "\n",
    "    # Mode 1: Simple\n",
    "    if mode == \"simple\":\n",
    "        correct_probs = probabilities[torch.arange(logits.size(0)), answer_token_indices[:, 0]]\n",
    "        incorrect_probs = probabilities[torch.arange(logits.size(0)), answer_token_indices[:, 1]]\n",
    "        prob_diff = correct_probs - incorrect_probs\n",
    "\n",
    "    # Mode 2: Pairs\n",
    "    elif mode == \"pairs\":\n",
    "        pair_diffs = probabilities[torch.arange(logits.size(0))[:, None], answer_token_indices[..., 0]] - \\\n",
    "                     probabilities[torch.arange(logits.size(0))[:, None], answer_token_indices[..., 1]]\n",
    "        prob_diff = pair_diffs.mean(dim=1)\n",
    "\n",
    "    # Mode 3: Groups\n",
    "    elif mode == \"groups\":\n",
    "        # Initialize tensors to store the probability differences for each batch item\n",
    "        assert flags_tensor is not None\n",
    "        prob_diff = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            # Select the probabilities for the token IDs of this batch item\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "\n",
    "            # Calculate the probability difference using the correct/incorrect flags\n",
    "            correct_probs = selected_probs[flags_tensor[i] == 1]\n",
    "            incorrect_probs = selected_probs[flags_tensor[i] == -1]\n",
    "\n",
    "            # Handle cases where there are no correct or incorrect tokens\n",
    "            if len(correct_probs) > 0:\n",
    "                correct_mean = correct_probs.mean()\n",
    "            else:\n",
    "                correct_mean = 0\n",
    "\n",
    "            if len(incorrect_probs) > 0:\n",
    "                incorrect_mean = incorrect_probs.mean()\n",
    "            else:\n",
    "                incorrect_mean = 0\n",
    "\n",
    "            prob_diff[i] = correct_mean - incorrect_mean\n",
    "\n",
    "    # Mode 4: Group Sum\n",
    "    elif mode == \"group_sum\":\n",
    "        assert flags_tensor is not None\n",
    "        prob_diff = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "\n",
    "            # Calculate the sum of probabilities using the correct/incorrect flags\n",
    "            correct_sum = selected_probs[flags_tensor[i] == 1].sum()\n",
    "            incorrect_sum = selected_probs[flags_tensor[i] == -1].sum()\n",
    "\n",
    "            prob_diff[i] = incorrect_sum - correct_sum\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified\")\n",
    "\n",
    "    return prob_diff.mean() if not per_prompt else prob_diff\n",
    "\n",
    "\n",
    "def compute_probability_mass(\n",
    "        logits: torch.Tensor, \n",
    "        answer_token_indices: torch.Tensor,\n",
    "        positions: torch.Tensor = None,\n",
    "        flags_tensor: torch.Tensor = None,\n",
    "        group=\"correct\",\n",
    "        mode=\"simple\"\n",
    ") -> torch.Tensor:\n",
    "    logits = get_positional_logits(logits, positions)\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    # Determine the flag value based on the specified group\n",
    "    flag_value = 1 if group == \"correct\" else -1\n",
    "\n",
    "    # Mode logic\n",
    "    if mode == \"simple\":\n",
    "        selected_indices = answer_token_indices[:, 0] if group == \"correct\" else answer_token_indices[:, 1]\n",
    "        group_probs = probabilities[torch.arange(logits.size(0)), selected_indices]\n",
    "\n",
    "    elif mode == \"pairs\":\n",
    "        group_probs = torch.zeros(logits.size(0), device=logits.device)\n",
    "        for i in range(logits.size(0)):\n",
    "            for pair in answer_token_indices[i]:\n",
    "                selected_index = pair[0] if group == \"correct\" else pair[1]\n",
    "                group_probs[i] += probabilities[i, selected_index]\n",
    "            group_probs[i] /= answer_token_indices.size(1)\n",
    "\n",
    "    elif mode == \"groups\":\n",
    "        assert flags_tensor is not None\n",
    "        group_probs = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "            group_probs[i] = selected_probs[flags_tensor[i] == flag_value].mean()\n",
    "\n",
    "    elif mode == \"group_sum\":\n",
    "        assert flags_tensor is not None\n",
    "        group_probs = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "            group_probs[i] = selected_probs[flags_tensor[i] == flag_value].sum()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified\")\n",
    "\n",
    "    return group_probs.mean()\n",
    "\n",
    "\n",
    "\n",
    "def compute_rank_0_rate(\n",
    "        logits: torch.Tensor, \n",
    "        answer_token_indices: torch.Tensor,\n",
    "        positions: torch.Tensor = None,\n",
    "        flags_tensor: torch.Tensor = None,\n",
    "        group=\"correct\",\n",
    "        mode=\"simple\"\n",
    ") -> torch.Tensor:\n",
    "    logits = get_positional_logits(logits, positions)\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    # Mode logic\n",
    "    if mode == \"simple\":\n",
    "        top_rank_indices = probabilities.argmax(dim=-1)\n",
    "        correct_indices = answer_token_indices[:, 0] if group == \"correct\" else answer_token_indices[:, 1]\n",
    "        rank_0_rate = (top_rank_indices == correct_indices).float().mean()\n",
    "\n",
    "    elif mode == \"pairs\":\n",
    "        rank_0_rate = torch.zeros(logits.size(0), device=logits.device)\n",
    "        for i in range(logits.size(0)):\n",
    "            for pair in answer_token_indices[i]:\n",
    "                top_rank_index = probabilities[i].argmax()\n",
    "                correct_index = pair[0] if group == \"correct\" else pair[1]\n",
    "                rank_0_rate[i] += (top_rank_index == correct_index).float()\n",
    "            rank_0_rate[i] /= answer_token_indices.size(1)\n",
    "\n",
    "    elif mode == \"groups\":\n",
    "        assert flags_tensor is not None\n",
    "        rank_0_rate = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "            top_rank_id = selected_probs.argmax()\n",
    "            rank_0_rate[i] = (flags_tensor[i, top_rank_id] == 1).float() if group == \"correct\" else \\\n",
    "                             (flags_tensor[i, top_rank_id] == -1).float()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified\")\n",
    "\n",
    "    return rank_0_rate.mean()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _logits_to_mean_logit_diff(logits: Float[Tensor, \"batch seq d_vocab\"], ioi_dataset: IOIDataset, per_prompt=False):\n",
    "    '''\n",
    "    Returns logit difference between the correct and incorrect answer.\n",
    "\n",
    "    If per_prompt=True, return the array of differences rather than the average.\n",
    "    '''\n",
    "\n",
    "    # Only the final logits are relevant for the answer\n",
    "    # Get the logits corresponding to the indirect object / subject tokens respectively\n",
    "    io_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), ioi_dataset.word_idx[\"end\"], ioi_dataset.io_tokenIDs]\n",
    "    print(io_logits.shape)\n",
    "    s_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), ioi_dataset.word_idx[\"end\"], ioi_dataset.s_tokenIDs]\n",
    "    # Find logit difference\n",
    "    answer_logit_diff = io_logits - s_logits\n",
    "    return answer_logit_diff if per_prompt else answer_logit_diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average logit diff (IOI dataset): 2.1560\n",
      "Average logit diff (ABC dataset): -2.3650\n"
     ]
    }
   ],
   "source": [
    "N = 70\n",
    "ioi_dataset, abc_dataset, _, _, _ = generate_data_and_caches(model, N, verbose=True)\n",
    "clean_toks = ioi_dataset.toks.to(device)\n",
    "corrupted_toks = abc_dataset.toks.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1210,  0.6408,  4.3679,  3.2872,  3.1052,  2.4821,  4.4465,  0.3192,\n",
       "         2.3799, -0.6708,  2.6662,  3.7824,  0.8591,  0.4677,  0.2200,  3.3067,\n",
       "         5.4179,  0.1047,  2.9370,  0.1339,  2.9071,  3.4575,  2.6687,  5.2737,\n",
       "         1.8654,  0.7553,  0.5263,  4.9929,  1.6752,  0.7784,  2.5545,  1.4186,\n",
       "         3.3981, -0.6675,  1.2263, -1.0020,  0.8154,  5.7192,  3.0224,  0.3226,\n",
       "         6.5352,  4.1738,  1.3393,  2.3986,  4.4888,  2.1893,  5.5038,  0.3437,\n",
       "        -0.7437,  4.7308,  0.9047,  2.9857,  2.0731,  3.2680,  4.3207,  3.0555,\n",
       "         2.4662,  3.7404,  0.7805,  2.3680,  2.7400,  1.5318,  0.1822,  0.9666,\n",
       "         0.9466,  0.9064, -0.6956,  0.6441,  2.0869,  2.6091], device='cuda:0')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(clean_toks)\n",
    "_logits_to_mean_logit_diff(logits, ioi_dataset, per_prompt=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average logit diff (IOI dataset): 4.1336\n",
      "Average logit diff (ABC dataset): -4.0758\n"
     ]
    }
   ],
   "source": [
    "ds = UniversalDataset.from_ioi(model, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(ds.toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.3623,  4.5626,  4.4264,  5.5464,  4.9847,  3.3949,  6.6630,  4.6075,\n",
       "         4.1188,  0.7323,  3.5184,  5.4281, -1.2406,  3.1855,  3.9544,  4.9033,\n",
       "         4.9604, -0.5812,  6.0159,  4.4829,  4.9364,  3.5120,  3.6026,  5.8260,\n",
       "         1.5506,  3.6591,  5.1734,  7.8747,  1.9717,  3.0304,  1.3404,  5.6763,\n",
       "         6.4933,  3.3304,  5.4253,  3.1553,  1.6709,  5.3956,  3.8436,  1.4698,\n",
       "         7.1663,  7.1676,  6.1948,  5.9164,  6.3674,  5.1585,  6.8696,  3.2112,\n",
       "         0.9600,  5.3084,  2.2612,  4.1867,  2.8197,  6.8392,  6.7878,  4.9921,\n",
       "         4.7495,  4.8143,  3.2216,  5.5183,  0.8875,  4.1779,  3.3880,  3.5489,\n",
       "         0.9434,  4.0729,  3.7018,  2.4539,  7.2850,  2.4210], device='cuda:0')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_logit_diff(logits, ds.answer_toks, ds.positions, per_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities=torch.Size([70, 50304])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3522,  0.1489,  0.1367,  0.2285,  0.1738,  0.2953,  0.3568,  0.4153,\n",
       "         0.2671,  0.0405,  0.6666,  0.4521, -0.0517,  0.2696,  0.2368,  0.2904,\n",
       "         0.0940, -0.0523,  0.4264,  0.4820,  0.2183,  0.2214,  0.2022,  0.1831,\n",
       "         0.1875,  0.0674,  0.4456,  0.2823,  0.1970,  0.0493,  0.3325,  0.2131,\n",
       "         0.7092,  0.1755,  0.3654,  0.1416,  0.0983,  0.6070,  0.1499,  0.1279,\n",
       "         0.7195,  0.4302,  0.1586,  0.2769,  0.2814,  0.1193,  0.5471,  0.1647,\n",
       "         0.0067,  0.2246,  0.0681,  0.2199,  0.6249,  0.3559,  0.3219,  0.2941,\n",
       "         0.2657,  0.1763,  0.3662,  0.1637,  0.1565,  0.1751,  0.1243,  0.4158,\n",
       "         0.0082,  0.3939,  0.2468,  0.2145,  0.3334,  0.2217], device='cuda:0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_probability_diff(logits, ds.answer_toks, ds.positions, per_prompt=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greater-Than"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.greater_than_dataset import get_prob_diff, YearDataset, get_valid_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_old = YearDataset(get_valid_years(model.tokenizer, 1100, 1800), 1000, Path(\"data/potential_nouns.txt\"), model.tokenizer)\n",
    "\n",
    "# def batch(iterable, n:int=1):\n",
    "#    current_batch = []\n",
    "#    for item in iterable:\n",
    "#        current_batch.append(item)\n",
    "#        if len(current_batch) == n:\n",
    "#            yield current_batch\n",
    "#            current_batch = []\n",
    "#    if current_batch:\n",
    "#        yield current_batch\n",
    "\n",
    "# clean = list(batch(ds.good_sentences, 9))\n",
    "# labels = list(batch(ds.years_YY, 9))\n",
    "# corrupted = list(batch(ds.bad_sentences, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 768\n",
    "#model.to_str_tokens(ds.good_toks[IDX]), model.to_str_tokens(ds.bad_toks[IDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def prepare_indices_for_prob_diff(tokenizer, years):\n",
    "    \"\"\"\n",
    "    Prepares two tensors for use with the compute_probability_diff function in 'groups' mode.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): Tokenizer to convert years to token indices.\n",
    "        years (torch.Tensor): Tensor containing the year for each prompt in the batch.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor, torch.Tensor: Two tensors, one for token IDs and one for correct/incorrect flags.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the indices for years 00 to 99\n",
    "    year_indices = get_year_indices(tokenizer)  # Tensor of size 100 with token IDs for years\n",
    "\n",
    "    # Prepare tensors to store token IDs and correct/incorrect flags\n",
    "    token_ids_tensor = year_indices.repeat(years.size(0), 1)  # Repeat the year_indices for each batch item\n",
    "    flags_tensor = torch.zeros_like(token_ids_tensor)  # Initialize the flags tensor with zeros\n",
    "\n",
    "    for i, year in enumerate(years):\n",
    "        # Mark years greater than the given year as correct (1)\n",
    "        flags_tensor[i, year + 1:] = 1\n",
    "        # Mark years less than or equal to the given year as incorrect (-1)\n",
    "        flags_tensor[i, :year + 1] = -1\n",
    "\n",
    "    return token_ids_tensor, flags_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_length = 1 + len(model.tokenizer(ds.good_sentences[0])[0])\n",
    "prob_diff = get_prob_diff(model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.circuit_utils import run_with_batches\n",
    "\n",
    "clean_logits = run_with_batches(model, ds_old.good_toks.to(device), batch_size=20, max_seq_len=12)\n",
    "corrupted_logits = run_with_batches(model, ds_old.bad_toks.to(device), batch_size=20, max_seq_len=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.8363, device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_diff(clean_logits,ds_old.years_YY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = UniversalDataset.from_greater_than(model, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(ds.toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities=torch.Size([1000, 50304])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.8294, device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_probability_diff(clean_logits, ds.answer_toks, flags_tensor=ds.group_flags, mode=\"group_sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_diff = compute_logit_diff(logits=clean_logits, answer_token_indices=answer_tokens, flags_tensor=group_flags, mode=\"groups\")\n",
    "probability_diff = compute_probability_diff(logits=clean_logits, answer_token_indices=answer_tokens, flags_tensor=group_flags, mode=\"group_sum\")\n",
    "probability_mass = compute_probability_mass(logits=clean_logits, answer_token_indices=answer_tokens, flags_tensor=group_flags, mode=\"groups\", group=\"correct\")\n",
    "rank_0_rate = compute_rank_0_rate(logits=clean_logits, answer_token_indices=answer_tokens, flags_tensor=group_flags, mode=\"groups\", group=\"correct\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.sentiment_datasets import get_dataset, PromptType, get_prompts\n",
    "from utils.circuit_analysis import get_logit_diff as get_logit_diff_ca"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = PromptType.CLASSIFICATION_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading prompts from config and filtering\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I thought this movie was amazing, I loved it. The acting was awesome, the plot was beautiful, and overall the movie was just very good. Review Sentiment:'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = get_dataset(model, device, prompt_type=ds_type)\n",
    "ds.all_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 35])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.clean_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 1, 2])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.answer_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_logits = model(ds.clean_tokens.to(device))\n",
    "corrupted_logits = model(ds.corrupted_tokens.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 1, 2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.answer_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import CircuitMetric\n",
    "logit_diff_metric = CircuitMetric(\"logit_diff_multi\", partial(get_logit_diff_ca, answer_tokens=ds.answer_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3842, device='cuda:0'), tensor(-0.3842, device='cuda:0'))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_diff_metric(clean_logits), logit_diff_metric(corrupted_logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading prompts from config and filtering\n"
     ]
    }
   ],
   "source": [
    "ds = UniversalDataset.from_sentiment(model, \"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(ds.toks)\n",
    "flipped_logits = model(ds.flipped_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3842, device='cuda:0'), tensor(-0.3842, device='cuda:0'))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_logit_diff(logits, ds.answer_toks, mode=\"pairs\"), compute_logit_diff(flipped_logits, ds.answer_toks, mode=\"pairs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = PromptType.COMPLETION_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading prompts from config and filtering\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I thought this movie was amazing, I loved it. The acting was awesome, the plot was beautiful, and overall it was just very'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = get_dataset(model, device, prompt_type=ds_type)\n",
    "ds.all_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 28])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.clean_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 5, 2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.answer_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_logits = model(ds.clean_tokens.to(device))\n",
    "corrupted_logits = model(ds.corrupted_tokens.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 5, 2])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.answer_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import CircuitMetric\n",
    "logit_diff_metric = CircuitMetric(\"logit_diff_multi\", partial(get_logit_diff_ca, answer_tokens=ds.answer_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.2004, device='cuda:0'), tensor(-3.2004, device='cuda:0'))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_diff_metric(clean_logits), logit_diff_metric(corrupted_logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading prompts from config and filtering\n"
     ]
    }
   ],
   "source": [
    "ds = UniversalPatchingDataset.from_sentiment(model, \"cont\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.7608, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " tensor(-1.7608, device='cuda:0', grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_step_9000 = model(ds.toks)\n",
    "compute_logit_diff(logits_step_9000, ds.answer_toks, mode=\"pairs\"), compute_logit_diff(flipped_logits, ds.answer_toks, mode=\"pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818723678589"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.metrics import compute_accuracy\n",
    "compute_accuracy(logits, ds.answer_toks, mode=\"pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_cache/pythia-160m-data-seed3/step8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-160m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model2 = load_model(\"EleutherAI/pythia-160m-data-seed3\",\"EleutherAI/pythia-160m\", \"step8000\", \"model_cache\")\n",
    "logits_step_8000 = model2(ds.toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2106, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_logit_diff(logits_step_8000, ds.answer_toks, mode=\"pairs\", per_prompt=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7608, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_logit_diff(logits_step_9000, ds.answer_toks, mode=\"pairs\", per_prompt=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
