{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_89/1711627342.py:5: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_89/1711627342.py:6: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "ipython = get_ipython()\n",
    "ipython.magic(\"load_ext autoreload\")\n",
    "ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional, Union, Dict, Tuple\n",
    "from pathlib import Path \n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import circuitsvis as cv\n",
    "\n",
    "import transformer_lens.utils as tl_utils\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens.patching as patching\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "import plotly.express as px\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "\n",
    "from path_patching_cm.path_patching import Node, IterNode, path_patch, act_patch\n",
    "from path_patching_cm.ioi_dataset import IOIDataset, NAMES\n",
    "from neel_plotly import imshow as imshow_n\n",
    "\n",
    "from utils.visualization import imshow_p, plot_attention_heads, plot_attention\n",
    "from utils.data_utils import generate_data_and_caches\n",
    "from utils.visualization_utils import (\n",
    "    plot_attention_heads,\n",
    "    scatter_attention_and_contribution,\n",
    "    get_attn_head_patterns\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4d97f075e641ccabc557ff5b07af71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f403db06d3c046438be5cbba486e1849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e40d5c40e540a984f43fa9d46cfbd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa55aef3c0545b18e34d99833a71029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e27b1bc62a4898a46fff12d24a3b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-160m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"EleutherAI/pythia-160m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=False,\n",
    ")\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_logits(\n",
    "        logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "        positions: Float[Tensor, \"batch\"] = None\n",
    ")-> Float[Tensor, \"batch d_vocab\"]:\n",
    "    \"\"\"Gets the logits at the provided positions. If no positions are provided, the final logits are returned.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits to use.\n",
    "        positions (torch.Tensor): Positions to get logits at. This should be a tensor of shape (batch_size,).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Logits at the provided positions.\n",
    "    \"\"\"\n",
    "    if positions is None:\n",
    "        return logits[:, -1, :]\n",
    "    \n",
    "    return logits[range(logits.size(0)), positions, :]\n",
    "\n",
    "\n",
    "def compute_logit_diff(\n",
    "        logits: Float[Tensor, \"batch seq d_vocab\"], \n",
    "        answer_token_indices: Float[Tensor, \"batch num_answers\"],\n",
    "        positions: Float[Tensor, \"batch\"] = None,\n",
    "        flags_tensor: torch.Tensor = None,\n",
    "        per_prompt=False,\n",
    "        mode=\"simple\"\n",
    ")-> Float[Tensor, \"batch num_answers\"]:\n",
    "    \"\"\"Computes the difference between a correct and incorrect logit (or mean of a group of logits) for each item in the batch.\n",
    "\n",
    "    Takes the full logits, and the indices of the tokens to compare. These indices can be of multiple types as follows:\n",
    "\n",
    "    - Simple: The tensor should be of shape (batch_size, 2), where the first index in the third dimension is the correct token index,\n",
    "        and the second index is the incorrect token index.\n",
    "\n",
    "    - Pairs: In this mode, answer_token_indices is a 3D tensor of shape (batch, num_pairs, 2). For each pair, you'll need to compute \n",
    "             the difference between the logits at the two indices, then average these differences across each pair for every batch item.\n",
    "\n",
    "    - Groups: Here, answer_token_indices is also a 3D tensor of shape (batch, num_tokens, 2). The third dimension indicates group membership \n",
    "              (correct or incorrect). The mean logits for each group are calculated and then subtracted from each other.\n",
    "              \n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits to use.\n",
    "        answer_token_indices (torch.Tensor): Indices of the tokens to compare.\n",
    "        positions (torch.Tensor): Positions to get logits at. Should be one position per batch item.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Difference between the logits of the provided tokens.\n",
    "    \"\"\"\n",
    "    logits = get_positional_logits(logits, positions)\n",
    "    \n",
    "    # Mode 1: Simple\n",
    "    if mode == \"simple\":\n",
    "        correct_logits = logits[torch.arange(logits.size(0)), answer_token_indices[:, 0]]\n",
    "        incorrect_logits = logits[torch.arange(logits.size(0)), answer_token_indices[:, 1]]\n",
    "        logit_diff = correct_logits - incorrect_logits\n",
    "\n",
    "    # Mode 2: Pairs\n",
    "    elif mode == \"pairs\":\n",
    "        pair_diffs = logits[torch.arange(logits.size(0))[:, None], answer_token_indices[..., 0]] - \\\n",
    "                     logits[torch.arange(logits.size(0))[:, None], answer_token_indices[..., 1]]\n",
    "        logit_diff = pair_diffs.mean(dim=1)\n",
    "\n",
    "    # Mode 3: Groups\n",
    "    elif mode == \"groups\":\n",
    "        assert flags_tensor is not None\n",
    "        logit_diff = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_logits = logits[i, answer_token_indices[i]]\n",
    "\n",
    "            # Calculate the logit difference using the correct/incorrect flags\n",
    "            correct_logits = selected_logits[flags_tensor[i] == 1]\n",
    "            incorrect_logits = selected_logits[flags_tensor[i] == -1]\n",
    "\n",
    "            # Handle cases where there are no correct or incorrect logits\n",
    "            if len(correct_logits) > 0:\n",
    "                correct_mean = correct_logits.mean()\n",
    "            else:\n",
    "                correct_mean = 0\n",
    "\n",
    "            if len(incorrect_logits) > 0:\n",
    "                incorrect_mean = incorrect_logits.mean()\n",
    "            else:\n",
    "                incorrect_mean = 0\n",
    "\n",
    "            logit_diff[i] = correct_mean - incorrect_mean\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified\")\n",
    "\n",
    "    return logit_diff.mean() if not per_prompt else logit_diff\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_probability_diff(\n",
    "        logits: torch.Tensor, \n",
    "        answer_token_indices: torch.Tensor,\n",
    "        positions: torch.Tensor = None,\n",
    "        flags_tensor: torch.Tensor = None,\n",
    "        per_prompt=False,\n",
    "        mode=\"simple\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Computes the difference between probability of a correct and incorrect logit (or mean of a group of logits) for each item in the batch.\n",
    "\n",
    "    Takes the full logits, and the indices of the tokens to compare. These indices can be of multiple types as follows:\n",
    "\n",
    "    - Simple: The tensor should be of shape (batch_size, 2), where the first index in the third dimension is the correct token index,\n",
    "        and the second index is the incorrect token index.\n",
    "\n",
    "    - Pairs: In this mode, answer_token_indices is a 3D tensor of shape (batch, num_pairs, 2). For each pair, you'll need to compute \n",
    "             the difference between the probabilities at the two indices, then average these differences across each pair for every batch item.\n",
    "\n",
    "    - Groups: Here, answer_token_indices is also a 3D tensor of shape (batch, num_tokens, 2). The third dimension indicates group membership \n",
    "              (correct or incorrect). The mean probabilities for each group are calculated and then subtracted from each other.\n",
    "              \n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits to use.\n",
    "        answer_token_indices (torch.Tensor): Indices of the tokens to compare.\n",
    "        positions (torch.Tensor): Positions to get logits at. Should be one position per batch item.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Difference between the logits of the provided tokens.\n",
    "    \"\"\"\n",
    "    logits = get_positional_logits(logits, positions)\n",
    "    probabilities = torch.softmax(logits, dim=-1)  # Applying softmax to logits\n",
    "    print(f\"probabilities={probabilities.shape}\")\n",
    "\n",
    "    # Mode 1: Simple\n",
    "    if mode == \"simple\":\n",
    "        correct_probs = probabilities[torch.arange(logits.size(0)), answer_token_indices[:, 0]]\n",
    "        incorrect_probs = probabilities[torch.arange(logits.size(0)), answer_token_indices[:, 1]]\n",
    "        prob_diff = correct_probs - incorrect_probs\n",
    "\n",
    "    # Mode 2: Pairs\n",
    "    elif mode == \"pairs\":\n",
    "        pair_diffs = probabilities[torch.arange(logits.size(0))[:, None], answer_token_indices[..., 0]] - \\\n",
    "                     probabilities[torch.arange(logits.size(0))[:, None], answer_token_indices[..., 1]]\n",
    "        prob_diff = pair_diffs.mean(dim=1)\n",
    "\n",
    "    # Mode 3: Groups\n",
    "    elif mode == \"groups\":\n",
    "        # Initialize tensors to store the probability differences for each batch item\n",
    "        assert flags_tensor is not None\n",
    "        prob_diff = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            # Select the probabilities for the token IDs of this batch item\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "\n",
    "            # Calculate the probability difference using the correct/incorrect flags\n",
    "            correct_probs = selected_probs[flags_tensor[i] == 1]\n",
    "            incorrect_probs = selected_probs[flags_tensor[i] == -1]\n",
    "\n",
    "            # Handle cases where there are no correct or incorrect tokens\n",
    "            if len(correct_probs) > 0:\n",
    "                correct_mean = correct_probs.mean()\n",
    "            else:\n",
    "                correct_mean = 0\n",
    "\n",
    "            if len(incorrect_probs) > 0:\n",
    "                incorrect_mean = incorrect_probs.mean()\n",
    "            else:\n",
    "                incorrect_mean = 0\n",
    "\n",
    "            prob_diff[i] = correct_mean - incorrect_mean\n",
    "\n",
    "    # Mode 4: Group Sum\n",
    "    elif mode == \"group_sum\":\n",
    "        assert flags_tensor is not None\n",
    "        prob_diff = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "\n",
    "            # Calculate the sum of probabilities using the correct/incorrect flags\n",
    "            correct_sum = selected_probs[flags_tensor[i] == 1].sum()\n",
    "            incorrect_sum = selected_probs[flags_tensor[i] == -1].sum()\n",
    "\n",
    "            prob_diff[i] = correct_sum - incorrect_sum\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified\")\n",
    "\n",
    "    return prob_diff.mean() if not per_prompt else prob_diff\n",
    "\n",
    "\n",
    "def compute_probability_mass(\n",
    "        logits: torch.Tensor, \n",
    "        answer_token_indices: torch.Tensor,\n",
    "        positions: torch.Tensor = None,\n",
    "        flags_tensor: torch.Tensor = None,\n",
    "        group=\"correct\",\n",
    "        mode=\"simple\"\n",
    ") -> torch.Tensor:\n",
    "    logits = get_positional_logits(logits, positions)\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    # Determine the flag value based on the specified group\n",
    "    flag_value = 1 if group == \"correct\" else -1\n",
    "\n",
    "    # Mode logic\n",
    "    if mode == \"simple\":\n",
    "        selected_indices = answer_token_indices[:, 0] if group == \"correct\" else answer_token_indices[:, 1]\n",
    "        group_probs = probabilities[torch.arange(logits.size(0)), selected_indices]\n",
    "\n",
    "    elif mode == \"pairs\":\n",
    "        group_probs = torch.zeros(logits.size(0), device=logits.device)\n",
    "        for i in range(logits.size(0)):\n",
    "            for pair in answer_token_indices[i]:\n",
    "                selected_index = pair[0] if group == \"correct\" else pair[1]\n",
    "                group_probs[i] += probabilities[i, selected_index]\n",
    "            group_probs[i] /= answer_token_indices.size(1)\n",
    "\n",
    "    elif mode == \"groups\":\n",
    "        assert flags_tensor is not None\n",
    "        group_probs = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "            group_probs[i] = selected_probs[flags_tensor[i] == flag_value].mean()\n",
    "\n",
    "    elif mode == \"group_sum\":\n",
    "        assert flags_tensor is not None\n",
    "        group_probs = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "            group_probs[i] = selected_probs[flags_tensor[i] == flag_value].sum()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified\")\n",
    "\n",
    "    return group_probs.mean()\n",
    "\n",
    "\n",
    "\n",
    "def compute_rank_0_rate(\n",
    "        logits: torch.Tensor, \n",
    "        answer_token_indices: torch.Tensor,\n",
    "        positions: torch.Tensor = None,\n",
    "        flags_tensor: torch.Tensor = None,\n",
    "        group=\"correct\",\n",
    "        mode=\"simple\"\n",
    ") -> torch.Tensor:\n",
    "    logits = get_positional_logits(logits, positions)\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    # Mode logic\n",
    "    if mode == \"simple\":\n",
    "        top_rank_indices = probabilities.argmax(dim=-1)\n",
    "        correct_indices = answer_token_indices[:, 0] if group == \"correct\" else answer_token_indices[:, 1]\n",
    "        rank_0_rate = (top_rank_indices == correct_indices).float().mean()\n",
    "\n",
    "    elif mode == \"pairs\":\n",
    "        rank_0_rate = torch.zeros(logits.size(0), device=logits.device)\n",
    "        for i in range(logits.size(0)):\n",
    "            for pair in answer_token_indices[i]:\n",
    "                top_rank_index = probabilities[i].argmax()\n",
    "                correct_index = pair[0] if group == \"correct\" else pair[1]\n",
    "                rank_0_rate[i] += (top_rank_index == correct_index).float()\n",
    "            rank_0_rate[i] /= answer_token_indices.size(1)\n",
    "\n",
    "    elif mode == \"groups\":\n",
    "        assert flags_tensor is not None\n",
    "        rank_0_rate = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "            top_rank_id = selected_probs.argmax()\n",
    "            rank_0_rate[i] = (flags_tensor[i, top_rank_id] == 1).float() if group == \"correct\" else \\\n",
    "                             (flags_tensor[i, top_rank_id] == -1).float()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified\")\n",
    "\n",
    "    return rank_0_rate.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities=torch.Size([1000, 50304])\n"
     ]
    }
   ],
   "source": [
    "logit_diff = compute_logit_diff(logits=clean_logits, answer_token_indices=answer_tokens, flags_tensor=group_flags, mode=\"groups\")\n",
    "probability_diff = compute_probability_diff(logits=clean_logits, answer_token_indices=answer_tokens, flags_tensor=group_flags, mode=\"group_sum\")\n",
    "probability_mass = compute_probability_mass(logits=clean_logits, answer_token_indices=answer_tokens, flags_tensor=group_flags, mode=\"group_sum\", group=\"correct\")\n",
    "rank_0_rate = compute_rank_0_rate(logits=clean_logits, answer_token_indices=answer_tokens, flags_tensor=group_flags, mode=\"groups\", group=\"correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.7268, device='cuda:0'),\n",
       " tensor(0.8294, device='cuda:0'),\n",
       " tensor(0.9136, device='cuda:0'),\n",
       " tensor(0.9960, device='cuda:0'))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_diff, probability_diff, probability_mass, rank_0_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 12, 50304])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_logits.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _logits_to_mean_logit_diff(logits: Float[Tensor, \"batch seq d_vocab\"], ioi_dataset: IOIDataset, per_prompt=False):\n",
    "    '''\n",
    "    Returns logit difference between the correct and incorrect answer.\n",
    "\n",
    "    If per_prompt=True, return the array of differences rather than the average.\n",
    "    '''\n",
    "\n",
    "    # Only the final logits are relevant for the answer\n",
    "    # Get the logits corresponding to the indirect object / subject tokens respectively\n",
    "    io_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), ioi_dataset.word_idx[\"end\"], ioi_dataset.io_tokenIDs]\n",
    "    print(io_logits.shape)\n",
    "    s_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), ioi_dataset.word_idx[\"end\"], ioi_dataset.s_tokenIDs]\n",
    "    # Find logit difference\n",
    "    answer_logit_diff = io_logits - s_logits\n",
    "    return answer_logit_diff if per_prompt else answer_logit_diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average logit diff (IOI dataset): 4.1336\n",
      "Average logit diff (ABC dataset): -4.0758\n"
     ]
    }
   ],
   "source": [
    "N = 70\n",
    "ioi_dataset, abc_dataset, _, _, _ = generate_data_and_caches(model, N, verbose=True)\n",
    "clean_toks = ioi_dataset.toks.to(device)\n",
    "corrupted_toks = abc_dataset.toks.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(clean_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_tokens = torch.cat((torch.Tensor(ioi_dataset.io_tokenIDs).unsqueeze(1), torch.Tensor(ioi_dataset.s_tokenIDs).unsqueeze(1)), dim=1).to(device)\n",
    "answer_tokens = answer_tokens.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.3623,  4.5626,  4.4264,  5.5464,  4.9847,  3.3949,  6.6630,  4.6075,\n",
       "         4.1188,  0.7323,  3.5184,  5.4281, -1.2406,  3.1855,  3.9544,  4.9033,\n",
       "         4.9604, -0.5812,  6.0159,  4.4829,  4.9364,  3.5120,  3.6026,  5.8260,\n",
       "         1.5506,  3.6591,  5.1734,  7.8747,  1.9717,  3.0304,  1.3404,  5.6763,\n",
       "         6.4933,  3.3304,  5.4253,  3.1553,  1.6709,  5.3956,  3.8436,  1.4698,\n",
       "         7.1663,  7.1676,  6.1948,  5.9164,  6.3674,  5.1585,  6.8696,  3.2112,\n",
       "         0.9600,  5.3084,  2.2612,  4.1867,  2.8197,  6.8392,  6.7878,  4.9921,\n",
       "         4.7495,  4.8143,  3.2216,  5.5183,  0.8875,  4.1779,  3.3880,  3.5489,\n",
       "         0.9434,  4.0729,  3.7018,  2.4539,  7.2850,  2.4210], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_logit_diff(logits, answer_tokens, ioi_dataset.word_idx[\"end\"].to(device), per_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 3.3623,  4.5626,  4.4264,  5.5464,  4.9847,  3.3949,  6.6630,  4.6075,\n",
       "         4.1188,  0.7323,  3.5184,  5.4281, -1.2406,  3.1855,  3.9544,  4.9033,\n",
       "         4.9604, -0.5812,  6.0159,  4.4829,  4.9364,  3.5120,  3.6026,  5.8260,\n",
       "         1.5506,  3.6591,  5.1734,  7.8747,  1.9717,  3.0304,  1.3404,  5.6763,\n",
       "         6.4933,  3.3304,  5.4253,  3.1553,  1.6709,  5.3956,  3.8436,  1.4698,\n",
       "         7.1663,  7.1676,  6.1948,  5.9164,  6.3674,  5.1585,  6.8696,  3.2112,\n",
       "         0.9600,  5.3084,  2.2612,  4.1867,  2.8197,  6.8392,  6.7878,  4.9921,\n",
       "         4.7495,  4.8143,  3.2216,  5.5183,  0.8875,  4.1779,  3.3880,  3.5489,\n",
       "         0.9434,  4.0729,  3.7018,  2.4539,  7.2850,  2.4210], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_logits_to_mean_logit_diff(logits, ioi_dataset, per_prompt=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greater-Than"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.greater_than_dataset import get_prob_diff, YearDataset, get_valid_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = YearDataset(get_valid_years(model.tokenizer, 1100, 1800), 1000, Path(\"data/potential_nouns.txt\"), model.tokenizer)\n",
    "\n",
    "def batch(iterable, n:int=1):\n",
    "   current_batch = []\n",
    "   for item in iterable:\n",
    "       current_batch.append(item)\n",
    "       if len(current_batch) == n:\n",
    "           yield current_batch\n",
    "           current_batch = []\n",
    "   if current_batch:\n",
    "       yield current_batch\n",
    "\n",
    "clean = list(batch(ds.good_sentences, 9))\n",
    "labels = list(batch(ds.years_YY, 9))\n",
    "corrupted = list(batch(ds.bad_sentences, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 768\n",
    "#model.to_str_tokens(ds.good_toks[IDX]), model.to_str_tokens(ds.bad_toks[IDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def prepare_indices_for_prob_diff(tokenizer, years):\n",
    "    \"\"\"\n",
    "    Prepares two tensors for use with the compute_probability_diff function in 'groups' mode.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): Tokenizer to convert years to token indices.\n",
    "        years (torch.Tensor): Tensor containing the year for each prompt in the batch.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor, torch.Tensor: Two tensors, one for token IDs and one for correct/incorrect flags.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the indices for years 00 to 99\n",
    "    year_indices = get_year_indices(tokenizer)  # Tensor of size 100 with token IDs for years\n",
    "\n",
    "    # Prepare tensors to store token IDs and correct/incorrect flags\n",
    "    token_ids_tensor = year_indices.repeat(years.size(0), 1)  # Repeat the year_indices for each batch item\n",
    "    flags_tensor = torch.zeros_like(token_ids_tensor)  # Initialize the flags tensor with zeros\n",
    "\n",
    "    for i, year in enumerate(years):\n",
    "        # Mark years greater than the given year as correct (1)\n",
    "        flags_tensor[i, year + 1:] = 1\n",
    "        # Mark years less than or equal to the given year as incorrect (-1)\n",
    "        flags_tensor[i, :year + 1] = -1\n",
    "\n",
    "    return token_ids_tensor, flags_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_length = 1 + len(model.tokenizer(ds.good_sentences[0])[0])\n",
    "prob_diff = get_prob_diff(model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.circuit_utils import run_with_batches\n",
    "\n",
    "clean_logits = run_with_batches(model, ds.good_toks.to(device), batch_size=20, max_seq_len=12)\n",
    "corrupted_logits = run_with_batches(model, ds.bad_toks.to(device), batch_size=20, max_seq_len=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_tokens, group_flags = prepare_indices_for_prob_diff(model.tokenizer, torch.Tensor(ds.years_YY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 12, 50304]), torch.Size([1000, 100, 2]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_logits.shape, answer_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.8294, device='cuda:0')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_diff(clean_logits,ds.years_YY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_diff = compute_logit_diff(logits=clean_logits, answer_token_indices=answer_tokens, flags_tensor=group_flags, mode=\"groups\")\n",
    "probability_diff = compute_probability_diff(logits=clean_logits, answer_token_indices=answer_tokens, flags_tensor=group_flags, mode=\"group_sum\")\n",
    "probability_mass = compute_probability_mass(logits=clean_logits, answer_token_indices=answer_tokens, flags_tensor=group_flags, mode=\"groups\", group=\"correct\")\n",
    "rank_0_rate = compute_rank_0_rate(logits=clean_logits, answer_token_indices=answer_tokens, flags_tensor=group_flags, mode=\"groups\", group=\"correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_cache/pythia-160m/step143000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-160m into HookedTransformer\n",
      "Clean logit diff: -0.8333\n",
      "Corrupted logit diff: 0.2896\n",
      "Moving model to device:  cpu\n",
      "Loading model for step 1...\n",
      "model_cache/pythia-160m/step1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-160m into HookedTransformer\n",
      "Getting metric values...\n",
      "Moving model to device:  cpu\n",
      "Loading model for step 2...\n",
      "model_cache/pythia-160m/step2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-160m into HookedTransformer\n",
      "Getting metric values...\n"
     ]
    }
   ],
   "source": [
    "from utils.model_utils import clear_gpu_memory, load_model\n",
    "from utils.circuit_utils import CircuitMetric\n",
    "import utils.circuit_utils as cu\n",
    "\n",
    "model_name = \"pythia-160m\"\n",
    "model_full_name = \"EleutherAI/pythia-160m\"\n",
    "model_tl_full_name = \"EleutherAI/pythia-160m\"\n",
    "cache_dir = \"model_cache\"\n",
    "batch_size = 20\n",
    "\n",
    "\n",
    "model = load_model(\n",
    "    model_full_name, model_tl_full_name, \"step143000\", cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "# set up data\n",
    "ds = YearDataset(get_valid_years(model.tokenizer, 1100, 1800), 100, Path(\"data/potential_nouns.txt\"), model.tokenizer)\n",
    "\n",
    "prob_diff = get_prob_diff(model.tokenizer)\n",
    "prob_diff_metric = CircuitMetric(\"prob_diff\", partial(prob_diff, years=ds.years_YY))\n",
    "\n",
    "metrics = [prob_diff_metric]\n",
    "\n",
    "# get baselines\n",
    "clean_logits = cu.run_with_batches(model, ds.good_toks.to(device), batch_size=20, max_seq_len=12)\n",
    "corrupted_logits = cu.run_with_batches(model, ds.bad_toks.to(device), batch_size=20, max_seq_len=12)\n",
    "\n",
    "clean_prob_diff = prob_diff_metric(clean_logits)\n",
    "print(f\"Clean logit diff: {clean_prob_diff:.4f}\")\n",
    "\n",
    "corrupted_prob_diff = prob_diff_metric(corrupted_logits)\n",
    "print(f\"Corrupted logit diff: {corrupted_prob_diff:.4f}\")\n",
    "\n",
    "clear_gpu_memory(model)\n",
    "\n",
    "# specify checkpoint schedule\n",
    "\n",
    "ckpts = [1, 2]\n",
    "\n",
    "# get values over time\n",
    "results_dict = cu.get_chronological_circuit_performance_flexible(\n",
    "    model_full_name,\n",
    "    model_tl_full_name,\n",
    "    cache_dir,\n",
    "    ckpts,\n",
    "    clean_tokens=ds.good_toks.to(device),\n",
    "    corrupted_tokens=ds.bad_toks.to(device),\n",
    "    metrics=metrics,\n",
    "    max_seq_len=12,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# save results\n",
    "os.makedirs(f\"results/{model_name}-no-dropout\", exist_ok=True)\n",
    "\n",
    "for metric in results_dict.keys():\n",
    "    torch.save(\n",
    "        results_dict[metric], f\"results/{model_name}-no-dropout/{metric}.pt\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.sentiment_datasets import get_dataset, PromptType, get_prompts\n",
    "from utils.circuit_analysis import get_logit_diff as get_logit_diff_ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = PromptType.CLASSIFICATION_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading prompts from config and filtering\n"
     ]
    }
   ],
   "source": [
    "prompts = get_prompts(model, ds_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I thought this movie was amazing, I loved it. The acting was awesome, the plot was beautiful, and overall the movie was just very good. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' amazing', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' awesome', ',', ' the', ' plot', ' was', ' beautiful', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was awesome, I loved it. The acting was beautiful, the plot was brilliant, and overall the movie was just very good. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' awesome', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' beautiful', ',', ' the', ' plot', ' was', ' brilliant', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was beautiful, I loved it. The acting was brilliant, the plot was exceptional, and overall the movie was just very good. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' beautiful', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' brilliant', ',', ' the', ' plot', ' was', ' exceptional', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was brilliant, I loved it. The acting was exceptional, the plot was extraordinary, and overall the movie was just very good. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' brilliant', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' exceptional', ',', ' the', ' plot', ' was', ' extraordinary', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was exceptional, I loved it. The acting was extraordinary, the plot was fabulous, and overall the movie was just very good. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' exceptional', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' extraordinary', ',', ' the', ' plot', ' was', ' fabulous', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was extraordinary, I loved it. The acting was fabulous, the plot was fantastic, and overall the movie was just very good. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' extraordinary', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' fabulous', ',', ' the', ' plot', ' was', ' fantastic', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was fabulous, I loved it. The acting was fantastic, the plot was good, and overall the movie was just very good. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' fabulous', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' fantastic', ',', ' the', ' plot', ' was', ' good', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was fantastic, I loved it. The acting was good, the plot was great, and overall the movie was just very good. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' fantastic', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' good', ',', ' the', ' plot', ' was', ' great', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was good, I loved it. The acting was great, the plot was incredible, and overall the movie was just very good. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' good', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' great', ',', ' the', ' plot', ' was', ' incredible', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was great, I loved it. The acting was incredible, the plot was lovely, and overall the movie was just very good. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' great', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' incredible', ',', ' the', ' plot', ' was', ' lovely', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was incredible, I loved it. The acting was lovely, the plot was outstanding, and overall the movie was just very good. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' incredible', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' lovely', ',', ' the', ' plot', ' was', ' outstanding', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was awful, I hated it. The acting was bad, the plot was disappointing, and overall the movie was just very applaud. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' awful', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' bad', ',', ' the', ' plot', ' was', ' disappointing', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' applaud', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was bad, I hated it. The acting was disappointing, the plot was disgusting, and overall the movie was just very appreciate. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' bad', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' disappointing', ',', ' the', ' plot', ' was', ' disgusting', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' appreciate', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was disappointing, I hated it. The acting was disgusting, the plot was dreadful, and overall the movie was just very commend. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' disappointing', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' disgusting', ',', ' the', ' plot', ' was', ' dreadful', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' commend', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was disgusting, I hated it. The acting was dreadful, the plot was horrible, and overall the movie was just very embrace. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' disgusting', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' dreadful', ',', ' the', ' plot', ' was', ' horrible', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' embrace', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was dreadful, I hated it. The acting was horrible, the plot was miserable, and overall the movie was just very endorse. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' dreadful', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' horrible', ',', ' the', ' plot', ' was', ' miserable', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' endorse', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was horrible, I hated it. The acting was miserable, the plot was offensive, and overall the movie was just very enjoy. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' horrible', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' miserable', ',', ' the', ' plot', ' was', ' offensive', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' enjoy', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was miserable, I hated it. The acting was offensive, the plot was terrible, and overall the movie was just very favor. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' miserable', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' offensive', ',', ' the', ' plot', ' was', ' terrible', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' favor', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was offensive, I hated it. The acting was terrible, the plot was unpleasant, and overall the movie was just very like. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' offensive', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' terrible', ',', ' the', ' plot', ' was', ' unpleasant', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' like', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was terrible, I hated it. The acting was unpleasant, the plot was wretched, and overall the movie was just very love. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' terrible', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' unpleasant', ',', ' the', ' plot', ' was', ' wretched', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' love', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was unpleasant, I hated it. The acting was wretched, the plot was awful, and overall the movie was just very praise. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' unpleasant', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' wretched', ',', ' the', ' plot', ' was', ' awful', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' praise', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n",
      "I thought this movie was wretched, I hated it. The acting was awful, the plot was bad, and overall the movie was just very respect. Review Sentiment:\n",
      "['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' wretched', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' awful', ',', ' the', ' plot', ' was', ' bad', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' respect', '.', ' Review', ' Sent', 'iment', ':']\n",
      "torch.Size([1, 35])\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts[0][\"positive\"]:\n",
    "    print(prompt)\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    print(model.to_str_tokens(tokens))\n",
    "    print(tokens.shape)\n",
    "\n",
    "for prompt in prompts[0][\"negative\"]:\n",
    "    print(prompt)\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    print(model.to_str_tokens(tokens))\n",
    "    print(tokens.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,    42,  1869,   436,  6440,   369,  8644,    13,   309,  7636,\n",
       "           352,    15,   380,  8534,   369, 13103,    13,   253,  7484,   369,\n",
       "          5389,    13,   285,  4583,   253,  6440,   369,   816,  1077,  1175,\n",
       "            15,  8439, 20580,  2092,    27]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_tokens(prompts[0][\"positive\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'positive': ['I thought this movie was amazing, I loved it. The acting was awesome, the plot was beautiful, and overall the movie was just very good. Review Sentiment:',\n",
       "   'I thought this movie was awesome, I loved it. The acting was beautiful, the plot was brilliant, and overall the movie was just very good. Review Sentiment:',\n",
       "   'I thought this movie was beautiful, I loved it. The acting was brilliant, the plot was exceptional, and overall the movie was just very good. Review Sentiment:',\n",
       "   'I thought this movie was brilliant, I loved it. The acting was exceptional, the plot was extraordinary, and overall the movie was just very good. Review Sentiment:',\n",
       "   'I thought this movie was exceptional, I loved it. The acting was extraordinary, the plot was fabulous, and overall the movie was just very good. Review Sentiment:',\n",
       "   'I thought this movie was extraordinary, I loved it. The acting was fabulous, the plot was fantastic, and overall the movie was just very good. Review Sentiment:',\n",
       "   'I thought this movie was fabulous, I loved it. The acting was fantastic, the plot was good, and overall the movie was just very good. Review Sentiment:',\n",
       "   'I thought this movie was fantastic, I loved it. The acting was good, the plot was great, and overall the movie was just very good. Review Sentiment:',\n",
       "   'I thought this movie was good, I loved it. The acting was great, the plot was incredible, and overall the movie was just very good. Review Sentiment:',\n",
       "   'I thought this movie was great, I loved it. The acting was incredible, the plot was lovely, and overall the movie was just very good. Review Sentiment:',\n",
       "   'I thought this movie was incredible, I loved it. The acting was lovely, the plot was outstanding, and overall the movie was just very good. Review Sentiment:'],\n",
       "  'negative': ['I thought this movie was awful, I hated it. The acting was bad, the plot was disappointing, and overall the movie was just very applaud. Review Sentiment:',\n",
       "   'I thought this movie was bad, I hated it. The acting was disappointing, the plot was disgusting, and overall the movie was just very appreciate. Review Sentiment:',\n",
       "   'I thought this movie was disappointing, I hated it. The acting was disgusting, the plot was dreadful, and overall the movie was just very commend. Review Sentiment:',\n",
       "   'I thought this movie was disgusting, I hated it. The acting was dreadful, the plot was horrible, and overall the movie was just very embrace. Review Sentiment:',\n",
       "   'I thought this movie was dreadful, I hated it. The acting was horrible, the plot was miserable, and overall the movie was just very endorse. Review Sentiment:',\n",
       "   'I thought this movie was horrible, I hated it. The acting was miserable, the plot was offensive, and overall the movie was just very enjoy. Review Sentiment:',\n",
       "   'I thought this movie was miserable, I hated it. The acting was offensive, the plot was terrible, and overall the movie was just very favor. Review Sentiment:',\n",
       "   'I thought this movie was offensive, I hated it. The acting was terrible, the plot was unpleasant, and overall the movie was just very like. Review Sentiment:',\n",
       "   'I thought this movie was terrible, I hated it. The acting was unpleasant, the plot was wretched, and overall the movie was just very love. Review Sentiment:',\n",
       "   'I thought this movie was unpleasant, I hated it. The acting was wretched, the plot was awful, and overall the movie was just very praise. Review Sentiment:',\n",
       "   'I thought this movie was wretched, I hated it. The acting was awful, the plot was bad, and overall the movie was just very respect. Review Sentiment:'],\n",
       "  'neutral': ['I thought this movie was ok, I watched it. The acting was okay, the plot was OK, and overall the movie was just very average. Review Sentiment:',\n",
       "   'I thought this movie was okay, I watched it. The acting was OK, the plot was alright, and overall the movie was just very average. Review Sentiment:',\n",
       "   'I thought this movie was OK, I watched it. The acting was alright, the plot was fine, and overall the movie was just very average. Review Sentiment:',\n",
       "   'I thought this movie was alright, I watched it. The acting was fine, the plot was neutral, and overall the movie was just very average. Review Sentiment:',\n",
       "   'I thought this movie was fine, I watched it. The acting was neutral, the plot was acceptable, and overall the movie was just very average. Review Sentiment:',\n",
       "   'I thought this movie was neutral, I watched it. The acting was acceptable, the plot was fair, and overall the movie was just very average. Review Sentiment:',\n",
       "   'I thought this movie was acceptable, I watched it. The acting was fair, the plot was standard, and overall the movie was just very average. Review Sentiment:',\n",
       "   'I thought this movie was fair, I watched it. The acting was standard, the plot was reasonable, and overall the movie was just very average. Review Sentiment:',\n",
       "   'I thought this movie was standard, I watched it. The acting was reasonable, the plot was average, and overall the movie was just very average. Review Sentiment:',\n",
       "   'I thought this movie was reasonable, I watched it. The acting was average, the plot was ok, and overall the movie was just very average. Review Sentiment:',\n",
       "   'I thought this movie was average, I watched it. The acting was ok, the plot was okay, and overall the movie was just very average. Review Sentiment:']},\n",
       " {'positive': CircularList([' Positive']),\n",
       "  'negative': CircularList([' Negative'])})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading prompts from config and filtering\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I thought this movie was amazing, I loved it. The acting was awesome, the plot was beautiful, and overall the movie was just very good. Review Sentiment:'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = get_dataset(model, device, prompt_type=ds_type)\n",
    "ds.all_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 35])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.clean_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 1, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.answer_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_logits = model(ds.clean_tokens.to(device))\n",
    "corrupted_logits = model(ds.corrupted_tokens.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 1, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.answer_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import CircuitMetric\n",
    "logit_diff_metric = CircuitMetric(\"logit_diff_multi\", partial(get_logit_diff_ca, answer_tokens=ds.answer_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3842, device='cuda:0'), tensor(-0.3842, device='cuda:0'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_diff_metric(clean_logits), logit_diff_metric(corrupted_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3842, device='cuda:0'), tensor(-0.3842, device='cuda:0'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_logit_diff(clean_logits, ds.answer_tokens, mode=\"pairs\"), compute_logit_diff(corrupted_logits, ds.answer_tokens, mode=\"pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
