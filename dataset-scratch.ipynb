{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_200/1711627342.py:5: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_200/1711627342.py:6: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "ipython = get_ipython()\n",
    "ipython.magic(\"load_ext autoreload\")\n",
    "ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f554e6db9a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Optional, Union, Dict, Tuple\n",
    "from pathlib import Path \n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import circuitsvis as cv\n",
    "\n",
    "import transformer_lens.utils as tl_utils\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens.patching as patching\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "import plotly.express as px\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "\n",
    "from path_patching_cm.path_patching import Node, IterNode, path_patch, act_patch\n",
    "from path_patching_cm.ioi_dataset import IOIDataset, NAMES\n",
    "from neel_plotly import imshow as imshow_n\n",
    "\n",
    "from utils.visualization import imshow_p, plot_attention_heads, plot_attention\n",
    "from utils.data_utils import generate_data_and_caches, UniversalPatchingDataset\n",
    "from utils.metrics import compute_logit_diff, compute_probability_diff, compute_probability_mass, compute_rank_0_rate\n",
    "from utils.visualization_utils import (\n",
    "    plot_attention_heads,\n",
    "    scatter_attention_and_contribution,\n",
    "    get_attn_head_patterns\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_reciprocal_rank(\n",
    "        logits: torch.Tensor, \n",
    "        answer_token_indices: torch.Tensor,\n",
    "        positions: torch.Tensor = None,\n",
    "        flags_tensor: torch.Tensor = None,\n",
    "        mode=\"simple\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the Mean Reciprocal Rank (MRR) for each item in the batch.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits to use.\n",
    "        answer_token_indices (torch.Tensor): Indices of the correct answer tokens.\n",
    "        positions (torch.Tensor): Positions to get logits at, one position per batch item.\n",
    "        flags_tensor (torch.Tensor): Flags indicating the grouping of tokens (used in \"groups\" mode).\n",
    "        mode (str): Mode of operation - \"simple\", \"pairs\", or \"groups\".\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The Mean Reciprocal Rank for the batch.\n",
    "    \"\"\"\n",
    "    logits = get_positional_logits(logits, positions)\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    mrr = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "    # Mode 1: Simple\n",
    "    if mode == \"simple\":\n",
    "        correct_indices = answer_token_indices[:, 0]\n",
    "        for i in range(logits.size(0)):\n",
    "            sorted_indices = probabilities[i].sort(descending=True)[1]\n",
    "            rank = (sorted_indices == correct_indices[i]).nonzero(as_tuple=True)[0].item() + 1\n",
    "            mrr[i] = 1.0 / rank\n",
    "\n",
    "    # Mode 2: Pairs\n",
    "    elif mode == \"pairs\":\n",
    "        for i in range(logits.size(0)):\n",
    "            for pair in answer_token_indices[i]:\n",
    "                sorted_indices = probabilities[i].sort(descending=True)[1]\n",
    "                rank = (sorted_indices == pair[0]).nonzero(as_tuple=True)[0].item() + 1\n",
    "                mrr[i] += 1.0 / rank\n",
    "            mrr[i] /= answer_token_indices.size(1)\n",
    "\n",
    "    # Mode 3: Groups\n",
    "    elif mode == \"groups\":\n",
    "        assert flags_tensor is not None\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "            sorted_indices = selected_probs.sort(descending=True)[1]\n",
    "            correct_ranks = (flags_tensor[i] == 1).nonzero(as_tuple=True)[0]\n",
    "            ranks = torch.tensor([sorted_indices.tolist().index(rank.item()) + 1 for rank in correct_ranks])\n",
    "            mrr[i] = (1.0 / ranks).mean()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified\")\n",
    "\n",
    "    return mrr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# model = HookedTransformer.from_pretrained(\n",
    "#     \"EleutherAI/pythia-2.8b\",\n",
    "#     center_unembed=True,\n",
    "#     center_writing_weights=True,\n",
    "#     fold_ln=True,\n",
    "#     refactor_factored_attn_matrices=False,\n",
    "# )\n",
    "# model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-2.8b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"EleutherAI/pythia-2.8b\",\n",
    "    checkpoint_value=10000,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    refactor_factored_attn_matrices=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_logits(\n",
    "        logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "        positions: Float[Tensor, \"batch\"] = None\n",
    ")-> Float[Tensor, \"batch d_vocab\"]:\n",
    "    \"\"\"Gets the logits at the provided positions. If no positions are provided, the final logits are returned.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits to use.\n",
    "        positions (torch.Tensor): Positions to get logits at. This should be a tensor of shape (batch_size,).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Logits at the provided positions.\n",
    "    \"\"\"\n",
    "    if positions is None:\n",
    "        return logits[:, -1, :]\n",
    "    \n",
    "    return logits[range(logits.size(0)), positions, :]\n",
    "\n",
    "\n",
    "def compute_logit_diff(\n",
    "        logits: Float[Tensor, \"batch seq d_vocab\"], \n",
    "        answer_token_indices: Float[Tensor, \"batch num_answers\"],\n",
    "        positions: Float[Tensor, \"batch\"] = None,\n",
    "        flags_tensor: torch.Tensor = None,\n",
    "        per_prompt=False,\n",
    "        mode=\"simple\"\n",
    ")-> Float[Tensor, \"batch num_answers\"]:\n",
    "    \"\"\"Computes the difference between a correct and incorrect logit (or mean of a group of logits) for each item in the batch.\n",
    "\n",
    "    Takes the full logits, and the indices of the tokens to compare. These indices can be of multiple types as follows:\n",
    "\n",
    "    - Simple: The tensor should be of shape (batch_size, 2), where the first index in the third dimension is the correct token index,\n",
    "        and the second index is the incorrect token index.\n",
    "\n",
    "    - Pairs: In this mode, answer_token_indices is a 3D tensor of shape (batch, num_pairs, 2). For each pair, you'll need to compute \n",
    "             the difference between the logits at the two indices, then average these differences across each pair for every batch item.\n",
    "\n",
    "    - Groups: Here, answer_token_indices is also a 3D tensor of shape (batch, num_tokens, 2). The third dimension indicates group membership \n",
    "              (correct or incorrect). The mean logits for each group are calculated and then subtracted from each other.\n",
    "              \n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits to use.\n",
    "        answer_token_indices (torch.Tensor): Indices of the tokens to compare.\n",
    "        positions (torch.Tensor): Positions to get logits at. Should be one position per batch item.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Difference between the logits of the provided tokens.\n",
    "    \"\"\"\n",
    "    logits = get_positional_logits(logits, positions)\n",
    "    \n",
    "    # Mode 1: Simple\n",
    "    if mode == \"simple\":\n",
    "        correct_logits = logits[torch.arange(logits.size(0)), answer_token_indices[:, 0]]\n",
    "        incorrect_logits = logits[torch.arange(logits.size(0)), answer_token_indices[:, 1]]\n",
    "        logit_diff = correct_logits - incorrect_logits\n",
    "\n",
    "    # Mode 2: Pairs\n",
    "    elif mode == \"pairs\":\n",
    "        pair_diffs = logits[torch.arange(logits.size(0))[:, None], answer_token_indices[..., 0]] - \\\n",
    "                     logits[torch.arange(logits.size(0))[:, None], answer_token_indices[..., 1]]\n",
    "        logit_diff = pair_diffs.mean(dim=1)\n",
    "\n",
    "    # Mode 3: Groups\n",
    "    elif mode == \"groups\":\n",
    "        assert flags_tensor is not None\n",
    "        logit_diff = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_logits = logits[i, answer_token_indices[i]]\n",
    "\n",
    "            # Calculate the logit difference using the correct/incorrect flags\n",
    "            correct_logits = selected_logits[flags_tensor[i] == 1]\n",
    "            incorrect_logits = selected_logits[flags_tensor[i] == -1]\n",
    "\n",
    "            # Handle cases where there are no correct or incorrect logits\n",
    "            if len(correct_logits) > 0:\n",
    "                correct_mean = correct_logits.mean()\n",
    "            else:\n",
    "                correct_mean = 0\n",
    "\n",
    "            if len(incorrect_logits) > 0:\n",
    "                incorrect_mean = incorrect_logits.mean()\n",
    "            else:\n",
    "                incorrect_mean = 0\n",
    "\n",
    "            logit_diff[i] = correct_mean - incorrect_mean\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified\")\n",
    "\n",
    "    return logit_diff.mean() if not per_prompt else logit_diff\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_probability_diff(\n",
    "        logits: torch.Tensor, \n",
    "        answer_token_indices: torch.Tensor,\n",
    "        positions: torch.Tensor = None,\n",
    "        flags_tensor: torch.Tensor = None,\n",
    "        per_prompt=False,\n",
    "        mode=\"simple\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Computes the difference between probability of a correct and incorrect logit (or mean of a group of logits) for each item in the batch.\n",
    "\n",
    "    Takes the full logits, and the indices of the tokens to compare. These indices can be of multiple types as follows:\n",
    "\n",
    "    - Simple: The tensor should be of shape (batch_size, 2), where the first index in the third dimension is the correct token index,\n",
    "        and the second index is the incorrect token index.\n",
    "\n",
    "    - Pairs: In this mode, answer_token_indices is a 3D tensor of shape (batch, num_pairs, 2). For each pair, you'll need to compute \n",
    "             the difference between the probabilities at the two indices, then average these differences across each pair for every batch item.\n",
    "\n",
    "    - Groups: Here, answer_token_indices is also a 3D tensor of shape (batch, num_tokens, 2). The third dimension indicates group membership \n",
    "              (correct or incorrect). The mean probabilities for each group are calculated and then subtracted from each other.\n",
    "              \n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits to use.\n",
    "        answer_token_indices (torch.Tensor): Indices of the tokens to compare.\n",
    "        positions (torch.Tensor): Positions to get logits at. Should be one position per batch item.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Difference between the logits of the provided tokens.\n",
    "    \"\"\"\n",
    "    logits = get_positional_logits(logits, positions)\n",
    "    probabilities = torch.softmax(logits, dim=-1)  # Applying softmax to logits\n",
    "    print(f\"probabilities={probabilities.shape}\")\n",
    "\n",
    "    # Mode 1: Simple\n",
    "    if mode == \"simple\":\n",
    "        correct_probs = probabilities[torch.arange(logits.size(0)), answer_token_indices[:, 0]]\n",
    "        incorrect_probs = probabilities[torch.arange(logits.size(0)), answer_token_indices[:, 1]]\n",
    "        prob_diff = correct_probs - incorrect_probs\n",
    "\n",
    "    # Mode 2: Pairs\n",
    "    elif mode == \"pairs\":\n",
    "        pair_diffs = probabilities[torch.arange(logits.size(0))[:, None], answer_token_indices[..., 0]] - \\\n",
    "                     probabilities[torch.arange(logits.size(0))[:, None], answer_token_indices[..., 1]]\n",
    "        prob_diff = pair_diffs.mean(dim=1)\n",
    "\n",
    "    # Mode 3: Groups\n",
    "    elif mode == \"groups\":\n",
    "        # Initialize tensors to store the probability differences for each batch item\n",
    "        assert flags_tensor is not None\n",
    "        prob_diff = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            # Select the probabilities for the token IDs of this batch item\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "\n",
    "            # Calculate the probability difference using the correct/incorrect flags\n",
    "            correct_probs = selected_probs[flags_tensor[i] == 1]\n",
    "            incorrect_probs = selected_probs[flags_tensor[i] == -1]\n",
    "\n",
    "            # Handle cases where there are no correct or incorrect tokens\n",
    "            if len(correct_probs) > 0:\n",
    "                correct_mean = correct_probs.mean()\n",
    "            else:\n",
    "                correct_mean = 0\n",
    "\n",
    "            if len(incorrect_probs) > 0:\n",
    "                incorrect_mean = incorrect_probs.mean()\n",
    "            else:\n",
    "                incorrect_mean = 0\n",
    "\n",
    "            prob_diff[i] = correct_mean - incorrect_mean\n",
    "\n",
    "    # Mode 4: Group Sum\n",
    "    elif mode == \"group_sum\":\n",
    "        assert flags_tensor is not None\n",
    "        prob_diff = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "\n",
    "            # Calculate the sum of probabilities using the correct/incorrect flags\n",
    "            correct_sum = selected_probs[flags_tensor[i] == 1].sum()\n",
    "            incorrect_sum = selected_probs[flags_tensor[i] == -1].sum()\n",
    "\n",
    "            prob_diff[i] = incorrect_sum - correct_sum\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified\")\n",
    "\n",
    "    return prob_diff.mean() if not per_prompt else prob_diff\n",
    "\n",
    "\n",
    "def compute_probability_mass(\n",
    "        logits: torch.Tensor, \n",
    "        answer_token_indices: torch.Tensor,\n",
    "        positions: torch.Tensor = None,\n",
    "        flags_tensor: torch.Tensor = None,\n",
    "        group=\"correct\",\n",
    "        mode=\"simple\"\n",
    ") -> torch.Tensor:\n",
    "    logits = get_positional_logits(logits, positions)\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    # Determine the flag value based on the specified group\n",
    "    flag_value = 1 if group == \"correct\" else -1\n",
    "\n",
    "    # Mode logic\n",
    "    if mode == \"simple\":\n",
    "        selected_indices = answer_token_indices[:, 0] if group == \"correct\" else answer_token_indices[:, 1]\n",
    "        group_probs = probabilities[torch.arange(logits.size(0)), selected_indices]\n",
    "\n",
    "    elif mode == \"pairs\":\n",
    "        group_probs = torch.zeros(logits.size(0), device=logits.device)\n",
    "        for i in range(logits.size(0)):\n",
    "            for pair in answer_token_indices[i]:\n",
    "                selected_index = pair[0] if group == \"correct\" else pair[1]\n",
    "                group_probs[i] += probabilities[i, selected_index]\n",
    "            group_probs[i] /= answer_token_indices.size(1)\n",
    "\n",
    "    elif mode == \"groups\":\n",
    "        assert flags_tensor is not None\n",
    "        group_probs = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "            group_probs[i] = selected_probs[flags_tensor[i] == flag_value].mean()\n",
    "\n",
    "    elif mode == \"group_sum\":\n",
    "        assert flags_tensor is not None\n",
    "        group_probs = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "            group_probs[i] = selected_probs[flags_tensor[i] == flag_value].sum()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified\")\n",
    "\n",
    "    return group_probs.mean()\n",
    "\n",
    "\n",
    "\n",
    "def compute_rank_0_rate(\n",
    "        logits: torch.Tensor, \n",
    "        answer_token_indices: torch.Tensor,\n",
    "        positions: torch.Tensor = None,\n",
    "        flags_tensor: torch.Tensor = None,\n",
    "        group=\"correct\",\n",
    "        mode=\"simple\"\n",
    ") -> torch.Tensor:\n",
    "    logits = get_positional_logits(logits, positions)\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    # Mode logic\n",
    "    if mode == \"simple\":\n",
    "        top_rank_indices = probabilities.argmax(dim=-1)\n",
    "        correct_indices = answer_token_indices[:, 0] if group == \"correct\" else answer_token_indices[:, 1]\n",
    "        rank_0_rate = (top_rank_indices == correct_indices).float().mean()\n",
    "\n",
    "    elif mode == \"pairs\":\n",
    "        rank_0_rate = torch.zeros(logits.size(0), device=logits.device)\n",
    "        for i in range(logits.size(0)):\n",
    "            for pair in answer_token_indices[i]:\n",
    "                top_rank_index = probabilities[i].argmax()\n",
    "                correct_index = pair[0] if group == \"correct\" else pair[1]\n",
    "                rank_0_rate[i] += (top_rank_index == correct_index).float()\n",
    "            rank_0_rate[i] /= answer_token_indices.size(1)\n",
    "\n",
    "    elif mode == \"groups\":\n",
    "        assert flags_tensor is not None\n",
    "        rank_0_rate = torch.zeros(logits.size(0), device=logits.device)\n",
    "\n",
    "        for i in range(logits.size(0)):\n",
    "            selected_probs = probabilities[i, answer_token_indices[i]]\n",
    "            top_rank_id = selected_probs.argmax()\n",
    "            rank_0_rate[i] = (flags_tensor[i, top_rank_id] == 1).float() if group == \"correct\" else \\\n",
    "                             (flags_tensor[i, top_rank_id] == -1).float()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified\")\n",
    "\n",
    "    return rank_0_rate.mean()\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def compute_max_group_rank_reciprocal(\n",
    "        logits: torch.Tensor, \n",
    "        answer_token_indices: torch.Tensor,\n",
    "        positions: torch.Tensor = None,\n",
    "        flags_tensor: torch.Tensor = None,\n",
    "        mode=\"simple\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the mean of the reciprocal of the maximum rank of members of the correct group across different modes.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Logits to use.\n",
    "        answer_token_indices (torch.Tensor): Indices of the tokens for comparison or grouping.\n",
    "        positions (torch.Tensor): Positions to get logits at, one position per batch item.\n",
    "        flags_tensor (torch.Tensor): Flags indicating the grouping of tokens (used in \"groups\" mode).\n",
    "        mode (str): Operation mode - \"simple\", \"pairs\", or \"groups\".\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The mean of the reciprocal of the maximum rank of correct group members.\n",
    "    \"\"\"\n",
    "    logits = get_positional_logits(logits, positions)\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    batch_size = logits.size(0)\n",
    "\n",
    "    # Initialize tensor to hold the reciprocal of the maximum rank for each item in the batch\n",
    "    reciprocal_max_rank = torch.zeros(batch_size, device=logits.device)\n",
    "\n",
    "    if mode == \"simple\":\n",
    "        for i in range(batch_size):\n",
    "            correct_index = answer_token_indices[i, 0]\n",
    "            sorted_indices = probabilities[i].sort(descending=True)[1]\n",
    "            rank = (sorted_indices == correct_index).nonzero(as_tuple=True)[0].item() + 1\n",
    "            reciprocal_max_rank[i] = 1.0 / rank\n",
    "\n",
    "    elif mode == \"pairs\":\n",
    "        for i in range(batch_size):\n",
    "            pair_ranks = []\n",
    "            for pair in answer_token_indices[i]:\n",
    "                # Only consider the first index in each pair as correct\n",
    "                correct_index = pair[0]\n",
    "                sorted_indices = probabilities[i].sort(descending=True)[1]\n",
    "                rank = (sorted_indices == correct_index).nonzero(as_tuple=True)[0].item() + 1\n",
    "                pair_ranks.append(rank)\n",
    "            # Use the max rank from pairs\n",
    "            max_rank = min(pair_ranks)\n",
    "            reciprocal_max_rank[i] = 1.0 / max_rank\n",
    "\n",
    "    elif mode == \"groups\":\n",
    "        for i in range(batch_size):\n",
    "            group_ranks = []\n",
    "            for j, flag in enumerate(flags_tensor[i]):\n",
    "                if flag == 1:  # Correct group member\n",
    "                    correct_index = answer_token_indices[i, j]\n",
    "                    sorted_indices = probabilities[i].sort(descending=True)[1]\n",
    "                    rank = (sorted_indices == correct_index).nonzero(as_tuple=True)[0].item() + 1\n",
    "                    group_ranks.append(rank)\n",
    "            # Use the max rank from correct group members\n",
    "            if group_ranks:\n",
    "                max_rank = min(group_ranks)\n",
    "                reciprocal_max_rank[i] = 1.0 / max_rank\n",
    "            else:\n",
    "                reciprocal_max_rank[i] = 0  # Handle case with no correct answers\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode specified\")\n",
    "\n",
    "    return reciprocal_max_rank.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple mode MRR: 1.0\n",
      "Pairs mode MRR: 0.5\n",
      "Groups mode MRR: 0.25\n"
     ]
    }
   ],
   "source": [
    "def test_compute_max_group_rank_reciprocal():\n",
    "    # Define a helper function to simplify the test cases creation\n",
    "    def create_logits_and_indices(logits_values, correct_indices, flags=None):\n",
    "        logits = torch.tensor(logits_values, dtype=torch.float).unsqueeze(0).unsqueeze(1)  # Add batch dimension if needed\n",
    "        answer_token_indices = torch.tensor(correct_indices, dtype=torch.long).unsqueeze(1)  # Adjust dimensions as needed\n",
    "        flags_tensor = torch.tensor(flags, dtype=torch.long).unsqueeze(1) if flags is not None else None\n",
    "        return logits, answer_token_indices, flags_tensor\n",
    "\n",
    "    # Simple Mode Test Case\n",
    "    logits, answer_token_indices, _ = create_logits_and_indices([0.1, 0.2, 0.7, 0.6], [2])\n",
    "    mrr_simple = compute_max_group_rank_reciprocal(logits, answer_token_indices, mode=\"simple\")\n",
    "    print(f\"Simple mode MRR: {mrr_simple}\")\n",
    "\n",
    "    # Pairs Mode Test Case\n",
    "    logits, answer_token_indices, _ = create_logits_and_indices([[0.1, 0.2], [0.7, 0.6], [0.4, 0.5]], [[[2, 1], [0, 3]]])\n",
    "    mrr_pairs = compute_max_group_rank_reciprocal(logits, answer_token_indices, mode=\"pairs\")\n",
    "    print(f\"Pairs mode MRR: {mrr_pairs}\")\n",
    "\n",
    "    # Groups Mode Test Case\n",
    "    logits, answer_token_indices, flags_tensor = create_logits_and_indices([0.1, 0.2, 0.7, 0.6], [0, 1, 2, 3], [1, -1, 1, -1])\n",
    "    mrr_groups = compute_max_group_rank_reciprocal(logits, answer_token_indices, flags_tensor=flags_tensor, mode=\"groups\")\n",
    "    print(f\"Groups mode MRR: {mrr_groups}\")\n",
    "\n",
    "# Execute the test function\n",
    "test_compute_max_group_rank_reciprocal()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _logits_to_mean_logit_diff(logits: Float[Tensor, \"batch seq d_vocab\"], ioi_dataset: IOIDataset, per_prompt=False):\n",
    "    '''\n",
    "    Returns logit difference between the correct and incorrect answer.\n",
    "\n",
    "    If per_prompt=True, return the array of differences rather than the average.\n",
    "    '''\n",
    "\n",
    "    # Only the final logits are relevant for the answer\n",
    "    # Get the logits corresponding to the indirect object / subject tokens respectively\n",
    "    io_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), ioi_dataset.word_idx[\"end\"], ioi_dataset.io_tokenIDs]\n",
    "    print(io_logits.shape)\n",
    "    s_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), ioi_dataset.word_idx[\"end\"], ioi_dataset.s_tokenIDs]\n",
    "    # Find logit difference\n",
    "    answer_logit_diff = io_logits - s_logits\n",
    "    return answer_logit_diff if per_prompt else answer_logit_diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average logit diff (IOI dataset): 2.1560\n",
      "Average logit diff (ABC dataset): -2.3650\n"
     ]
    }
   ],
   "source": [
    "N = 70\n",
    "ioi_dataset, abc_dataset, _, _, _ = generate_data_and_caches(model, N, verbose=True)\n",
    "clean_toks = ioi_dataset.toks.to(device)\n",
    "corrupted_toks = abc_dataset.toks.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1210,  0.6408,  4.3679,  3.2872,  3.1052,  2.4821,  4.4465,  0.3192,\n",
       "         2.3799, -0.6708,  2.6662,  3.7824,  0.8591,  0.4677,  0.2200,  3.3067,\n",
       "         5.4179,  0.1047,  2.9370,  0.1339,  2.9071,  3.4575,  2.6687,  5.2737,\n",
       "         1.8654,  0.7553,  0.5263,  4.9929,  1.6752,  0.7784,  2.5545,  1.4186,\n",
       "         3.3981, -0.6675,  1.2263, -1.0020,  0.8154,  5.7192,  3.0224,  0.3226,\n",
       "         6.5352,  4.1738,  1.3393,  2.3986,  4.4888,  2.1893,  5.5038,  0.3437,\n",
       "        -0.7437,  4.7308,  0.9047,  2.9857,  2.0731,  3.2680,  4.3207,  3.0555,\n",
       "         2.4662,  3.7404,  0.7805,  2.3680,  2.7400,  1.5318,  0.1822,  0.9666,\n",
       "         0.9466,  0.9064, -0.6956,  0.6441,  2.0869,  2.6091], device='cuda:0')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(clean_toks)\n",
    "_logits_to_mean_logit_diff(logits, ioi_dataset, per_prompt=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average logit diff (IOI dataset): 4.1921\n",
      "Average logit diff (ABC dataset): -3.9570\n"
     ]
    }
   ],
   "source": [
    "ds = UniversalPatchingDataset.from_ioi(model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(ds.toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.1921, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_logit_diff(logits, ds.answer_toks, ds.positions, per_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities=torch.Size([100, 50304])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2722, device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_probability_diff(logits, ds.answer_toks, ds.positions, per_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8254, device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_mean_reciprocal_rank(logits, ds.answer_toks, ds.positions, mode=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8254, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_max_group_rank_reciprocal(logits, ds.answer_toks, ds.positions, mode=\"simple\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greater-Than"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.greater_than_dataset import get_prob_diff, YearDataset, get_valid_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_old = YearDataset(get_valid_years(model.tokenizer, 1100, 1800), 1000, Path(\"data/potential_nouns.txt\"), model.tokenizer)\n",
    "\n",
    "# def batch(iterable, n:int=1):\n",
    "#    current_batch = []\n",
    "#    for item in iterable:\n",
    "#        current_batch.append(item)\n",
    "#        if len(current_batch) == n:\n",
    "#            yield current_batch\n",
    "#            current_batch = []\n",
    "#    if current_batch:\n",
    "#        yield current_batch\n",
    "\n",
    "# clean = list(batch(ds.good_sentences, 9))\n",
    "# labels = list(batch(ds.years_YY, 9))\n",
    "# corrupted = list(batch(ds.bad_sentences, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 768\n",
    "#model.to_str_tokens(ds.good_toks[IDX]), model.to_str_tokens(ds.bad_toks[IDX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def prepare_indices_for_prob_diff(tokenizer, years):\n",
    "    \"\"\"\n",
    "    Prepares two tensors for use with the compute_probability_diff function in 'groups' mode.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): Tokenizer to convert years to token indices.\n",
    "        years (torch.Tensor): Tensor containing the year for each prompt in the batch.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor, torch.Tensor: Two tensors, one for token IDs and one for correct/incorrect flags.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the indices for years 00 to 99\n",
    "    year_indices = get_year_indices(tokenizer)  # Tensor of size 100 with token IDs for years\n",
    "\n",
    "    # Prepare tensors to store token IDs and correct/incorrect flags\n",
    "    token_ids_tensor = year_indices.repeat(years.size(0), 1)  # Repeat the year_indices for each batch item\n",
    "    flags_tensor = torch.zeros_like(token_ids_tensor)  # Initialize the flags tensor with zeros\n",
    "\n",
    "    for i, year in enumerate(years):\n",
    "        # Mark years greater than the given year as correct (1)\n",
    "        flags_tensor[i, year + 1:] = 1\n",
    "        # Mark years less than or equal to the given year as incorrect (-1)\n",
    "        flags_tensor[i, :year + 1] = -1\n",
    "\n",
    "    return token_ids_tensor, flags_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_length = 1 + len(model.tokenizer(ds.good_sentences[0])[0])\n",
    "prob_diff = get_prob_diff(model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.circuit_utils import run_with_batches\n",
    "\n",
    "clean_logits = run_with_batches(model, ds_old.good_toks.to(device), batch_size=20, max_seq_len=12)\n",
    "corrupted_logits = run_with_batches(model, ds_old.bad_toks.to(device), batch_size=20, max_seq_len=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.8363, device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_diff(clean_logits,ds_old.years_YY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = UniversalPatchingDataset.from_greater_than(model, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(ds.toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities=torch.Size([1000, 50304])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.8294, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_probability_diff(logits, ds.answer_toks, flags_tensor=ds.group_flags, mode=\"group_sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1409, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_mean_reciprocal_rank(logits, ds.answer_toks, ds.positions, ds.group_flags, mode=\"groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9980, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_max_group_rank_reciprocal(logits, ds.answer_toks, ds.positions, ds.group_flags, mode=\"groups\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.sentiment_datasets import get_dataset, PromptType, get_prompts\n",
    "from utils.circuit_analysis import get_logit_diff as get_logit_diff_ca"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = PromptType.CLASSIFICATION_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading prompts from config and filtering\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I thought this movie was amazing, I loved it. The acting was awesome, the plot was beautiful, and overall the movie was just very good. Review Sentiment:'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = get_dataset(model, device, prompt_type=ds_type)\n",
    "ds.all_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 35])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.clean_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 1, 2])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.answer_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_logits = model(ds.clean_tokens.to(device))\n",
    "corrupted_logits = model(ds.corrupted_tokens.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 1, 2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.answer_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import CircuitMetric\n",
    "logit_diff_metric = CircuitMetric(\"logit_diff_multi\", partial(get_logit_diff_ca, answer_tokens=ds.answer_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3842, device='cuda:0'), tensor(-0.3842, device='cuda:0'))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_diff_metric(clean_logits), logit_diff_metric(corrupted_logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading prompts from config and filtering\n"
     ]
    }
   ],
   "source": [
    "ds = UniversalPatchingDataset.from_sentiment(model, \"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(ds.toks)\n",
    "flipped_logits = model(ds.flipped_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.1217, device='cuda:0'), tensor(-3.1217, device='cuda:0'))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_logit_diff(logits, ds.answer_toks, mode=\"pairs\"), compute_logit_diff(flipped_logits, ds.answer_toks, mode=\"pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4267, device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_mean_reciprocal_rank(logits, ds.answer_toks, ds.positions, mode=\"pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4267, device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_max_group_rank_reciprocal(logits, ds.answer_toks, ds.positions, mode=\"pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' amazing', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' awesome', ',', ' the', ' plot', ' was', ' beautiful', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.18</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.65</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.18\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m9.65\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.18 Prob:  9.65% Token: | Positive|\n",
      "Top 1th token. Logit: 14.07 Prob:  8.68% Token: | 4|\n",
      "Top 2th token. Logit: 13.85 Prob:  6.99% Token: | 9|\n",
      "Top 3th token. Logit: 13.75 Prob:  6.29% Token: | 8|\n",
      "Top 4th token. Logit: 13.68 Prob:  5.84% Token: | 5|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' awful', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' bad', ',', ' the', ' plot', ' was', ' disappointing', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' applaud', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47</span><span style=\"font-weight: bold\">       Logit:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.94</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.20</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m47\u001b[0m\u001b[1m       Logit:  \u001b[0m\u001b[1;36m9.94\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.20\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.13 Prob: 13.44% Token: | 1|\n",
      "Top 1th token. Logit: 13.89 Prob: 10.63% Token: | 0|\n",
      "Top 2th token. Logit: 13.31 Prob:  5.96% Token: | 3|\n",
      "Top 3th token. Logit: 13.10 Prob:  4.82% Token: | 4|\n",
      "Top 4th token. Logit: 13.08 Prob:  4.70% Token: | 2|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m47\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' awesome', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' beautiful', ',', ' the', ' plot', ' was', ' brilliant', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.20</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.69</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.20\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m9.69\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.20 Prob:  9.69% Token: | Positive|\n",
      "Top 1th token. Logit: 14.14 Prob:  9.14% Token: | 4|\n",
      "Top 2th token. Logit: 13.88 Prob:  7.06% Token: | 8|\n",
      "Top 3th token. Logit: 13.87 Prob:  6.93% Token: | 9|\n",
      "Top 4th token. Logit: 13.69 Prob:  5.83% Token: | 5|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' bad', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' disappointing', ',', ' the', ' plot', ' was', ' disgusting', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' appreciate', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.13</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.67</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m20\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m11.13\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.67\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 13.66 Prob:  8.46% Token: | 1|\n",
      "Top 1th token. Logit: 13.62 Prob:  8.07% Token: | 0|\n",
      "Top 2th token. Logit: 13.15 Prob:  5.07% Token: | 5|\n",
      "Top 3th token. Logit: 13.14 Prob:  4.99% Token: | 4|\n",
      "Top 4th token. Logit: 13.09 Prob:  4.75% Token: | 3|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m20\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' beautiful', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' brilliant', ',', ' the', ' plot', ' was', ' exceptional', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.19</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.50</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.19\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m9.50\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.22 Prob:  9.78% Token: | 4|\n",
      "Top 1th token. Logit: 14.19 Prob:  9.50% Token: | Positive|\n",
      "Top 2th token. Logit: 13.92 Prob:  7.22% Token: | 9|\n",
      "Top 3th token. Logit: 13.86 Prob:  6.80% Token: | 5|\n",
      "Top 4th token. Logit: 13.80 Prob:  6.40% Token: | 8|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' disappointing', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' disgusting', ',', ' the', ' plot', ' was', ' dreadful', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' commend', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.32</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.22</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m35\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m10.32\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.22\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.64 Prob: 16.21% Token: | 1|\n",
      "Top 1th token. Logit: 14.35 Prob: 12.09% Token: | 0|\n",
      "Top 2th token. Logit: 13.76 Prob:  6.74% Token: | 3|\n",
      "Top 3th token. Logit: 13.58 Prob:  5.64% Token: | 2|\n",
      "Top 4th token. Logit: 13.53 Prob:  5.34% Token: | 4|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m35\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' brilliant', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' exceptional', ',', ' the', ' plot', ' was', ' extraordinary', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.96</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.68</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.96\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m7.68\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.19 Prob:  9.65% Token: | 4|\n",
      "Top 1th token. Logit: 13.96 Prob:  7.68% Token: | Positive|\n",
      "Top 2th token. Logit: 13.93 Prob:  7.45% Token: | 8|\n",
      "Top 3th token. Logit: 13.91 Prob:  7.34% Token: | 9|\n",
      "Top 4th token. Logit: 13.84 Prob:  6.81% Token: | 5|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' disgusting', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' dreadful', ',', ' the', ' plot', ' was', ' horrible', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' embrace', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.30</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.31</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m34\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m10.30\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.31\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 13.94 Prob: 11.96% Token: | 0|\n",
      "Top 1th token. Logit: 13.75 Prob:  9.86% Token: | 1|\n",
      "Top 2th token. Logit: 12.82 Prob:  3.91% Token: | 3|\n",
      "Top 3th token. Logit: 12.74 Prob:  3.61% Token: | -|\n",
      "Top 4th token. Logit: 12.64 Prob:  3.27% Token: |\n",
      "|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m34\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' exceptional', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' extraordinary', ',', ' the', ' plot', ' was', ' fabulous', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.91</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.53</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m13.91\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m7.53\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.08 Prob:  8.93% Token: | 4|\n",
      "Top 1th token. Logit: 13.91 Prob:  7.53% Token: | Positive|\n",
      "Top 2th token. Logit: 13.87 Prob:  7.22% Token: | 9|\n",
      "Top 3th token. Logit: 13.80 Prob:  6.73% Token: | 5|\n",
      "Top 4th token. Logit: 13.78 Prob:  6.57% Token: | 8|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' dreadful', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' horrible', ',', ' the', ' plot', ' was', ' miserable', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' endorse', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.26</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.30</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m29\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m10.26\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.30\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.13 Prob: 14.34% Token: | 0|\n",
      "Top 1th token. Logit: 14.03 Prob: 13.07% Token: | 1|\n",
      "Top 2th token. Logit: 13.31 Prob:  6.31% Token: | -|\n",
      "Top 3th token. Logit: 12.90 Prob:  4.22% Token: | 3|\n",
      "Top 4th token. Logit: 12.70 Prob:  3.43% Token: | 2|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m29\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' extraordinary', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' fabulous', ',', ' the', ' plot', ' was', ' fantastic', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.04</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.65</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.04\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m8.65\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.17 Prob:  9.91% Token: | 4|\n",
      "Top 1th token. Logit: 14.04 Prob:  8.65% Token: | Positive|\n",
      "Top 2th token. Logit: 13.79 Prob:  6.76% Token: | 5|\n",
      "Top 3th token. Logit: 13.77 Prob:  6.58% Token: | 8|\n",
      "Top 4th token. Logit: 13.76 Prob:  6.57% Token: | 9|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' horrible', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' miserable', ',', ' the', ' plot', ' was', ' offensive', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' enjoy', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.34</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.27</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m35\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m10.34\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.27\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.34 Prob: 14.58% Token: | 0|\n",
      "Top 1th token. Logit: 14.19 Prob: 12.53% Token: | 1|\n",
      "Top 2th token. Logit: 13.23 Prob:  4.79% Token: |\n",
      "|\n",
      "Top 3th token. Logit: 13.10 Prob:  4.21% Token: | 3|\n",
      "Top 4th token. Logit: 13.02 Prob:  3.89% Token: | 2|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m35\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' fabulous', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' fantastic', ',', ' the', ' plot', ' was', ' good', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.46</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.11</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.46\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m12.11\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.46 Prob: 12.11% Token: | Positive|\n",
      "Top 1th token. Logit: 14.29 Prob: 10.18% Token: | 4|\n",
      "Top 2th token. Logit: 13.81 Prob:  6.31% Token: | 8|\n",
      "Top 3th token. Logit: 13.73 Prob:  5.85% Token: | 5|\n",
      "Top 4th token. Logit: 13.56 Prob:  4.92% Token: | 9|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' miserable', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' offensive', ',', ' the', ' plot', ' was', ' terrible', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' favor', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.44</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.33</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m27\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m10.44\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.33\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.07 Prob: 12.34% Token: | 0|\n",
      "Top 1th token. Logit: 14.07 Prob: 12.34% Token: | 1|\n",
      "Top 2th token. Logit: 13.51 Prob:  7.03% Token: | -|\n",
      "Top 3th token. Logit: 13.25 Prob:  5.41% Token: | 3|\n",
      "Top 4th token. Logit: 13.06 Prob:  4.48% Token: | 2|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m27\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' fantastic', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' good', ',', ' the', ' plot', ' was', ' great', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.65</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.99</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.65\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m13.99\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.65 Prob: 13.99% Token: | Positive|\n",
      "Top 1th token. Logit: 14.20 Prob:  8.91% Token: | 4|\n",
      "Top 2th token. Logit: 13.94 Prob:  6.86% Token: | 8|\n",
      "Top 3th token. Logit: 13.81 Prob:  6.01% Token: | 9|\n",
      "Top 4th token. Logit: 13.63 Prob:  5.03% Token: | 5|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' offensive', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' terrible', ',', ' the', ' plot', ' was', ' unpleasant', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' like', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.32</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.88</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m18\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m11.32\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.88\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 13.75 Prob:  9.94% Token: | 1|\n",
      "Top 1th token. Logit: 13.74 Prob:  9.81% Token: | 0|\n",
      "Top 2th token. Logit: 12.78 Prob:  3.76% Token: | 3|\n",
      "Top 3th token. Logit: 12.71 Prob:  3.50% Token: | -|\n",
      "Top 4th token. Logit: 12.59 Prob:  3.10% Token: | 2|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m18\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' good', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' great', ',', ' the', ' plot', ' was', ' incredible', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.54</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.84</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.54\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m12.84\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.54 Prob: 12.84% Token: | Positive|\n",
      "Top 1th token. Logit: 14.18 Prob:  8.93% Token: | 4|\n",
      "Top 2th token. Logit: 14.18 Prob:  8.89% Token: | 8|\n",
      "Top 3th token. Logit: 13.80 Prob:  6.07% Token: | 9|\n",
      "Top 4th token. Logit: 13.56 Prob:  4.78% Token: | 5|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' terrible', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' unpleasant', ',', ' the', ' plot', ' was', ' wretched', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' love', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.66</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.47</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m28\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m10.66\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.47\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 13.62 Prob:  9.15% Token: | 1|\n",
      "Top 1th token. Logit: 13.57 Prob:  8.66% Token: | 0|\n",
      "Top 2th token. Logit: 12.73 Prob:  3.74% Token: |\n",
      "|\n",
      "Top 3th token. Logit: 12.73 Prob:  3.74% Token: | 3|\n",
      "Top 4th token. Logit: 12.72 Prob:  3.69% Token: | -|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m28\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' great', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' incredible', ',', ' the', ' plot', ' was', ' lovely', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.37</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.96</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.37\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m10.96\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.37 Prob: 10.96% Token: | Positive|\n",
      "Top 1th token. Logit: 14.24 Prob:  9.69% Token: | 4|\n",
      "Top 2th token. Logit: 14.04 Prob:  7.92% Token: | 8|\n",
      "Top 3th token. Logit: 13.90 Prob:  6.84% Token: | 9|\n",
      "Top 4th token. Logit: 13.74 Prob:  5.88% Token: | 5|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' unpleasant', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' wretched', ',', ' the', ' plot', ' was', ' awful', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' praise', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.33</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.34</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m27\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m10.33\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.34\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 13.92 Prob: 12.14% Token: | 1|\n",
      "Top 1th token. Logit: 13.72 Prob:  9.93% Token: | 0|\n",
      "Top 2th token. Logit: 13.14 Prob:  5.59% Token: | 3|\n",
      "Top 3th token. Logit: 12.88 Prob:  4.30% Token: | 4|\n",
      "Top 4th token. Logit: 12.79 Prob:  3.91% Token: | 2|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m27\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' incredible', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' lovely', ',', ' the', ' plot', ' was', ' outstanding', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' good', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.20</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.15</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.20\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m9.15\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.27 Prob:  9.80% Token: | 4|\n",
      "Top 1th token. Logit: 14.20 Prob:  9.15% Token: | Positive|\n",
      "Top 2th token. Logit: 14.03 Prob:  7.75% Token: | 9|\n",
      "Top 3th token. Logit: 13.98 Prob:  7.35% Token: | 8|\n",
      "Top 4th token. Logit: 13.86 Prob:  6.48% Token: | 5|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' wretched', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' awful', ',', ' the', ' plot', ' was', ' bad', ',', ' and', ' overall', ' the', ' movie', ' was', ' just', ' very', ' respect', '.', ' Review', ' Sent', 'iment', ':']\n",
      "Tokenized answer: [' Positive']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.46</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.32</span><span style=\"font-weight: bold\">% Token: | Positive|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m32\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m10.46\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.32\u001b[0m\u001b[1m% Token: | Positive|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 14.23 Prob: 13.97% Token: | 1|\n",
      "Top 1th token. Logit: 13.76 Prob:  8.79% Token: | 0|\n",
      "Top 2th token. Logit: 13.22 Prob:  5.12% Token: | 3|\n",
      "Top 3th token. Logit: 13.06 Prob:  4.33% Token: | 2|\n",
      "Top 4th token. Logit: 12.97 Prob:  3.98% Token: | -|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Positive'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Positive'\u001b[0m, \u001b[1;36m32\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "for prompt_tokens in ds.toks:\n",
    "    prompt = model.to_string(prompt_tokens[1:])\n",
    "    test_prompt(prompt, \" Positive\", model, top_k=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_type = PromptType.COMPLETION_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading prompts from config and filtering\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I thought this movie was amazing, I loved it. The acting was awesome, the plot was beautiful, and overall it was just very'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = get_dataset(model, device, prompt_type=ds_type)\n",
    "ds.all_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 28])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.clean_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 5, 2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.answer_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_logits = model(ds.clean_tokens.to(device))\n",
    "corrupted_logits = model(ds.corrupted_tokens.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 5, 2])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.answer_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import CircuitMetric\n",
    "logit_diff_metric = CircuitMetric(\"logit_diff_multi\", partial(get_logit_diff_ca, answer_tokens=ds.answer_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.2004, device='cuda:0'), tensor(-3.2004, device='cuda:0'))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_diff_metric(clean_logits), logit_diff_metric(corrupted_logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading prompts from config and filtering\n"
     ]
    }
   ],
   "source": [
    "ds = UniversalPatchingDataset.from_sentiment(model, \"cont\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2004, device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(ds.toks)\n",
    "compute_logit_diff(logits, ds.answer_toks, mode=\"pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.metrics import compute_accuracy\n",
    "compute_accuracy(logits, ds.answer_toks, mode=\"pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2233, device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_mean_reciprocal_rank(logits, ds.answer_toks, mode=\"pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_max_group_rank_reciprocal(logits, ds.answer_toks, mode=\"pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' amazing', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' awesome', ',', ' the', ' plot', ' was', ' beautiful', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">75</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.11</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.15</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m75\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m27.11\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.80 Prob: 15.98% Token: | good|\n",
      "Top 1th token. Logit: 31.14 Prob:  8.25% Token: | well|\n",
      "Top 2th token. Logit: 30.82 Prob:  5.98% Token: | enjoyable|\n",
      "Top 3th token. Logit: 30.66 Prob:  5.10% Token: | entertaining|\n",
      "Top 4th token. Logit: 30.62 Prob:  4.92% Token: | fun|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">75</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m75\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' awful', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' bad', ',', ' the', ' plot', ' was', ' disappointing', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31.67</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.77</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m31.67\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m14.77\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.67 Prob: 14.77% Token: | bad|\n",
      "Top 1th token. Logit: 30.85 Prob:  6.50% Token: | poor|\n",
      "Top 2th token. Logit: 30.38 Prob:  4.07% Token: | dull|\n",
      "Top 3th token. Logit: 30.29 Prob:  3.72% Token: | boring|\n",
      "Top 4th token. Logit: 30.24 Prob:  3.55% Token: |,|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' awesome', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' beautiful', ',', ' the', ' plot', ' was', ' brilliant', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">63</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.16</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.16</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m63\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m27.16\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.16\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.84 Prob: 17.54% Token: | good|\n",
      "Top 1th token. Logit: 31.12 Prob:  8.56% Token: | well|\n",
      "Top 2th token. Logit: 30.94 Prob:  7.14% Token: | enjoyable|\n",
      "Top 3th token. Logit: 30.79 Prob:  6.14% Token: | fun|\n",
      "Top 4th token. Logit: 30.64 Prob:  5.28% Token: | entertaining|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">63</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m63\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' bad', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' disappointing', ',', ' the', ' plot', ' was', ' disgusting', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31.95</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.35</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m31.95\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m18.35\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.95 Prob: 18.35% Token: | bad|\n",
      "Top 1th token. Logit: 30.74 Prob:  5.49% Token: | poor|\n",
      "Top 2th token. Logit: 30.60 Prob:  4.77% Token: | boring|\n",
      "Top 3th token. Logit: 30.36 Prob:  3.76% Token: | good|\n",
      "Top 4th token. Logit: 30.34 Prob:  3.69% Token: | dull|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' beautiful', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' brilliant', ',', ' the', ' plot', ' was', ' exceptional', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">74</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.15</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.15</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m74\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m27.15\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 32.06 Prob: 19.87% Token: | good|\n",
      "Top 1th token. Logit: 31.41 Prob: 10.45% Token: | well|\n",
      "Top 2th token. Logit: 30.81 Prob:  5.72% Token: | enjoyable|\n",
      "Top 3th token. Logit: 30.40 Prob:  3.80% Token: | entertaining|\n",
      "Top 4th token. Logit: 30.22 Prob:  3.16% Token: | fun|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">74</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m74\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' disappointing', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' disgusting', ',', ' the', ' plot', ' was', ' dreadful', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31.37</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.52</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m31.37\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m9.52\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.37 Prob:  9.52% Token: | bad|\n",
      "Top 1th token. Logit: 31.06 Prob:  6.94% Token: | boring|\n",
      "Top 2th token. Logit: 30.77 Prob:  5.24% Token: | poor|\n",
      "Top 3th token. Logit: 30.69 Prob:  4.80% Token: | dull|\n",
      "Top 4th token. Logit: 30.53 Prob:  4.12% Token: | poorly|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' brilliant', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' exceptional', ',', ' the', ' plot', ' was', ' extraordinary', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">61</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.32</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.16</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m61\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m27.32\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.16\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 32.23 Prob: 21.64% Token: | good|\n",
      "Top 1th token. Logit: 31.37 Prob:  9.15% Token: | well|\n",
      "Top 2th token. Logit: 31.11 Prob:  7.03% Token: | enjoyable|\n",
      "Top 3th token. Logit: 30.84 Prob:  5.34% Token: | entertaining|\n",
      "Top 4th token. Logit: 30.49 Prob:  3.78% Token: | fun|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">61</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m61\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' disgusting', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' dreadful', ',', ' the', ' plot', ' was', ' horrible', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31.37</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.55</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m31.37\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m10.55\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.37 Prob: 10.55% Token: | bad|\n",
      "Top 1th token. Logit: 30.60 Prob:  4.90% Token: | boring|\n",
      "Top 2th token. Logit: 30.38 Prob:  3.91% Token: | poor|\n",
      "Top 3th token. Logit: 30.29 Prob:  3.57% Token: |,|\n",
      "Top 4th token. Logit: 30.14 Prob:  3.09% Token: | dull|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' exceptional', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' extraordinary', ',', ' the', ' plot', ' was', ' fabulous', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">68</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.18</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.16</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m68\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m27.18\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.16\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.99 Prob: 19.69% Token: | good|\n",
      "Top 1th token. Logit: 31.26 Prob:  9.44% Token: | well|\n",
      "Top 2th token. Logit: 31.21 Prob:  9.01% Token: | enjoyable|\n",
      "Top 3th token. Logit: 30.59 Prob:  4.83% Token: | entertaining|\n",
      "Top 4th token. Logit: 30.20 Prob:  3.29% Token: | fun|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">68</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m68\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' dreadful', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' horrible', ',', ' the', ' plot', ' was', ' miserable', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31.62</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.30</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m31.62\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m13.30\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.62 Prob: 13.30% Token: | bad|\n",
      "Top 1th token. Logit: 31.05 Prob:  7.56% Token: | poor|\n",
      "Top 2th token. Logit: 30.77 Prob:  5.70% Token: | dull|\n",
      "Top 3th token. Logit: 30.66 Prob:  5.09% Token: | boring|\n",
      "Top 4th token. Logit: 30.41 Prob:  3.99% Token: |,|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' extraordinary', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' fabulous', ',', ' the', ' plot', ' was', ' fantastic', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26.70</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.12</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m98\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m26.70\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.12\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.44 Prob: 13.29% Token: | good|\n",
      "Top 1th token. Logit: 31.06 Prob:  9.10% Token: | well|\n",
      "Top 2th token. Logit: 30.72 Prob:  6.51% Token: | enjoyable|\n",
      "Top 3th token. Logit: 30.50 Prob:  5.22% Token: | entertaining|\n",
      "Top 4th token. Logit: 29.86 Prob:  2.75% Token: |,|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m98\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' horrible', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' miserable', ',', ' the', ' plot', ' was', ' offensive', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32.07</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.31</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m32.07\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m18.31\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 32.07 Prob: 18.31% Token: | bad|\n",
      "Top 1th token. Logit: 31.03 Prob:  6.42% Token: | poor|\n",
      "Top 2th token. Logit: 30.50 Prob:  3.81% Token: | boring|\n",
      "Top 3th token. Logit: 30.37 Prob:  3.32% Token: | poorly|\n",
      "Top 4th token. Logit: 30.36 Prob:  3.29% Token: |,|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' fabulous', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' fantastic', ',', ' the', ' plot', ' was', ' good', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">82</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26.86</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.12</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m82\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m26.86\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.12\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.96 Prob: 19.68% Token: | good|\n",
      "Top 1th token. Logit: 31.32 Prob: 10.45% Token: | well|\n",
      "Top 2th token. Logit: 31.09 Prob:  8.27% Token: | enjoyable|\n",
      "Top 3th token. Logit: 30.61 Prob:  5.11% Token: | fun|\n",
      "Top 4th token. Logit: 30.59 Prob:  5.03% Token: | entertaining|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">82</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m82\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' miserable', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' offensive', ',', ' the', ' plot', ' was', ' terrible', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31.67</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.67</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m31.67\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m13.67\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.67 Prob: 13.67% Token: | bad|\n",
      "Top 1th token. Logit: 30.98 Prob:  6.83% Token: | boring|\n",
      "Top 2th token. Logit: 30.95 Prob:  6.60% Token: | poor|\n",
      "Top 3th token. Logit: 30.55 Prob:  4.45% Token: | dull|\n",
      "Top 4th token. Logit: 30.20 Prob:  3.12% Token: | depressing|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' fantastic', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' good', ',', ' the', ' plot', ' was', ' great', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.30</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.18</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m55\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m27.30\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.18\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 32.05 Prob: 20.33% Token: | good|\n",
      "Top 1th token. Logit: 31.42 Prob: 10.91% Token: | well|\n",
      "Top 2th token. Logit: 31.26 Prob:  9.24% Token: | enjoyable|\n",
      "Top 3th token. Logit: 30.64 Prob:  5.00% Token: | entertaining|\n",
      "Top 4th token. Logit: 30.48 Prob:  4.23% Token: | fun|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">55</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m55\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' offensive', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' terrible', ',', ' the', ' plot', ' was', ' unpleasant', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31.45</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.41</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m31.45\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m12.41\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.45 Prob: 12.41% Token: | bad|\n",
      "Top 1th token. Logit: 30.07 Prob:  3.13% Token: | offensive|\n",
      "Top 2th token. Logit: 30.02 Prob:  2.99% Token: | poorly|\n",
      "Top 3th token. Logit: 30.01 Prob:  2.96% Token: | boring|\n",
      "Top 4th token. Logit: 29.95 Prob:  2.78% Token: |,|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' good', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' great', ',', ' the', ' plot', ' was', ' incredible', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.77</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.23</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m40\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m27.77\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.23\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 32.44 Prob: 24.96% Token: | good|\n",
      "Top 1th token. Logit: 31.52 Prob:  9.99% Token: | well|\n",
      "Top 2th token. Logit: 31.40 Prob:  8.79% Token: | enjoyable|\n",
      "Top 3th token. Logit: 30.79 Prob:  4.81% Token: | fun|\n",
      "Top 4th token. Logit: 30.72 Prob:  4.49% Token: | entertaining|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m40\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' terrible', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' unpleasant', ',', ' the', ' plot', ' was', ' wretched', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32.15</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.42</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m32.15\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m19.42\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 32.15 Prob: 19.42% Token: | bad|\n",
      "Top 1th token. Logit: 31.25 Prob:  7.86% Token: | poor|\n",
      "Top 2th token. Logit: 30.72 Prob:  4.65% Token: | boring|\n",
      "Top 3th token. Logit: 30.66 Prob:  4.39% Token: | dull|\n",
      "Top 4th token. Logit: 30.65 Prob:  4.32% Token: | good|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' great', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' incredible', ',', ' the', ' plot', ' was', ' lovely', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.21</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.16</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m64\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m27.21\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.16\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.91 Prob: 17.65% Token: | good|\n",
      "Top 1th token. Logit: 31.30 Prob:  9.57% Token: | well|\n",
      "Top 2th token. Logit: 31.19 Prob:  8.61% Token: | enjoyable|\n",
      "Top 3th token. Logit: 30.78 Prob:  5.69% Token: | fun|\n",
      "Top 4th token. Logit: 30.55 Prob:  4.54% Token: | entertaining|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' unpleasant', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' wretched', ',', ' the', ' plot', ' was', ' awful', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31.53</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.50</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m31.53\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m13.50\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.53 Prob: 13.50% Token: | bad|\n",
      "Top 1th token. Logit: 30.71 Prob:  5.94% Token: | dull|\n",
      "Top 2th token. Logit: 30.65 Prob:  5.58% Token: | boring|\n",
      "Top 3th token. Logit: 30.23 Prob:  3.68% Token: |,|\n",
      "Top 4th token. Logit: 30.06 Prob:  3.11% Token: | poor|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' incredible', ',', ' I', ' loved', ' it', '.', ' The', ' acting', ' was', ' lovely', ',', ' the', ' plot', ' was', ' outstanding', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">91</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26.80</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.12</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m91\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m26.80\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.12\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.53 Prob: 13.50% Token: | good|\n",
      "Top 1th token. Logit: 31.21 Prob:  9.87% Token: | well|\n",
      "Top 2th token. Logit: 30.92 Prob:  7.35% Token: | enjoyable|\n",
      "Top 3th token. Logit: 30.62 Prob:  5.47% Token: | entertaining|\n",
      "Top 4th token. Logit: 30.22 Prob:  3.65% Token: | fun|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">91</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m91\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'I', ' thought', ' this', ' movie', ' was', ' wretched', ',', ' I', ' hated', ' it', '.', ' The', ' acting', ' was', ' awful', ',', ' the', ' plot', ' was', ' bad', ',', ' and', ' overall', ' it', ' was', ' just', ' very']\n",
      "Tokenized answer: [' bad']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31.44</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.08</span><span style=\"font-weight: bold\">% Token: | bad|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m31.44\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m12.08\u001b[0m\u001b[1m% Token: | bad|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 31.44 Prob: 12.08% Token: | bad|\n",
      "Top 1th token. Logit: 31.07 Prob:  8.35% Token: | poor|\n",
      "Top 2th token. Logit: 30.85 Prob:  6.68% Token: | dull|\n",
      "Top 3th token. Logit: 30.51 Prob:  4.75% Token: | boring|\n",
      "Top 4th token. Logit: 30.14 Prob:  3.30% Token: |,|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' bad'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' bad'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "for prompt_tokens in ds.toks:\n",
    "    prompt = model.to_string(prompt_tokens[1:])\n",
    "    test_prompt(prompt, \"bad\", model, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model.name = \"EleutherAI/pythia-2.8b\"\n",
    "\n",
    "\n",
    "def filter_function(example, model):\n",
    "    prompt = model.to_tokens(example['text'] + \" Review Sentiment:\", prepend_bos=False)\n",
    "    answer = torch.tensor([29071, 32725]).unsqueeze(0).unsqueeze(0).to(device) if example['label'] == 1 else torch.tensor([32725, 29071]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    #print(answer.shape)\n",
    "    logits = model(prompt, return_type=\"logits\")\n",
    "    logit_diff = compute_logit_diff(logits, answer, mode=\"pairs\")\n",
    "    \n",
    "    # Determine if the top answer (index 0) token is in top 10 logits\n",
    "    _, top_indices = logits.topk(10, dim=-1)  # Get indices of top 10 logits\n",
    "    top_answer_token = answer[0, 0, 0]  # Assuming answer is of shape (1, 1, 2) and the top answer token is at index 0\n",
    "    is_top_answer_in_top_10_logits = (top_indices == top_answer_token).any()\n",
    "    \n",
    "    # Add a new field 'keep_example' to the example\n",
    "    example['keep_example'] = (logit_diff > 0.0) and is_top_answer_in_top_10_logits.item()\n",
    "    return example\n",
    "\n",
    "\n",
    "def concatenate_classification_prompts(examples):\n",
    "    return {\"text\": (examples['text'] + \" Review Sentiment:\")}\n",
    "\n",
    "\n",
    "def get_final_pos_index(examples):\n",
    "    return {'final_pos_index': examples[\"attention_mask\"].sum() - 1}\n",
    "\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "\n",
    "def find_dataset_positions(example, token_id=13):\n",
    "    # Create a tensor of zeros with the same shape as example['tokens']\n",
    "    positions = torch.zeros_like(torch.tensor(example['tokens']), dtype=torch.int)\n",
    "\n",
    "    # Find positions where tokens match the given token_id\n",
    "    positions[example['tokens'] == token_id] = 1\n",
    "    has_token = True if positions.sum() > 0 else False\n",
    "\n",
    "    return {'positions': positions, 'has_token': has_token}\n",
    "\n",
    "\n",
    "def convert_answers(example, pos_answer_id=29071, neg_answer_id=32725):\n",
    "    if example['label'] == 1:\n",
    "        answers = torch.tensor([pos_answer_id, neg_answer_id])\n",
    "    else:\n",
    "        answers = torch.tensor([neg_answer_id, pos_answer_id])\n",
    "\n",
    "    return {'answers': answers}\n",
    "\n",
    "\n",
    "def get_random_subset(dataset, n):\n",
    "    total_size = len(dataset)\n",
    "    random_indices = random.sample(range(total_size), n)\n",
    "    return dataset.select(random_indices)\n",
    "\n",
    "\n",
    "def prepare_sst_for_model(\n",
    "        model: HookedTransformer,\n",
    "        dataset_name: str = \"sst2\", \n",
    "        batch_size: int = 5,\n",
    "        pad_token_id: int = 1, \n",
    "        pos_answer_id: int = 29071, \n",
    "        neg_answer_id: int = 32725\n",
    "    ) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    # Define the batch size\n",
    "    BATCH_SIZE = batch_size\n",
    "\n",
    "    sst_data = load_from_disk(dataset_name)\n",
    "\n",
    "    # Use the map function to apply the filter_function\n",
    "    filter_function_for_model = partial(filter_function, model=model)\n",
    "    sst_data_with_flag_train = sst_data['train'].map(filter_function_for_model, keep_in_memory=True)\n",
    "    sst_data_with_flag_dev = sst_data['dev'].map(filter_function_for_model, keep_in_memory=True)\n",
    "    sst_data_with_flag_test = sst_data['test'].map(filter_function_for_model, keep_in_memory=True)\n",
    "    #sst_data_with_flag = concatenate_datasets([sst_data['train'], sst_data['dev'], sst_data['test']])\n",
    "    sst_data_with_flag = concatenate_datasets([sst_data_with_flag_train, sst_data_with_flag_dev, sst_data_with_flag_test])\n",
    "    #sst_data_with_flag = sst_data_with_flag_dev\n",
    "\n",
    "    # Use the filter function to keep only the examples where 'keep_example' is True\n",
    "    sst_zero_shot = sst_data_with_flag.filter(lambda x: x['keep_example'])\n",
    "    # print number of items in dataset\n",
    "    print(f\"Number of items in dataset: {len(sst_zero_shot)}\")\n",
    "    # save dataset\n",
    "    #new model name without slashes\n",
    "    model_abbr = re.sub(r'/', '_', model.name)\n",
    "    sst_zero_shot.save_to_disk(f\"sst_zero_shot_{model_abbr}\")\n",
    "\n",
    "    # Load a tokenizer (you'll need to specify the appropriate model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model.name)\n",
    "    # set padding token\n",
    "    tokenizer.pad_token = model.to_string([pad_token_id])\n",
    "\n",
    "    dataset = sst_zero_shot.map(concatenate_classification_prompts, batched=False)\n",
    "    tokenizer_function_for_model = partial(tokenize_function, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(tokenizer_function_for_model, batched=False)\n",
    "    \n",
    "    convert_answers_for_model = partial(convert_answers, pos_answer_id=pos_answer_id, neg_answer_id=neg_answer_id)\n",
    "    dataset = dataset.map(convert_answers_for_model, batched=False)\n",
    "    dataset = dataset.rename_column(\"input_ids\", \"tokens\")\n",
    "    dataset.set_format(type=\"torch\", columns=[\"tokens\", \"attention_mask\", \"label\", \"answers\"])\n",
    "    dataset = dataset.map(get_final_pos_index, batched=False)\n",
    "    dataset = dataset.map(find_dataset_positions, batched=False)\n",
    "    dataset = dataset.filter(lambda example: example['has_token']==True)\n",
    "\n",
    "    # create a subset with only positive labels\n",
    "    pos_dataset = dataset.filter(lambda example: example['label']==1)\n",
    "    neg_dataset = dataset.filter(lambda example: example['label']==0)\n",
    "    len(pos_dataset), len(neg_dataset)\n",
    "\n",
    "    subset_size = (min(len(pos_dataset), len(neg_dataset)) // BATCH_SIZE) * BATCH_SIZE\n",
    "\n",
    "    pos_subset = get_random_subset(pos_dataset, subset_size)\n",
    "    neg_subset = get_random_subset(neg_dataset, subset_size)\n",
    "    balanced_subset = concatenate_datasets([pos_subset, neg_subset])\n",
    "    # randomize the order of balanced_subset\n",
    "    balanced_subset = balanced_subset.shuffle(len(balanced_subset))\n",
    "\n",
    "    balanced_subset.save_to_disk(f\"sst_zero_shot_balanced_{model_abbr}\")\n",
    "\n",
    "\n",
    "    print(f\"Number of items in pos dataset: {len(pos_subset)}\")\n",
    "    print(f\"Number of items in neg dataset: {len(neg_subset)}\")\n",
    "    print(f\"Number of items in balanced dataset: {len(balanced_subset)}\")\n",
    "    return pos_subset, neg_subset, balanced_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf1b23a2440426d8860ee817307d2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7864 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2dc368a62d4a4d9dc7b213ce32047d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1007 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a36fd8b96c4caea31e818265a8a340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2058 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e651b62295b94198b3f29907fcc747e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in dataset: 6169\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32af4db820fd4d58b09f5a2d875ba5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bc49e2fe8147818229a7c5c8c1a9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48163f00cb7843d4b2edf72e9d0154c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c8b0f13ac24472835e51a313de85ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057d05e78b594f358fc19d087830f08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d68189e8976410a94a8985e4b74a944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/133324064.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  positions = torch.zeros_like(torch.tensor(example['tokens']), dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562c51996c7a4e4cab48ba2e29463fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6169 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08fdaa8bb264782bfede001611d8eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3318 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8d5768034e41eeb672d0f7083dd73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3318 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adca3f9f42bf43249ac13c8f32a516df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1390 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in pos dataset: 695\n",
      "Number of items in neg dataset: 695\n",
      "Number of items in balanced dataset: 1390\n"
     ]
    }
   ],
   "source": [
    "pos_ds, neg_ds, balanced_ds = prepare_sst_for_model(model, \"data/sst2\", 5, 1, 29071, 32725)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "ds = load_from_disk(\"sst_zero_shot_balanced_EleutherAI_pythia-2.8b\")\n",
    "\n",
    "# Turn all items in ['tokens'] into a single tensor\n",
    "all_tokens = torch.cat([item['tokens'].unsqueeze(0) for item in ds], dim=0)\n",
    "all_answers = torch.cat([item['answers'].unsqueeze(0) for item in ds], dim=0)\n",
    "all_positions = torch.cat([item['final_pos_index'].unsqueeze(0) for item in ds], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'keep_example', 'tokens', 'attention_mask', 'answers', 'final_pos_index', 'positions', 'has_token'],\n",
       "    num_rows: 1390\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2042,   253,  1524,    14,  8206,  2926,   310, 27364, 39637,  1050,\n",
       "            13,   253,  6440, 10576,    84,   441,   347,   973,    15,  8439,\n",
       "         20580,  2092,    27,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1]),\n",
       " tensor([29071, 32725]),\n",
       " tensor(22),\n",
       " tensor(27))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]['tokens'], ds[0]['answers'], ds[0]['final_pos_index'], ds[0]['tokens'][ds[0]['final_pos_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1390, 64]), torch.Size([1390, 2]), torch.Size([1390]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens.shape, all_answers.shape, all_positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.circuit_utils import run_with_batches\n",
    "\n",
    "logits = run_with_batches(model, all_tokens[:1000].to(device), batch_size=10, max_seq_len=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.metrics import compute_accuracy\n",
    "compute_accuracy(logits, all_answers[:1000], positions=all_positions[:1000], mode=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pythia models on HF were updated on 4/3/23! add '-v0' to model name to access the old models.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-2.8b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"EleutherAI/pythia-2.8b\",\n",
    "    checkpoint_value=10000,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    refactor_factored_attn_matrices=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6299999952316284"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.circuit_utils import run_with_batches\n",
    "from utils.metrics import compute_accuracy\n",
    "logits = run_with_batches(model, all_tokens[:100].to(device), batch_size=10, max_seq_len=64)\n",
    "compute_accuracy(logits, all_answers[:100], positions=all_positions[:100], mode=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
