{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108/1711627342.py:5: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_108/1711627342.py:6: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "ipython = get_ipython()\n",
    "ipython.magic(\"load_ext autoreload\")\n",
    "ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional, Union, Dict, Tuple\n",
    "from pathlib import Path \n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import circuitsvis as cv\n",
    "\n",
    "import transformer_lens.utils as tl_utils\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens.patching as patching\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "import plotly.express as px\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "\n",
    "from path_patching_cm.path_patching import Node, IterNode, path_patch, act_patch\n",
    "from path_patching_cm.ioi_dataset import IOIDataset, NAMES\n",
    "from neel_plotly import imshow as imshow_n\n",
    "\n",
    "from utils.visualization import imshow_p, plot_attention_heads, plot_attention\n",
    "\n",
    "from utils.visualization_utils import (\n",
    "    plot_attention_heads,\n",
    "    scatter_attention_and_contribution,\n",
    "    get_attn_head_patterns\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-160m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"EleutherAI/pythia-160m\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=False,\n",
    ")\n",
    "model.set_use_hook_mlp_in(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greater-Than"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.greater_than_dataset import get_prob_diff, YearDataset, get_valid_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = YearDataset(get_valid_years(model.tokenizer, 1100, 1800), 1000, Path(\"data/potential_nouns.txt\"), model.tokenizer)\n",
    "\n",
    "def batch(iterable, n:int=1):\n",
    "   current_batch = []\n",
    "   for item in iterable:\n",
    "       current_batch.append(item)\n",
    "       if len(current_batch) == n:\n",
    "           yield current_batch\n",
    "           current_batch = []\n",
    "   if current_batch:\n",
    "       yield current_batch\n",
    "\n",
    "clean = list(batch(ds.good_sentences, 9))\n",
    "labels = list(batch(ds.years_YY, 9))\n",
    "corrupted = list(batch(ds.bad_sentences, 9))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The shift lasted from the year 1702 to the year 17',\n",
       "  'The growth lasted from the year 1703 to the year 17',\n",
       "  'The program lasted from the year 1704 to the year 17',\n",
       "  'The deal lasted from the year 1705 to the year 17',\n",
       "  'The employment lasted from the year 1706 to the year 17',\n",
       "  'The shift lasted from the year 1707 to the year 17',\n",
       "  'The order lasted from the year 1708 to the year 17',\n",
       "  'The impact lasted from the year 1709 to the year 17',\n",
       "  'The challenge lasted from the year 1710 to the year 17'],\n",
       " [tensor(2),\n",
       "  tensor(3),\n",
       "  tensor(4),\n",
       "  tensor(5),\n",
       "  tensor(6),\n",
       "  tensor(7),\n",
       "  tensor(8),\n",
       "  tensor(9),\n",
       "  tensor(10)],\n",
       " ['The shift lasted from the year 1701 to the year 17',\n",
       "  'The growth lasted from the year 1701 to the year 17',\n",
       "  'The program lasted from the year 1701 to the year 17',\n",
       "  'The deal lasted from the year 1701 to the year 17',\n",
       "  'The employment lasted from the year 1701 to the year 17',\n",
       "  'The shift lasted from the year 1701 to the year 17',\n",
       "  'The order lasted from the year 1701 to the year 17',\n",
       "  'The impact lasted from the year 1701 to the year 17',\n",
       "  'The challenge lasted from the year 1701 to the year 17'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean[0], labels[0], corrupted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 740],\n",
       "        [ 883],\n",
       "        [ 805],\n",
       "        [1012],\n",
       "        [1047],\n",
       "        [1010],\n",
       "        [1036],\n",
       "        [1166],\n",
       "        [1093],\n",
       "        [ 746],\n",
       "        [ 938],\n",
       "        [1797],\n",
       "        [1423],\n",
       "        [1508],\n",
       "        [1348],\n",
       "        [1099],\n",
       "        [1731],\n",
       "        [1630],\n",
       "        [1619],\n",
       "        [1717],\n",
       "        [1229],\n",
       "        [2405],\n",
       "        [1237],\n",
       "        [1610],\n",
       "        [1706],\n",
       "        [1671],\n",
       "        [1812],\n",
       "        [1787],\n",
       "        [1839],\n",
       "        [1867],\n",
       "        [1449],\n",
       "        [3156],\n",
       "        [2945],\n",
       "        [3079],\n",
       "        [2031],\n",
       "        [1857],\n",
       "        [2950],\n",
       "        [2504],\n",
       "        [2385],\n",
       "        [2537],\n",
       "        [1235],\n",
       "        [3712],\n",
       "        [3583],\n",
       "        [3357],\n",
       "        [3439],\n",
       "        [2417],\n",
       "        [3208],\n",
       "        [3011],\n",
       "        [3680],\n",
       "        [3046],\n",
       "        [1549],\n",
       "        [3832],\n",
       "        [3763],\n",
       "        [3571],\n",
       "        [1540],\n",
       "        [2082],\n",
       "        [2526],\n",
       "        [2251],\n",
       "        [2358],\n",
       "        [2090],\n",
       "        [1967],\n",
       "        [3677],\n",
       "        [3547],\n",
       "        [3655],\n",
       "        [3566],\n",
       "        [1976],\n",
       "        [3121],\n",
       "        [2357],\n",
       "        [3141],\n",
       "        [2787],\n",
       "        [1438],\n",
       "        [3593],\n",
       "        [3507],\n",
       "        [3245],\n",
       "        [2759],\n",
       "        [2227],\n",
       "        [2691],\n",
       "        [2597],\n",
       "        [2055],\n",
       "        [2511],\n",
       "        [2270],\n",
       "        [4739],\n",
       "        [4529],\n",
       "        [4590],\n",
       "        [3953],\n",
       "        [2222],\n",
       "        [4196],\n",
       "        [4148],\n",
       "        [4185],\n",
       "        [1525]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_tokens([str(x) for x in range(10, 100)], prepend_bos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 1 + len(model.tokenizer(ds.good_sentences[0])[0])\n",
    "prob_diff = get_prob_diff(model.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function data.greater_than_dataset.get_prob_diff.<locals>.prob_diff(logits, years)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_diff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
